# 小方向 3.4：Kernel 融合 (Kernel Fusion)

> **模块编号**: 3.4 = Phase 3（模型压缩与加速）第 4 个子模块  
> **Git Repo**: `sageLLM-kernel-fusion` | **Priority**: P1 | **Phase**: Week 11-12

______________________________________________________________________

## 模块定位

### 核心职责
- 提供统一的 Kernel 融合框架，涵盖 Attention/FFN/LayerNorm/Activation/Softmax 等算子融合
- 集成 FlashAttention-2/3、PagedAttention、xFormers 等高性能 Attention 实现
- 支持自定义 CUDA/Triton Kernel，实现算子级优化（如 Fused LayerNorm + Linear）
- 与量化（3.1）、稀疏化（3.2）协同，实现 Fused Quantized/Sparse Kernels

### 为什么需要独立模块
- **性能关键**：Kernel 融合可将延迟降低 20-40%，显存占用减少 30-50%（尤其 FlashAttention）
- **算子多样性**：Attention/FFN/Norm 各有最优融合策略，需要统一接口管理
- **硬件适配**：CUDA/ROCm/国产算力需要不同 Kernel 实现，需要抽象层
- **研究价值**：FlashAttention-3、Paged Attention、自定义 Triton Kernel 快速演进

### Baseline 参考
1. **FlashAttention-2/3**: 高效 Attention 实现，节省显存（O(N) vs O(N²)）
2. **PagedAttention (vLLM)**: 分块 Attention，与 KV Cache 管理深度集成
3. **xFormers**: Memory-efficient attention variants (cutlass/triton)
4. **FasterTransformer**: NVIDIA 官方融合 Kernel 集合
5. **Triton**: OpenAI 开源的 GPU Kernel DSL，易于自定义融合

______________________________________________________________________

## 技术规格

### 输入接口（Kernel Fusion Protocol）
```python
# core/protocols/kernel_fusion.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Literal, Optional, List, Dict, Callable
import torch

FusionTarget = Literal[
    "attention", "ffn", "layernorm", "softmax", 
    "activation", "ln_linear", "gelu_linear"
]
AttentionBackend = Literal["flash2", "flash3", "paged", "xformers", "vanilla", "triton"]
FusionBackend = Literal["cuda", "triton", "rocm", "tensorrt"]

@dataclass
class KernelFusionConfig:
    # Attention 融合配置
    attention_backend: AttentionBackend = "flash2"
    use_paged_attention: bool = True              # 是否使用 PagedAttention
    page_size: int = 16                            # PagedAttention 块大小
    enable_gqa: bool = True                        # 是否启用 GQA (Grouped Query Attention)
    num_kv_heads: Optional[int] = None             # GQA: KV heads 数量
    
    # FFN 融合配置
    fuse_ffn: bool = True                          # 是否融合 FFN (gate + up + down)
    fuse_gelu_linear: bool = True                  # GELU + Linear 融合
    fuse_silu_linear: bool = True                  # SiLU + Linear 融合
    
    # LayerNorm 融合配置
    fuse_ln: bool = True                           # 是否融合 LayerNorm
    fuse_ln_linear: bool = True                    # LayerNorm + Linear 融合
    fuse_rmsnorm: bool = True                      # 是否使用 RMSNorm 融合实现
    
    # Softmax 融合配置
    fuse_softmax: bool = True                      # 是否融合 Softmax（在线计算）
    use_online_softmax: bool = True                # 在线 Softmax（FlashAttention 风格）
    
    # 量化/稀疏融合（与 3.1/3.2 协同）
    fuse_with_quant: bool = False                  # 是否与量化融合
    quant_dtype: Optional[str] = None              # 量化类型（int4/fp8）
    fuse_with_sparse: bool = False                 # 是否与稀疏化融合
    sparse_pattern: Optional[str] = None           # 稀疏模式（n:m/unstructured）
    
    # 自定义 Kernel 配置
    custom_kernels: Dict[str, Callable] = None     # 用户自定义 Kernel 映射
    use_triton: bool = False                       # 是否优先使用 Triton Kernel
    use_cuda_graph: bool = False                   # 是否使用 CUDA Graph
    
    # 性能优化
    backend: FusionBackend = "cuda"
    enable_tf32: bool = True                       # TensorFloat-32（Ampere+）
    enable_fp16: bool = True                       # FP16 精度
    enable_bf16: bool = False                      # BF16 精度（Ampere+）
    
    # Telemetry
    profile_kernels: bool = False                  # 是否 profile 每个 Kernel
    log_fusion_stats: bool = True                  # 记录融合统计信息

class KernelFusionProtocol(ABC):
    @abstractmethod
    def fuse_attention(
        self, 
        q: torch.Tensor, 
        k: torch.Tensor, 
        v: torch.Tensor,
        config: KernelFusionConfig
    ) -> torch.Tensor:
        """融合 Attention（FlashAttention/PagedAttention）"""
        raise NotImplementedError
    
    @abstractmethod
    def fuse_ffn(
        self, 
        x: torch.Tensor,
        gate_weight: torch.Tensor,
        up_weight: torch.Tensor,
        down_weight: torch.Tensor,
        config: KernelFusionConfig
    ) -> torch.Tensor:
        """融合 FFN (gate + up + down)"""
        raise NotImplementedError
    
    @abstractmethod
    def fuse_layernorm_linear(
        self,
        x: torch.Tensor,
        ln_weight: torch.Tensor,
        ln_bias: torch.Tensor,
        linear_weight: torch.Tensor,
        config: KernelFusionConfig
    ) -> torch.Tensor:
        """融合 LayerNorm + Linear"""
        raise NotImplementedError
    
    @abstractmethod
    def register_custom_kernel(self, name: str, kernel: Callable):
        """注册自定义 Kernel"""
        raise NotImplementedError
    
    @abstractmethod
    def report(self) -> Dict[str, float]:
        """返回融合统计（加速比、显存节省、Kernel 调用次数）"""
        raise NotImplementedError
```

### 输出接口（使用示例）
```python
from sage.llm.engines.sagellm.accel.kernel_fusion import KernelFusionManager, KernelFusionConfig

# 示例 1: FlashAttention-2（最常用）
config_flash2 = KernelFusionConfig(
    attention_backend="flash2",
    use_paged_attention=True,
    page_size=16,
    enable_gqa=True,
    fuse_ffn=True,
    fuse_ln=True,
)

manager = KernelFusionManager(config_flash2)

# Attention 融合
output = manager.fuse_attention(q, k, v, config_flash2)

# FFN 融合
ffn_output = manager.fuse_ffn(x, gate_w, up_w, down_w, config_flash2)

# LayerNorm + Linear 融合
ln_linear_output = manager.fuse_layernorm_linear(x, ln_w, ln_b, linear_w, config_flash2)

report = manager.report()
# report: {"attention_speedup": 2.1, "ffn_speedup": 1.4, "memory_saving": 0.45}

# 示例 2: PagedAttention（KV Cache 分块）
config_paged = KernelFusionConfig(
    attention_backend="paged",
    use_paged_attention=True,
    page_size=16,  # 16 tokens per block
    enable_gqa=True,
    num_kv_heads=8,  # GQA: 32 Q heads, 8 KV heads
)

manager = KernelFusionManager(config_paged)
# 需要传入 KV Cache block tables
output = manager.fuse_attention(q, k_cache, v_cache, block_tables=block_tables, config=config_paged)

# 示例 3: 自定义 Triton Kernel
config_triton = KernelFusionConfig(
    use_triton=True,
    fuse_gelu_linear=True,
    backend="triton",
)

# 注册自定义 GELU+Linear 融合 Kernel
@triton.jit
def custom_gelu_linear_kernel(...):
    # 自定义 Triton 实现
    pass

manager.register_custom_kernel("gelu_linear", custom_gelu_linear_kernel)
output = manager.fuse_gelu_linear(x, linear_w, config_triton)

# 示例 4: 量化融合（与 3.1 协同）
config_quant_fused = KernelFusionConfig(
    attention_backend="flash2",
    fuse_with_quant=True,
    quant_dtype="int4",  # INT4 量化 + FlashAttention 融合
    fuse_ffn=True,
)

manager = KernelFusionManager(config_quant_fused)
# Attention 使用 INT4 量化的 K/V
output = manager.fuse_attention(q, k_int4, v_int4, config_quant_fused)

# 示例 5: 稀疏融合（与 3.2 协同）
config_sparse_fused = KernelFusionConfig(
    fuse_with_sparse=True,
    sparse_pattern="n:m",  # 2:4 稀疏
    fuse_ffn=True,
)

manager = KernelFusionManager(config_sparse_fused)
# FFN 使用稀疏权重 + 融合
ffn_output = manager.fuse_ffn(x, sparse_gate_w, sparse_up_w, sparse_down_w, config_sparse_fused)
```

______________________________________________________________________

## 核心能力设计

### 1. Attention 融合

#### 1.1 FlashAttention-2 集成
```python
# kernels/flash_attention.py
from flash_attn import flash_attn_func, flash_attn_varlen_func

class FlashAttention2:
    def forward(
        self,
        q: torch.Tensor,  # [batch, seqlen, num_heads, head_dim]
        k: torch.Tensor,
        v: torch.Tensor,
        causal: bool = True,
        softmax_scale: Optional[float] = None
    ) -> torch.Tensor:
        """FlashAttention-2 forward（内存高效 O(N) 显存）"""
        # FlashAttention 自动处理 tiling 和 recomputation
        output = flash_attn_func(
            q, k, v,
            causal=causal,
            softmax_scale=softmax_scale or (1.0 / math.sqrt(q.size(-1)))
        )
        return output
    
    def forward_varlen(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        cu_seqlens_q: torch.Tensor,  # 累积序列长度（batch）
        cu_seqlens_k: torch.Tensor,
        max_seqlen_q: int,
        max_seqlen_k: int,
    ) -> torch.Tensor:
        """变长序列 FlashAttention（适用于 batch padding 优化）"""
        return flash_attn_varlen_func(
            q, k, v,
            cu_seqlens_q, cu_seqlens_k,
            max_seqlen_q, max_seqlen_k,
            causal=True
        )
```

#### 1.2 PagedAttention 集成（vLLM）
```python
# kernels/paged_attention.py
from vllm import _custom_ops as ops

class PagedAttention:
    def __init__(self, num_heads: int, head_dim: int, page_size: int = 16):
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.page_size = page_size
    
    def forward(
        self,
        query: torch.Tensor,  # [num_tokens, num_heads, head_dim]
        key_cache: torch.Tensor,  # [num_blocks, num_heads, page_size, head_dim]
        value_cache: torch.Tensor,
        block_tables: torch.Tensor,  # [batch_size, max_num_blocks]
        context_lens: torch.Tensor,  # [batch_size]
    ) -> torch.Tensor:
        """PagedAttention forward（分块 KV Cache）"""
        output = torch.empty_like(query)
        
        # vLLM PagedAttention CUDA Kernel
        ops.paged_attention_v1(
            output,
            query,
            key_cache,
            value_cache,
            self.num_heads,
            self.head_dim,
            self.page_size,
            block_tables,
            context_lens,
            max_context_len=context_lens.max().item(),
        )
        
        return output
```

#### 1.3 GQA (Grouped Query Attention) 融合
```python
# kernels/gqa_attention.py
class GroupedQueryAttention:
    def __init__(self, num_q_heads: int, num_kv_heads: int, head_dim: int):
        assert num_q_heads % num_kv_heads == 0
        self.num_q_heads = num_q_heads
        self.num_kv_heads = num_kv_heads
        self.num_queries_per_kv = num_q_heads // num_kv_heads
        self.head_dim = head_dim
    
    def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor):
        """GQA: 多个 Q head 共享一个 KV head"""
        # q: [batch, seqlen, num_q_heads, head_dim]
        # k/v: [batch, seqlen, num_kv_heads, head_dim]
        
        # 扩展 KV 以匹配 Q heads
        k = k.repeat_interleave(self.num_queries_per_kv, dim=2)
        v = v.repeat_interleave(self.num_queries_per_kv, dim=2)
        
        # 使用 FlashAttention（自动融合）
        output = flash_attn_func(q, k, v, causal=True)
        return output
```

### 2. FFN 融合

#### 2.1 Fused Gate + Up + Down
```python
# kernels/fused_ffn.py
import triton
import triton.language as tl

@triton.jit
def fused_ffn_kernel(
    x_ptr, gate_w_ptr, up_w_ptr, down_w_ptr, out_ptr,
    N, D, H,  # N: batch*seq, D: hidden, H: intermediate
    BLOCK_SIZE: tl.constexpr,
):
    """Fused FFN Kernel (Triton)
    FFN(x) = down(silu(gate(x)) * up(x))
    """
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N
    
    # Load input
    x = tl.load(x_ptr + offsets * D, mask=mask)
    
    # Gate projection + SiLU
    gate = tl.dot(x, gate_w_ptr)  # [N, H]
    gate = gate * tl.sigmoid(gate)  # SiLU activation
    
    # Up projection
    up = tl.dot(x, up_w_ptr)  # [N, H]
    
    # Element-wise multiply
    hidden = gate * up
    
    # Down projection
    out = tl.dot(hidden, down_w_ptr)  # [N, D]
    
    # Store output
    tl.store(out_ptr + offsets * D, out, mask=mask)

class FusedFFN:
    def forward(self, x, gate_w, up_w, down_w):
        """PyTorch 包装 Triton Kernel"""
        N, D = x.shape
        H = gate_w.size(1)
        output = torch.empty(N, D, device=x.device, dtype=x.dtype)
        
        grid = lambda meta: (triton.cdiv(N, meta['BLOCK_SIZE']),)
        fused_ffn_kernel[grid](
            x, gate_w, up_w, down_w, output,
            N, D, H, BLOCK_SIZE=128
        )
        return output
```

#### 2.2 Fused GELU + Linear
```python
# kernels/fused_gelu_linear.py
@triton.jit
def gelu_linear_kernel(x_ptr, w_ptr, b_ptr, out_ptr, N, D_in, D_out, BLOCK_SIZE: tl.constexpr):
    """Fused GELU + Linear"""
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N
    
    # Load input
    x = tl.load(x_ptr + offsets * D_in, mask=mask)
    
    # GELU activation (approximation)
    # gelu(x) = 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715 * x^3)))
    x_cubed = x * x * x
    gelu_x = 0.5 * x * (1.0 + tl.tanh(0.7978845608 * (x + 0.044715 * x_cubed)))
    
    # Linear projection
    out = tl.dot(gelu_x, w_ptr) + b_ptr
    
    tl.store(out_ptr + offsets * D_out, out, mask=mask)
```

### 3. LayerNorm 融合

#### 3.1 Fused LayerNorm + Linear
```python
# kernels/fused_ln_linear.py
@triton.jit
def ln_linear_kernel(
    x_ptr, ln_w_ptr, ln_b_ptr, linear_w_ptr, out_ptr,
    N, D, eps: tl.constexpr, BLOCK_SIZE: tl.constexpr
):
    """Fused LayerNorm + Linear"""
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N
    
    # Load input
    x = tl.load(x_ptr + offsets * D, mask=mask)
    
    # LayerNorm
    mean = tl.sum(x, axis=0) / D
    var = tl.sum((x - mean) ** 2, axis=0) / D
    x_norm = (x - mean) / tl.sqrt(var + eps)
    x_norm = x_norm * ln_w_ptr + ln_b_ptr
    
    # Linear projection (fused)
    out = tl.dot(x_norm, linear_w_ptr)
    
    tl.store(out_ptr + offsets * D, out, mask=mask)
```

#### 3.2 Fused RMSNorm（Llama 风格）
```python
# kernels/fused_rmsnorm.py
@triton.jit
def rmsnorm_kernel(x_ptr, w_ptr, out_ptr, N, D, eps: tl.constexpr, BLOCK_SIZE: tl.constexpr):
    """Fused RMSNorm (Root Mean Square Normalization)"""
    pid = tl.program_id(0)
    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)
    mask = offsets < N
    
    x = tl.load(x_ptr + offsets * D, mask=mask)
    
    # RMSNorm: x / sqrt(mean(x^2) + eps) * weight
    rms = tl.sqrt(tl.sum(x * x, axis=0) / D + eps)
    x_norm = x / rms
    out = x_norm * w_ptr
    
    tl.store(out_ptr + offsets * D, out, mask=mask)
```

### 4. 量化/稀疏融合

#### 4.1 Fused INT4 Attention
```python
# kernels/fused_int4_attention.py
class FusedINT4Attention:
    """INT4 量化 KV Cache + FlashAttention 融合"""
    def forward(self, q_fp16, k_int4, v_int4, k_scales, v_scales):
        # Step 1: 反量化 K/V（在线完成，节省显存）
        k_fp16 = self._dequantize_int4(k_int4, k_scales)
        v_fp16 = self._dequantize_int4(v_int4, v_scales)
        
        # Step 2: FlashAttention（融合 softmax + matmul）
        output = flash_attn_func(q_fp16, k_fp16, v_fp16, causal=True)
        return output
    
    def _dequantize_int4(self, x_int4, scales):
        """INT4 → FP16 反量化（可融合到 Attention Kernel）"""
        return x_int4.float() * scales
```

#### 4.2 Fused Sparse FFN
```python
# kernels/fused_sparse_ffn.py
class FusedSparseFFN:
    """稀疏权重 FFN 融合（2:4 结构化稀疏）"""
    def forward(self, x, sparse_gate_w, sparse_up_w, sparse_down_w):
        # 使用 NVIDIA Sparse Tensor Core（Ampere+）
        # 2:4 稀疏矩阵乘法自动加速
        gate = torch.sparse.mm(x, sparse_gate_w)
        gate = F.silu(gate)
        
        up = torch.sparse.mm(x, sparse_up_w)
        hidden = gate * up
        
        out = torch.sparse.mm(hidden, sparse_down_w)
        return out
```

______________________________________________________________________

## 与其他模块协同

### 1. 与量化模块（3.1）集成
```python
# 量化 KV Cache + FlashAttention 融合
from sage.llm.engines.sagellm.accel.quantization import QuantizationManager
from sage.llm.engines.sagellm.accel.kernel_fusion import KernelFusionManager

quant_mgr = QuantizationManager(QuantConfig(kv_cache_dtype="fp8"))
fusion_mgr = KernelFusionManager(KernelFusionConfig(
    attention_backend="flash2",
    fuse_with_quant=True,
    quant_dtype="fp8"
))

# KV Cache 以 FP8 存储，Attention 时在线反量化 + FlashAttention
output = fusion_mgr.fuse_attention(q, k_fp8, v_fp8, config)
```

### 2. 与稀疏化模块（3.2）集成
```python
# 稀疏权重 + FFN 融合
from sage.llm.engines.sagellm.accel.sparsity import SparsityManager

sparse_mgr = SparsityManager(SparsityConfig(pattern="n:m", n=2, m=4))
fusion_mgr = KernelFusionManager(KernelFusionConfig(
    fuse_ffn=True,
    fuse_with_sparse=True,
    sparse_pattern="n:m"
))

# 稀疏 FFN 权重 + SiLU 融合
ffn_output = fusion_mgr.fuse_ffn(x, sparse_gate, sparse_up, sparse_down, config)
```

### 3. 与 KV Pool（2.2）集成
```python
# PagedAttention + KV Pool 协同
from sage.llm.engines.sagellm.kv_runtime.kv_pool import KVPool

kv_pool = KVPool(max_blocks=1000, block_size=16)
fusion_mgr = KernelFusionManager(KernelFusionConfig(
    attention_backend="paged",
    page_size=16
))

# KV Pool 提供 block tables，PagedAttention 使用分块 KV
block_tables = kv_pool.get_block_tables(request_ids)
output = fusion_mgr.fuse_attention(q, k_cache, v_cache, block_tables=block_tables, config=config)
```

______________________________________________________________________

## 国产硬件 Kernel 融合适配方案

> **重要**：sageLLM 定位为"面向国产算力的推理引擎"，Kernel 融合模块需要为每种国产硬件提供对应的高性能实现。

### 昇腾 910B Kernel 适配 (P0)

**SDK 要求**：CANN >= 7.0.0, torch_npu >= 2.1.0

**Attention 替代方案**：
| CUDA 实现 | 昇腾替代 | 说明 |
|-----------|---------|------|
| FlashAttention-2 | FlashAttention-Ascend | 华为适配版，CANN 7.0+ 内置 |
| PagedAttention | NPU PagedAttention | vLLM-Ascend 项目提供 |
| xFormers | 不支持 | 使用 FlashAttention-Ascend |

**Kernel 实现示例**：
```python
# kernels/ascend_attention.py
import torch_npu

class AscendFlashAttention:
    """昇腾 FlashAttention 适配"""
    def forward(self, q, k, v, causal=True):
        # 使用 CANN 内置 FlashAttention
        output = torch_npu.npu_fusion_attention(
            q, k, v,
            head_num=q.size(2),
            input_layout="BSND",
            scale=1.0 / math.sqrt(q.size(-1)),
            pre_tokens=65536 if causal else 0,
            next_tokens=0 if causal else 65536,
        )
        return output[0]
```

**FFN 融合**：
```python
# kernels/ascend_fused_ffn.py
class AscendFusedFFN:
    """昇腾 Fused FFN（使用 Ascend C 自定义算子）"""
    def forward(self, x, gate_w, up_w, down_w):
        # 方案 1: 使用 torch_npu 融合算子
        hidden = torch_npu.npu_silu(torch.matmul(x, gate_w)) * torch.matmul(x, up_w)
        output = torch.matmul(hidden, down_w)
        return output
        
        # 方案 2: 自定义 Ascend C Kernel（高级）
        # return ascend_fused_ffn_kernel(x, gate_w, up_w, down_w)
```

**已知限制**：
- FlashAttention-Ascend 不支持 dropout（推理场景无影响）
- GQA 需要 CANN >= 7.1
- 自定义 Triton Kernel 不可用（需用 Ascend C 替代）

### 寒武纪 MLU370/590 Kernel 适配 (P0)

**SDK 要求**：CNToolkit >= 3.0.0, torch_mlu >= 1.10.0

**Attention 替代方案**：
| CUDA 实现 | 寒武纪替代 | 说明 |
|-----------|-----------|------|
| FlashAttention-2 | MLU FlashAttention | torch_mlu 内置 |
| PagedAttention | MLU PagedAttention | 需自行适配 |

**Kernel 实现示例**：
```python
# kernels/mlu_attention.py
import torch_mlu

class MLUFlashAttention:
    """寒武纪 FlashAttention 适配"""
    def forward(self, q, k, v, causal=True):
        # torch_mlu 提供 flash_attention 封装
        output = torch_mlu.ops.flash_attention(
            q, k, v,
            is_causal=causal,
            scale=1.0 / math.sqrt(q.size(-1)),
        )
        return output
```

**FFN 融合**：使用 BANG 语言编写自定义算子，或通过 MagicMind 图优化自动融合。

### 海光 DCU Z100 Kernel 适配 (P1)

**SDK 要求**：ROCm >= 5.4.0

**Attention 替代方案**：
| CUDA 实现 | 海光替代 | 说明 |
|-----------|---------|------|
| FlashAttention-2 | FlashAttention-ROCm | 官方 ROCm 移植 |
| PagedAttention | vLLM-ROCm | 社区适配 |

**优势**：基于 ROCm/HIP，与 CUDA 高度兼容，大部分 Kernel 可直接移植。

```python
# kernels/rocm_attention.py
# 可直接使用 flash-attention ROCm 版本
from flash_attn_rocm import flash_attn_func

class ROCmFlashAttention:
    def forward(self, q, k, v, causal=True):
        return flash_attn_func(q, k, v, causal=causal)
```

### 昆仑芯 R300 Kernel 适配 (P1)

**SDK 要求**：XPU SDK >= 2.6.0

**适配策略**：
- 主要通过 PaddlePaddle 接入
- PyTorch 支持相对不成熟
- 建议使用 Paddle 后端或等待 torch-xpu 成熟

### Backend 统一抽象

```python
# core/kernel_fusion/backend_registry.py
class KernelFusionBackendRegistry:
    """Kernel 融合后端注册表"""
    _backends = {}
    
    @classmethod
    def register(cls, name: str, backend_cls):
        cls._backends[name] = backend_cls
    
    @classmethod
    def get(cls, name: str):
        if name not in cls._backends:
            raise ValueError(f"Unsupported backend: {name}")
        return cls._backends[name]()
    
    @classmethod
    def detect_backend(cls) -> str:
        """自动检测可用后端"""
        try:
            import torch_npu
            if torch_npu.npu.is_available():
                return "ascend"
        except: pass
        try:
            import torch_mlu
            if torch_mlu.mlu.is_available():
                return "mlu"
        except: pass
        try:
            import torch
            if torch.cuda.is_available():
                # 检测是否为海光 DCU
                if "AMD" in torch.cuda.get_device_name(0) or "DCU" in torch.cuda.get_device_name(0):
                    return "rocm"
                return "cuda"
        except: pass
        return "cpu"

# 注册各后端
KernelFusionBackendRegistry.register("cuda", CUDAKernelFusion)
KernelFusionBackendRegistry.register("ascend", AscendKernelFusion)
KernelFusionBackendRegistry.register("mlu", MLUKernelFusion)
KernelFusionBackendRegistry.register("rocm", ROCmKernelFusion)
```

### 国产硬件 Kernel 融合路线图

| 周次 | 昇腾 910B | 寒武纪 MLU | 海光 DCU | 昆仑芯 |
|------|-----------|-----------|---------|--------|
| Week 11 | FlashAttention 集成 | FlashAttention 集成 | - | - |
| Week 12 | Fused FFN | Fused FFN | FlashAttention | - |
| Week 13 | PagedAttention | PagedAttention | Fused FFN | 评估 |
| Week 14 | 性能调优 | 性能调优 | 性能调优 | - |

______________________________________________________________________

## 性能指标与验证

### 目标指标
| 指标 | 目标 | 测量方法 |
|------|------|---------|
| Attention 加速比 | 2-3x (FlashAttention) | TTFT/TPOT 对比 vanilla attention |
| FFN 加速比 | 1.3-1.5x | FFN forward 时间对比 |
| 显存节省 | 30-50% (FlashAttention) | Peak memory 对比 |
| Kernel 调用次数 | -50% | CUDA profiler 统计 |
| 端到端延迟 | -20-40% | 完整 forward 时间 |

### Benchmark 设计
```python
# benchmarks/kernel_fusion_bench.py
def benchmark_kernel_fusion(model_path: str, config: KernelFusionConfig):
    """Kernel 融合 Benchmark"""
    # 1. Baseline: 无融合
    model_vanilla = load_model(model_path, fuse_kernels=False)
    vanilla_metrics = measure_performance(model_vanilla, test_inputs)
    
    # 2. Fused: 融合 Kernel
    model_fused = load_model(model_path, fuse_kernels=True, fusion_config=config)
    fused_metrics = measure_performance(model_fused, test_inputs)
    
    # 3. 对比报告
    report = {
        "attention_speedup": vanilla_metrics["attn_time"] / fused_metrics["attn_time"],
        "ffn_speedup": vanilla_metrics["ffn_time"] / fused_metrics["ffn_time"],
        "memory_saving": (vanilla_metrics["peak_mem"] - fused_metrics["peak_mem"]) / vanilla_metrics["peak_mem"],
        "kernel_calls_reduction": (vanilla_metrics["kernel_calls"] - fused_metrics["kernel_calls"]) / vanilla_metrics["kernel_calls"],
        "e2e_speedup": vanilla_metrics["total_time"] / fused_metrics["total_time"],
    }
    return report
```

______________________________________________________________________

## 实现路线图

### Week 11: 核心融合实现
- [ ] FlashAttention-2/3 集成
- [ ] PagedAttention 集成
- [ ] Fused FFN (Triton)
- [ ] Fused LayerNorm + Linear

### Week 12: 高级融合
- [ ] GQA (Grouped Query Attention)
- [ ] Fused RMSNorm
- [ ] Fused GELU + Linear
- [ ] 自定义 Triton Kernel 注册

### Week 13: 联合优化
- [ ] 量化融合（INT4/FP8 Attention）
- [ ] 稀疏融合（Sparse FFN）
- [ ] Benchmark 与性能验证
- [ ] CUDA Graph 优化

______________________________________________________________________

## 参考资源

### 论文
1. **FlashAttention**: "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness" (2022)
2. **FlashAttention-2**: "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning" (2023)
3. **PagedAttention**: "Efficient Memory Management for Large Language Model Serving with PagedAttention" (vLLM, 2023)
4. **GQA**: "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints" (2023)

### 代码参考
- FlashAttention: https://github.com/Dao-AILab/flash-attention
- vLLM PagedAttention: https://github.com/vllm-project/vllm
- xFormers: https://github.com/facebookresearch/xformers
- Triton: https://github.com/openai/triton
- FasterTransformer: https://github.com/NVIDIA/FasterTransformer

______________________________________________________________________

## FAQ

**Q: FlashAttention 和 PagedAttention 有什么区别？**  
A: FlashAttention 关注显存效率（O(N) vs O(N²)），PagedAttention 关注 KV Cache 管理（分块存储）。两者可以结合：PagedAttention + FlashAttention Kernel。

**Q: Triton Kernel 比 CUDA Kernel 慢吗？**  
A: 取决于实现质量。简单融合（如 GELU+Linear）Triton 性能接近手写 CUDA。复杂融合（如 FlashAttention）CUDA 更优。Triton 优势是易于开发和维护。

**Q: Kernel 融合会影响精度吗？**  
A: 通常不会。FlashAttention 是精确实现（数值等价于 vanilla attention）。融合只是改变计算顺序和内存访问模式，不改变数学逻辑。

**Q: GQA 和 MQA 有什么区别？**  
A: MQA (Multi-Query Attention) 是极端情况：所有 Q heads 共享 1 组 KV heads。GQA (Grouped Query Attention) 是折中：多个 Q heads 共享 1 组 KV heads（如 32 Q heads / 8 KV heads）。

**Q: 如何选择 Attention Backend？**  
A: 
- 长上下文（>4K）：FlashAttention-2/3（显存高效）
- 推理服务（动态 batch）：PagedAttention（KV Cache 管理）
- 简单实验：vanilla attention（易于调试）
- 自定义需求：Triton（易于魔改）
