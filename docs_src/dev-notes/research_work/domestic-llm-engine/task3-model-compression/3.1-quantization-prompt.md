# 小方向 3.1：量化优化（Quantization）

> **模块编号**: 3.1 = Phase 3（加速优化）第 1 个子模块  \
> **Git Repo**: `sageLLM-quantization` | **Priority**: P1 | **Phase**: Week 11-12

______________________________________________________________________

## 模块定位

### 核心职责
- 为 LLM 推理提供统一的权重量化与激活量化方案，覆盖 INT8 / INT4 / FP8 / 混合精度（W8A16、W8A8）
- 支持 PTQ（AWQ、GPTQ、SmoothQuant）、QAT、KV Cache 量化，以及面向 Speculative Decoding 的 Draft/Verify 双路兼容
- 自动匹配硬件 Kernel（CUDA/TensorRT/ROCm）和 Kernel Fusion（与 3.2 融合模块协同），保证延迟、吞吐、显存的平衡

### 为什么需要独立模块
- **性能核心**：量化决定了吞吐上限和显存占用，直接影响并发度和成本
- **跨组件依赖**：需要与调度器 IR（2.4）、KV Pool（2.2）、前缀缓存（2.1）协同，必须统一接口
- **硬件异构**：GPU 架构、Kernel 后端差异巨大，需要抽象层屏蔽细节
- **研究价值**：PTQ/QAT/激活裁剪/平滑策略可独立演进，利于论文与实验

### Baseline 参考
1. **vLLM FP8 / INT4 Support**: `quantization.py` + `cutlass_kernels/*`（W4A16、W8A16）
2. **AWQ (Activation-aware Weight Quantization)**: 权重感知激活，减少精度损失
3. **GPTQ**: 分层感知的后训练量化，适合大模型
4. **SmoothQuant**: 激活平滑减少 outlier，配合 W8A8/FP8
5. **ZeroQuant / QLoRA**: LoRA + 4bit 适配，提示参数高效微调

______________________________________________________________________

## 技术规格

### 输入接口（Quantization Protocol）
```python
# core/protocols/quantization.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Literal, Optional, Dict

QuantDType = Literal["int8", "int4", "fp8", "w8a16", "w8a8", "nf4"]
CalibrationMethod = Literal["awq", "gptq", "smoothquant", "percentile", "minmax"]
KVQuantType = Literal["none", "int8", "fp8"]

@dataclass
class QuantConfig:
    weight_dtype: QuantDType              # 权重量化类型
    activation_dtype: QuantDType          # 激活量化类型（执行态）
    calibration: CalibrationMethod        # PTQ 校准方法
    group_size: int = 128                 # 分组量化大小（典型 128/32）
    symmetric: bool = True                # 对称/非对称
    per_channel: bool = True              # 是否按通道量化
    kv_cache_dtype: KVQuantType = "none" # KV Cache 量化
    clip_ratio: float = 0.0               # Smooth/Percentile 裁剪比例（0 表示关闭）
    enable_qat: bool = False              # 是否启用 QAT
    fuse_ln: bool = True                  # 与 LayerNorm 融合（交由 3.2 模块执行）
    fuse_qdq: bool = True                 # 是否插入 Q/DQ 并尝试融合
    backend: Literal["cuda", "tensorrt", "rocm"] = "cuda"
    priority: Literal["latency", "throughput", "memory"] = "throughput"
    draft_mode: bool = False              # 与 3.3 投机解码的 Draft 路兼容
    scale_precision: Literal["fp16", "fp32"] = "fp16"  # 缩放因子精度

class QuantizationProtocol(ABC):
    @abstractmethod
    def calibrate(self, model_path: str, dataset_path: str, config: QuantConfig) -> str:
        """执行校准，返回量化模型产物路径"""
        raise NotImplementedError

    @abstractmethod
    def load_quantized(self, model_path: str, config: QuantConfig):
        """加载量化模型并返回可供推理的权重/Kernel 绑定信息"""
        raise NotImplementedError

    @abstractmethod
    def select_kernel(self, config: QuantConfig) -> str:
        """根据 dtype/backend/硬件选择最佳 Kernel 名称"""
        raise NotImplementedError

    @abstractmethod
    def report(self) -> Dict[str, float]:
        """返回校准/推理的关键指标（精度、延迟、显存、放大系数）"""
        raise NotImplementedError
```

### 输出接口（使用示例）
```python
from sage.llm.sagellm.accel.quantization import QuantizationManager, QuantConfig

config = QuantConfig(
    weight_dtype="int4",
    activation_dtype="w8a16",
    calibration="awq",
    group_size=128,
    kv_cache_dtype="fp8",
    fuse_ln=True,
    fuse_qdq=True,
    backend="cuda",
    priority="throughput",
)

manager = QuantizationManager(config)
# 1) 校准（PTQ）
artifact = manager.calibrate(
    model_path="/models/Qwen2-7B",
    dataset_path="/data/calib/pile-1k.jsonl",
)
# 2) 加载量化模型 + Kernel 绑定
runtime = manager.load_quantized(artifact)
# 3) 与 UnifiedInferenceClient (Control Plane) 集成
client = UnifiedInferenceClient.create()
resp = client.chat([
    {"role": "user", "content": "你好，介绍一下量化的优势"}
], runtime=runtime)
# 确保所有推理走 Control Plane，不允许直接 vLLM 实例化
```

> **禁止**：直接 `from vllm import LLM` / `LLM(model=...)`，必须通过 Control Plane。

______________________________________________________________________

## 设计与实现

### 组件分层
- **Protocol 层**：`QuantizationProtocol` 定义校准、加载、Kernel 选择、指标上报
- **Algorithm 层**：AWQ、GPTQ、SmoothQuant、Percentile、MinMax、QAT Hook
- **Runtime 层**：Kernel Registry（CUDA/TensorRT/ROCm）、Q/DQ 插入、KV Cache 量化
- **协同层**：与 3.2 Kernel Fusion、2.4 Scheduler IR（量化感知调度）、2.2 KV Pool（KV dtype）

### 核心数据结构
- `QuantConfig`：量化策略配置（dtype、分组、校准方法、KV 量化）
- `CalibrationRecord`：校准统计（激活分布、裁剪阈值、scale/zero_point）
- `KernelBinding`：量化权重与 Kernel 对应关系（算子 → Kernel 名称/属性）
- `KVQuantPlan`：KV Cache 的 dtype/分块策略（对齐 block_size = 16/32，与 2.2/2.1 对齐）

### 算法流程（PTQ: AWQ + SmoothQuant 示例）
1. **样本采集**：加载 512~1024 条代表性校准样本，生成激活统计
2. **激活平滑**：SmoothQuant 计算通道级 `alpha`，抑制 outlier
3. **权重重标定 (AWQ)**：在平滑激活下最小化重构误差，生成分组 scale/zero_point
4. **裁剪与量化**：按 `group_size` / 对称量化，生成 INT4/INT8 权重
5. **KV 策略**：KV 选择 FP8 (E4M3) 或 INT8，对齐 block_size，写入 `KVQuantPlan`
6. **Kernel 选择**：依据 dtype/backend/硬件，选择最佳 Kernel（CUTLASS W4A16、FP8 WMMA 等）
7. **产物固化**：导出量化权重 + 校准记录 + KernelBinding 元数据

### 算法流程（QAT 简要）
1. 插入 `FakeQuant` / `Observer`（按 `QuantConfig`）
2. 低学习率微调 1-3 epoch，保持 LoRA/Adapter 兼容
3. 导出静态量化图，复用 PTQ 的 KernelBinding 路径

### Kernel 选择策略
- **INT4 / W4A16**：优先 CUTLASS/TC GEMM，TensorRT INT4 若硬件支持
- **INT8 / W8A8**：TensorRT INT8 / CUTLASS W8A8；激活保持 FP16 可选
- **FP8**：E4M3/E5M2 动态选择；LayerNorm 保持 FP16 以稳态精度
- **KV Cache**：优先 FP8；若显存紧张可 INT8，但需评估延迟回退

### 运行时策略
- **调度协同**：2.4 Scheduler IR 增加量化感知字段（dtype、kernel），便于批处理与融合排序
- **内存协同**：2.2 KV Pool 记录 kv_cache_dtype；与 2.1 前缀缓存共享量化后的 KV block
- **融合协同**：3.2 Kernel Fusion 读取 KernelBinding，生成融合子图（Q/DQ 融合、Ln+Matmul 融合）
- **投机协同**：3.3 Draft 路可用较轻量 dtype（如 int8），Verify 路用更高精度（fp16/fp8）

______________________________________________________________________

## 性能与质量目标

| 指标 | 目标 | Baseline (vLLM) | 说明 |
| --- | --- | --- | --- |
| 延迟 (prefill) | -25% | 0% | INT4/W8A16 + 融合 Kernel |
| 吞吐 (tok/s) | +20% | 0% | A100 80GB, batch=8 |
| 显存 (weights) | -50% | 0% | INT4 相比 FP16 |
| 精度 (ΔBLEU/MMLU) | ≤ -0.5 | 0 | 校准样本 1k，AWQ+Smooth |
| KV 显存 | -30% | 0 | KV FP8/INT8 vs FP16 |
| 调度开销 | +5% 以内 | 0 | IR 增量字段 |

______________________________________________________________________

## API 与伪代码

### QuantizationManager（核心入口）
```python
class QuantizationManager:
    def __init__(self, config: QuantConfig):
        self.config = config
        self.protocol = self._select_protocol(config.calibration)
        self.kernel = None
        self.report_metrics: Dict[str, float] = {}

    def calibrate(self, model_path: str, dataset_path: str) -> str:
        artifact = self.protocol.calibrate(model_path, dataset_path, self.config)
        self.report_metrics = self.protocol.report()
        return artifact

    def load_quantized(self, artifact_path: str):
        self.kernel = self.protocol.select_kernel(self.config)
        runtime = self.protocol.load_quantized(artifact_path, self.config)
        return runtime

    def select_kernel(self) -> str:
        return self.protocol.select_kernel(self.config)

    def report(self) -> Dict[str, float]:
        return self.report_metrics
```

### Protocol 选择
```python
def _select_protocol(calibration: CalibrationMethod):
    if calibration == "awq":
        return AWQProtocol()
    if calibration == "gptq":
        return GPTQProtocol()
    if calibration == "smoothquant":
        return SmoothQuantProtocol()
    if calibration in ["percentile", "minmax"]:
        return BasicPTQProtocol()
    raise ValueError(f"Unsupported calibration: {calibration}")
```

### Kernel Registry（示例）
```python
KERNEL_REGISTRY = {
    ("int4", "cuda", "latency"): "cutlass_w4a16_kernel",
    ("int4", "cuda", "throughput"): "cutlass_w4a16_batched",
    ("int8", "cuda", "throughput"): "tensorrt_int8_kernel",
    ("fp8", "cuda", "latency"): "wmma_fp8_e4m3",
    ("int8", "rocm", "throughput"): "hip_int8_kernel",
}

def select_kernel(config: QuantConfig) -> str:
    key = (config.weight_dtype, config.backend, config.priority)
    return KERNEL_REGISTRY.get(key, "fallback_fp16_kernel")
```

> 注意：`fallback_fp16_kernel` 仅用于显式配置，**禁止隐藏式 fallback**。

### KV Cache 量化计划
```python
@dataclass
class KVQuantPlan:
    dtype: KVQuantType
    block_size: int
    group_size: int
    scale_precision: Literal["fp16", "fp32"]


def build_kv_plan(config: QuantConfig) -> KVQuantPlan:
    if config.kv_cache_dtype == "none":
        return KVQuantPlan("none", 16, 0, "fp16")
    if config.kv_cache_dtype == "fp8":
        return KVQuantPlan("fp8", 16, 128, config.scale_precision)
    if config.kv_cache_dtype == "int8":
        return KVQuantPlan("int8", 16, 128, config.scale_precision)
    raise ValueError("Unsupported kv_cache_dtype")
```

### 校准主流程（AWQ + SmoothQuant）
```python
def calibrate_awq_smooth(model, calib_loader, config: QuantConfig):
    stats = collect_activation_stats(model, calib_loader)
    alphas = compute_smooth_alphas(stats, config.clip_ratio)
    smooth_model = apply_smooth(model, alphas)
    weight_scales = awq_optimize(smooth_model, calib_loader, config.group_size)
    quant_model = quantize_weights(smooth_model, weight_scales, config)
    kv_plan = build_kv_plan(config)
    save_artifact(quant_model, kv_plan, "artifact_dir")
    return "artifact_dir"
```

______________________________________________________________________

## 与其他子模块的依赖关系

- **上游输入**：
  - 训练/预训练模型权重（FP16/BF16）
  - 校准数据集（文本语料）
- **下游输出**：
  - 量化权重产物 + KernelBinding 元数据
  - Scheduler IR 增量字段（量化 dtype、kernel 名称）
  - KVQuantPlan（供 2.2 KV Pool / 2.1 前缀缓存共享）
- **跨 Phase 协同**：
  - Phase 2: Scheduler IR、KV Pool、前缀缓存
  - Phase 3: Kernel Fusion (3.2)、Speculative Decoding (3.3)、Sparse/FlashAttention (3.4/3.5)

______________________________________________________________________

## CLI / Control Plane 使用

> 所有引擎管理必须通过 Control Plane，禁止直接启动 vLLM。

```bash
# 1) 启动量化引擎（示例：INT4 + KV FP8）
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --quantization int4 \
  --kv-quant fp8 \
  --backend cuda

# 2) 查看引擎
sage llm engine list

# 3) 运行推理（客户端自动走 Control Plane）
python - <<'PY'
from sage.llm import UnifiedInferenceClient
client = UnifiedInferenceClient.create()
resp = client.chat([{ "role": "user", "content": "介绍一下量化" }])
print(resp)
PY

# 4) 校准（离线 PTQ）
sage-dev quant calibrate \
  --model /models/Qwen2-7B \
  --dataset /data/calib/pile-1k.jsonl \
  --method awq \
  --weight-dtype int4 \
  --activation-dtype w8a16 \
  --kv-quant fp8

# 5) 指标上报
auri quant report --artifact /artifacts/qwen2-7b-awq-int4
```

______________________________________________________________________

## 开发计划（Week 11-12）

| 周次 | 里程碑 | 说明 |
| --- | --- | --- |
| 11.1 | 完成 Protocol & Config | QuantConfig, Protocol 接口, KVPlan 定义 |
| 11.2 | 实现 AWQ + SmoothQuant | 支持 group_size 128/32，裁剪，KV FP8 |
| 11.3 | Kernel Registry | CUDA/TensorRT/ROCm 映射，选择策略 |
| 11.4 | 集成调度/融合 | 2.4 IR 增量字段，3.2 融合接口对接 |
| 12.1 | 精度回归 | MMLU/CEval 回归，Δ≤ -0.5 |
| 12.2 | 性能回归 | A100 延迟/吞吐/显存对比 vLLM |

______________________________________________________________________

## 验收标准（Definition of Done）
- ✅ 支持 INT8 / INT4 / FP8 / W8A16 / W8A8；KV Cache FP8/INT8 可选
- ✅ 完整 PTQ 流程（AWQ、SmoothQuant、GPTQ 至少一种实现）
- ✅ Kernel Registry 覆盖 CUDA/TensorRT（ROCm 如硬件可用）
- ✅ Scheduler IR/Kernel Fusion/KV Pool 集成完成
- ✅ 精度回归：ΔBLEU/MMLU ≤ -0.5；性能对比 vLLM 达成表格目标
- ✅ CLI 覆盖：calibrate / engine start / report

______________________________________________________________________

## 风险与缓解
- **精度下降**：部分层 outlier 严重 → 提升 clip_ratio / 对异常层保持 FP16
- **Kernel 缺失**：硬件不支持 INT4 → 回退到 INT8/FP8（需显式配置，不做隐式 fallback）
- **KV 误差放大**：长上下文时 FP8 漂移 → 对关键层 KV 维持 FP16，或调高 scale_precision
- **调度复杂度**：IR 增加字段导致排序开销 → 控制在 +5% 以内

______________________________________________________________________

## 国产硬件量化适配方案

> **重要**：sageLLM 定位为"面向国产算力的推理引擎"，量化模块需要同时支持 CUDA 和国产硬件后端。

### 昇腾 910B 量化适配

**支持的量化格式**：
| 格式 | 支持状态 | 说明 |
|------|----------|------|
| INT8 | ✅ 原生支持 | CANN AOL 提供高效 INT8 GEMM |
| INT4 | ⚠️ 有限支持 | 需使用 packed INT4（2个INT4打包为INT8） |
| FP16 | ✅ 原生支持 | 默认精度 |
| BF16 | ✅ CANN 7.0+ | 910B3 架构新增 |

**Kernel 实现**：
```python
# backend: "ascend"
class AscendQuantKernel:
    """昇腾量化 Kernel 适配"""
    def select_kernel(self, config: QuantConfig) -> str:
        if config.weight_dtype == "int8":
            return "aclnn_matmul_int8"  # CANN AOL INT8 GEMM
        elif config.weight_dtype == "int4":
            return "aclnn_matmul_int4_packed"  # Packed INT4
        else:
            return "aclnn_matmul_fp16"
```

**已知限制**：
- INT4 需要 group_size=32 或 64（不支持 128）
- KV Cache FP8 需要 CANN >= 7.1

### 寒武纪 MLU370/590 量化适配

**支持的量化格式**：
| 格式 | 支持状态 | 说明 |
|------|----------|------|
| INT8 | ✅ 原生支持 | MagicMind INT8 优化 |
| INT4 | ⚠️ 需 MagicMind | 通过图优化实现 |
| FP16 | ✅ 原生支持 | 默认精度 |

**Kernel 实现**：
```python
# backend: "mlu"
class MLUQuantKernel:
    """寒武纪量化 Kernel 适配"""
    def calibrate_with_magicmind(self, model_path: str, config: QuantConfig):
        """使用 MagicMind 进行量化校准"""
        # MagicMind 提供 PTQ 能力
        mm_config = {
            "precision": "int8" if config.weight_dtype == "int8" else "fp16",
            "calibration_data": config.calibration_dataset,
        }
        return magicmind.quantize(model_path, mm_config)
```

### 海光 DCU Z100 量化适配

**支持的量化格式**：
| 格式 | 支持状态 | 说明 |
|------|----------|------|
| INT8 | ✅ MIOpen | ROCm 生态 |
| FP8 | ⚠️ 有限支持 | 需检查具体型号 |
| FP16 | ✅ 原生支持 | 默认精度 |

**Kernel 实现**：基于 ROCm/HIP，与 CUDA 高度兼容，可复用大部分 CUTLASS 逻辑。

### Backend 扩展

```python
# core/protocols/quantization.py 扩展
@dataclass
class QuantConfig:
    # ... existing fields ...
    backend: Literal["cuda", "tensorrt", "rocm", "ascend", "mlu", "kunlun"] = "cuda"
    
    def get_available_backends(self) -> List[str]:
        """检测可用后端"""
        backends = []
        try:
            import torch
            if torch.cuda.is_available():
                backends.append("cuda")
        except: pass
        try:
            import torch_npu
            if torch_npu.npu.is_available():
                backends.append("ascend")
        except: pass
        try:
            import torch_mlu
            if torch_mlu.mlu.is_available():
                backends.append("mlu")
        except: pass
        return backends
```

______________________________________________________________________

## FAQ
1. **为什么首选 AWQ + SmoothQuant？** 组合能同时降低激活 outlier 与权重误差，INT4 仍保持较好精度。
2. **KV 必须量化吗？** 非必须。长上下文下 FP8 可节省约 30% 显存；若出现漂移可仅权重量化。
3. **Speculative Decoding 如何配合？** Draft 路用 int8/kv int8，Verify 路用 fp16/fp8，降低验证开销。
4. **与 Kernel Fusion 的关系？** 量化模块产出的 KernelBinding 提供算子 dtype，方便 3.2 进行 Q/DQ + Ln 融合。
5. **是否支持 QAT？** 支持基本 FakeQuant QAT，主要用于精度敏感场景；PTQ 仍是主路径。
6. **精度回退策略？** 需要显式配置回退 dtype，不允许自动 silent fallback。
7. **国产硬件量化精度如何？** 昇腾 INT8 精度与 CUDA 基本一致；INT4 需要额外校验，推荐在昇腾上使用 W8A16。

______________________________________________________________________

## 参考与对标
- vLLM Quantization (FP8/INT4) 实现与性能报告
- AWQ: Activation-Aware Weight Quantization for LLMs
- GPTQ: Post-Training Quantization for LLMs
- SmoothQuant: Accurate and Efficient LLM Inference via Activation Smoothing
- TensorRT-LLM INT4/FP8 Kernel 文档

______________________________________________________________________

## 与 Phase 2 / Phase 3 其他模块的接口清单
- `QuantConfig` → 2.4 Scheduler IR 扩展字段：`dtype`, `kernel`，供批处理/融合排序
- `KVQuantPlan` → 2.2 KV Pool & 2.1 Prefix Cache：共享 kv_cache_dtype, block_size
- `KernelBinding` → 3.2 Kernel Fusion：融合规则输入，决定是否 Q/DQ 内联
- `draft_mode` → 3.3 Speculative Decoding：区分 Draft/Verify 路径的 dtype
- `priority` → 2.4 调度决策：latency/throughput/memory 约束

______________________________________________________________________

## 端到端集成示例（量化 + 调度 + 融合 + KV）
```python
from sage.llm.sagellm.accel.quantization import QuantConfig, QuantizationManager
from sage.llm.sagellm.scheduler_ir import BaseSchedulerIR
from sage.llm.sagellm.kv_runtime.kv_pool import GPUKVPool
from sage.llm.sagellm.prefix_reuse import RadixPrefixCache
from sage.llm import UnifiedInferenceClient

# 1) 量化配置
qconfig = QuantConfig(
    weight_dtype="int4",
    activation_dtype="w8a16",
    calibration="awq",
    kv_cache_dtype="fp8",
    group_size=128,
    priority="throughput",
)
manager = QuantizationManager(qconfig)
artifact = manager.calibrate("/models/Qwen2-7B", "/data/calib/pile-1k.jsonl")
runtime = manager.load_quantized(artifact)

# 2) 调度 IR 集成
ir = BaseSchedulerIR()
ir.set_quantization(runtime.kernel, qconfig.weight_dtype)

# 3) KV Pool + 前缀缓存集成
kv_pool = GPUKVPool(total_blocks=20000, block_size=16, kv_dtype=qconfig.kv_cache_dtype)
prefix_cache = RadixPrefixCache(max_blocks=4000)

# 4) 统一推理客户端（Control Plane）
client = UnifiedInferenceClient.create()
resp = client.chat([
    {"role": "user", "content": "介绍一下量化优化"}
], runtime=runtime)
print(resp)
```

______________________________________________________________________

## 度量与上报
- **精度**：ΔMMLU / ΔCEval / ΔBLEU，目标 ≤ -0.5
- **性能**：延迟、吞吐、显存（权重 + KV），对比 vLLM FP16 基线
- **稳定性**：长上下文 (>8k) 精度漂移，记录 KV 漂移率
- **兼容性**：硬件矩阵（A100/H100/4090/MI300）覆盖率

______________________________________________________________________

## 结束语
本模块提供统一的量化抽象、校准与内核选择能力，是 Phase 3 加速链路的起点，为后续算子融合、投机解码、稀疏与 FlashAttention 奠定性能与显存基础。
