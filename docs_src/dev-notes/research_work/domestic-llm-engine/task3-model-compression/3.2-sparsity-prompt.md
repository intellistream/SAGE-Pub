# 小方向 3.2：稀疏化优化 (Sparsity Optimization)

> **模块编号**: 3.2 = Phase 3（模型压缩与加速）第 2 个子模块  
> **Git Repo**: `sageLLM-sparsity` | **Priority**: P1 | **Phase**: Week 11-12

______________________________________________________________________

## 模块定位

### 核心职责
- 为 LLM 推理提供统一的权重稀疏化、激活稀疏化和动态稀疏化方案
- 支持结构化稀疏（N:M sparsity, 2:4/4:8）和非结构化稀疏（幅度剪枝、梯度剪枝）
- 实现稀疏感知的推理 Kernel（cuSPARSE, Triton, TensorRT），支持稀疏矩阵格式（CSR/COO/BSR）
- 与量化模块（3.1）协同实现稀疏+量化联合优化（Sparse + INT4/FP8）

### 为什么需要独立模块
- **性能核心**：稀疏化可将 FFN/Attention 计算量减少 50-80%，直接提升吞吐量
- **硬件适配**：NVIDIA Ampere/Hopper 支持 2:4 结构化稀疏硬件加速，需要专用 Kernel
- **跨组件协同**：需要与量化（3.1）、Kernel 融合（3.4）、调度器 IR（2.4）协同
- **研究价值**：Magnitude pruning、Wanda、SparseGPT 等算法可独立演进

### Baseline 参考
1. **SparseGPT**: 基于 Hessian 的分层权重剪枝，精度损失小
2. **Wanda (Weights and Activations)**: 联合权重-激活重要性剪枝
3. **Magnitude Pruning**: 简单幅度裁剪，适合快速实验
4. **N:M Sparsity**: 2:4/4:8 结构化稀疏，硬件加速友好
5. **DejaVu**: 动态稀疏化，根据激活值选择性跳过计算

______________________________________________________________________

## 技术规格

### 输入接口（Sparsity Protocol）
```python
# core/protocols/sparsity.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Literal, Optional, Dict, List

SparsityPattern = Literal["unstructured", "n:m", "block", "dynamic"]
PruningMethod = Literal["magnitude", "gradient", "hessian", "wanda", "sparsegpt"]
SparsityFormat = Literal["csr", "coo", "bsr", "dense"]  # 稀疏矩阵存储格式

@dataclass
class SparsityConfig:
    pattern: SparsityPattern = "n:m"          # 稀疏模式
    sparsity_ratio: float = 0.5               # 稀疏度（0.5 = 50% 权重置零）
    n: int = 2                                 # N:M sparsity 的 N（保留的元素数）
    m: int = 4                                 # N:M sparsity 的 M（窗口大小）
    pruning_method: PruningMethod = "magnitude"  # 剪枝方法
    target_layers: List[str] = None           # 目标层（None = 全部）
    exclude_layers: List[str] = None          # 排除层（如 embedding/lm_head）
    calibration_dataset: str = None           # 校准数据集路径（用于 SparseGPT/Wanda）
    use_gradient: bool = False                # 是否使用梯度信息
    block_size: int = 32                      # Block sparsity 块大小
    format: SparsityFormat = "csr"            # 存储格式
    enable_dynamic: bool = False              # 是否启用动态稀疏化（DejaVu）
    dynamic_threshold: float = 0.1            # 动态稀疏化激活阈值
    fuse_with_quant: bool = False             # 是否与量化（3.1）联合优化
    quant_dtype: Optional[str] = None         # 联合量化时的数据类型（int4/fp8）
    backend: Literal["cuda", "triton", "tensorrt"] = "cuda"
    use_hardware_accel: bool = True           # 是否使用硬件加速（Ampere 2:4）

class SparsityProtocol(ABC):
    @abstractmethod
    def prune(self, model_path: str, config: SparsityConfig) -> str:
        """执行剪枝，返回稀疏模型路径"""
        raise NotImplementedError

    @abstractmethod
    def load_sparse_model(self, model_path: str, config: SparsityConfig):
        """加载稀疏模型并返回稀疏 Kernel 绑定信息"""
        raise NotImplementedError

    @abstractmethod
    def verify_sparsity(self, model_path: str) -> Dict[str, float]:
        """验证实际稀疏度（按层统计）"""
        raise NotImplementedError

    @abstractmethod
    def select_kernel(self, config: SparsityConfig) -> str:
        """根据稀疏模式/硬件选择最佳 Kernel（cuSPARSE/Triton/TensorRT）"""
        raise NotImplementedError

    @abstractmethod
    def report(self) -> Dict[str, float]:
        """返回关键指标（稀疏度、加速比、精度损失、显存节省）"""
        raise NotImplementedError
```

### 输出接口（使用示例）
```python
from sage.llm.sagellm.accel.sparsity import SparsityManager, SparsityConfig

# 示例 1: 2:4 结构化稀疏（硬件加速）
config_24 = SparsityConfig(
    pattern="n:m",
    n=2, m=4,
    pruning_method="magnitude",
    target_layers=["self_attn.q_proj", "self_attn.v_proj", "mlp.up_proj"],
    use_hardware_accel=True,  # 使用 Ampere/Hopper 硬件加速
    backend="cuda",
)

manager = SparsityManager(config_24)
sparse_model_path = manager.prune("Qwen/Qwen2.5-7B-Instruct", config_24)
manager.load_sparse_model(sparse_model_path, config_24)
report = manager.report()
# report: {"avg_sparsity": 0.5, "speedup": 1.8, "perplexity_delta": 0.02}

# 示例 2: SparseGPT 非结构化 50% 稀疏
config_sparsegpt = SparsityConfig(
    pattern="unstructured",
    sparsity_ratio=0.5,
    pruning_method="sparsegpt",
    calibration_dataset="wikitext-2",
    exclude_layers=["embed_tokens", "lm_head"],
    format="csr",
)

manager.prune("meta-llama/Llama-3-8B", config_sparsegpt)
sparsity_stats = manager.verify_sparsity(sparse_model_path)
# sparsity_stats: {"layer.0.mlp.up_proj": 0.51, "layer.0.mlp.down_proj": 0.49, ...}

# 示例 3: 动态稀疏化（DejaVu）
config_dynamic = SparsityConfig(
    pattern="dynamic",
    enable_dynamic=True,
    dynamic_threshold=0.1,  # 激活值 < 0.1 的跳过计算
    backend="triton",       # 使用 Triton 自定义 Kernel
)

manager.load_sparse_model(sparse_model_path, config_dynamic)
# 推理时根据激活值动态调整稀疏度

# 示例 4: 稀疏 + 量化联合优化
config_joint = SparsityConfig(
    pattern="n:m",
    n=2, m=4,
    pruning_method="wanda",
    fuse_with_quant=True,
    quant_dtype="int4",     # 与 3.1 量化模块协同
    calibration_dataset="c4",
)

manager.prune("Qwen/Qwen2.5-14B", config_joint)
# 先剪枝后量化，或联合优化（取决于实现策略）
```

______________________________________________________________________

## 核心能力设计

### 1. 剪枝算法实现

#### 1.1 幅度剪枝（Magnitude Pruning）
- **算法**: 按权重绝对值排序，保留 top-k
- **优点**: 简单快速，无需校准数据
- **缺点**: 可能忽略权重-激活协同效应
- **实现**: `pruners/magnitude.py`

```python
# pruners/magnitude.py
def magnitude_prune(weight: torch.Tensor, ratio: float) -> torch.Tensor:
    """幅度剪枝：保留绝对值最大的 (1-ratio) 比例权重"""
    threshold = torch.quantile(weight.abs(), ratio)
    mask = weight.abs() >= threshold
    return weight * mask
```

#### 1.2 SparseGPT（Hessian-based）
- **算法**: 基于 Hessian 矩阵的分层最优化剪枝
- **优点**: 精度损失最小，理论保证
- **缺点**: 计算开销大，需要校准数据
- **Baseline**: [SparseGPT Paper](https://arxiv.org/abs/2301.00774)
- **实现**: `pruners/sparsegpt.py`

```python
# pruners/sparsegpt.py
class SparseGPT:
    def prune_layer(self, layer: nn.Linear, H: torch.Tensor, ratio: float):
        """基于 Hessian 的分层剪枝"""
        W = layer.weight.data
        # 计算最优剪枝掩码（最小化 ||WH - W_pruned H||^2）
        mask = self._optimal_brain_compression(W, H, ratio)
        layer.weight.data *= mask
```

#### 1.3 Wanda（Weights and Activations）
- **算法**: 联合权重-激活重要性评分
- **优点**: 考虑权重与激活的协同效应
- **Baseline**: [Wanda Paper](https://arxiv.org/abs/2306.11695)
- **实现**: `pruners/wanda.py`

```python
# pruners/wanda.py
def wanda_prune(weight: torch.Tensor, activations: torch.Tensor, ratio: float):
    """Wanda 剪枝：score = |weight| * norm(activations)"""
    act_norm = activations.norm(dim=0)  # 按输出维度计算激活范数
    importance = weight.abs() * act_norm
    threshold = torch.quantile(importance, ratio)
    mask = importance >= threshold
    return weight * mask
```

#### 1.4 N:M 结构化稀疏
- **算法**: 在每 M 个元素中保留最大的 N 个
- **优点**: 硬件加速友好（Ampere 2:4, Hopper 4:8）
- **实现**: `pruners/nm_sparse.py`

```python
# pruners/nm_sparse.py
def apply_nm_sparsity(weight: torch.Tensor, n: int, m: int) -> torch.Tensor:
    """N:M 稀疏化：每 m 个元素保留最大的 n 个"""
    B, C = weight.shape
    weight_reshaped = weight.view(B, C // m, m)
    # 在每个 m-窗口内选择 top-n
    topk_indices = weight_reshaped.abs().topk(n, dim=-1).indices
    mask = torch.zeros_like(weight_reshaped, dtype=torch.bool)
    mask.scatter_(-1, topk_indices, True)
    return (weight * mask.view(B, C)).contiguous()
```

### 2. 稀疏 Kernel 选择与调度

#### 2.1 Kernel 后端适配
```python
# kernels/selector.py
def select_sparse_kernel(config: SparsityConfig, device_info: DeviceInfo) -> str:
    """根据稀疏模式和硬件选择最佳 Kernel"""
    if config.pattern == "n:m" and device_info.compute_capability >= 8.0:
        # Ampere/Hopper 硬件加速
        return "cuda_sparse_nm_kernel" if config.use_hardware_accel else "triton_nm_kernel"
    
    elif config.pattern == "unstructured":
        if config.format == "csr":
            return "cusparse_csr_kernel"  # cuSPARSE CSR SpMM
        elif config.format == "coo":
            return "cusparse_coo_kernel"
        else:
            return "triton_sparse_kernel"  # 自定义 Triton Kernel
    
    elif config.pattern == "dynamic":
        return "triton_dynamic_sparse_kernel"  # DejaVu-style
    
    else:
        return "dense_fallback_kernel"
```

#### 2.2 硬件加速检测
```python
# utils/hardware.py
def check_nm_hardware_support(device: torch.device) -> bool:
    """检查是否支持 N:M 稀疏硬件加速"""
    if device.type != "cuda":
        return False
    
    capability = torch.cuda.get_device_capability(device)
    # Ampere (8.0+) 支持 2:4, Hopper (9.0+) 支持 4:8
    return capability[0] >= 8
```

### 3. 稀疏矩阵格式转换

#### 3.1 CSR (Compressed Sparse Row)
```python
# formats/csr.py
def to_csr(weight: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """转换为 CSR 格式（适合行优先访问）"""
    sparse = weight.to_sparse()
    indices = sparse._indices()
    values = sparse._values()
    
    row_ptr = torch.zeros(weight.size(0) + 1, dtype=torch.int32)
    for i in range(indices.size(1)):
        row_ptr[indices[0, i] + 1] += 1
    row_ptr = row_ptr.cumsum(0)
    
    col_indices = indices[1, :].int()
    return values, col_indices, row_ptr
```

### 4. 动态稀疏化（DejaVu）

#### 4.1 激活感知跳过
```python
# dynamic/dejavu.py
class DejaVuSparse:
    def __init__(self, threshold: float = 0.1):
        self.threshold = threshold
    
    def forward(self, x: torch.Tensor, weight: torch.Tensor) -> torch.Tensor:
        """动态稀疏化：根据激活值选择性计算"""
        # 1. 预测哪些激活重要（使用轻量级 predictor）
        importance_mask = self._predict_importance(x)
        
        # 2. 只对重要的激活执行完整计算
        output = torch.zeros(x.size(0), weight.size(0), device=x.device)
        important_indices = importance_mask.nonzero(as_tuple=False).squeeze()
        output[:, important_indices] = F.linear(x, weight[important_indices])
        
        return output
    
    def _predict_importance(self, x: torch.Tensor) -> torch.Tensor:
        """预测激活重要性（简化版：基于 L2 范数）"""
        return (x.norm(dim=-1) > self.threshold)
```

______________________________________________________________________

## 与其他模块协同

### 1. 与量化模块（3.1）联合优化
```python
# integration/sparse_quant.py
from sage.llm.sagellm.accel.quantization import QuantizationManager, QuantConfig
from sage.llm.sagellm.accel.sparsity import SparsityManager, SparsityConfig

def joint_sparse_quant(model_path: str) -> str:
    """先稀疏化后量化（推荐顺序）"""
    # Step 1: 稀疏化
    sparse_config = SparsityConfig(
        pattern="n:m", n=2, m=4,
        pruning_method="wanda",
        calibration_dataset="wikitext-2",
    )
    sparse_mgr = SparsityManager(sparse_config)
    sparse_model = sparse_mgr.prune(model_path, sparse_config)
    
    # Step 2: 量化
    quant_config = QuantConfig(
        weight_dtype="int4",
        activation_dtype="w8a16",
        calibration="awq",
    )
    quant_mgr = QuantizationManager(quant_config)
    final_model = quant_mgr.calibrate(sparse_model, "wikitext-2", quant_config)
    
    return final_model
```

### 2. 与 Kernel 融合（3.4）协同
```python
# 稀疏矩阵乘法 + FlashAttention 融合
# 由 3.4 模块提供 FusedSparseAttention Kernel
from sage.llm.sagellm.accel.kernel_fusion import FusedSparseAttention

sparse_attn = FusedSparseAttention(
    sparsity_pattern="n:m", n=2, m=4,
    use_flash_attention=True,
)
output = sparse_attn(query, sparse_key, sparse_value)
```

### 3. 与调度器 IR（2.4）协同
```python
# scheduler_ir 下发稀疏化配置
ir_node = IRNode(
    type="decode",
    config={
        "sparsity": {"pattern": "n:m", "n": 2, "m": 4},
        "use_sparse_kernel": True,
    }
)
```

______________________________________________________________________

## 性能指标与验证

### 目标指标
| 指标 | 目标 | 测量方法 |
|------|------|---------|
| 稀疏度（Sparsity Ratio） | 50-80% | 实际零值权重占比 |
| 加速比（Speedup） | ≥1.5x (decode), ≥1.3x (prefill) | TTFT/TPOT 对比 dense baseline |
| 精度损失（PPL Delta） | <5% | Wikitext-2 perplexity 回退 |
| 显存节省（Memory Saving） | ≥30% | 权重存储大小 |
| Kernel 效率（Kernel Efficiency） | ≥90% 理论峰值 | CUDA profiler FLOPS 利用率 |

### Benchmark 设计
```python
# benchmarks/sparsity_bench.py
def benchmark_sparsity(model_path: str, config: SparsityConfig):
    """稀疏化 Benchmark"""
    # 1. Baseline: Dense 模型
    dense_model = load_model(model_path)
    dense_metrics = measure_performance(dense_model)
    
    # 2. Sparse 模型
    sparse_mgr = SparsityManager(config)
    sparse_model_path = sparse_mgr.prune(model_path, config)
    sparse_model = sparse_mgr.load_sparse_model(sparse_model_path, config)
    sparse_metrics = measure_performance(sparse_model)
    
    # 3. 对比报告
    report = {
        "sparsity_ratio": sparse_mgr.verify_sparsity(sparse_model_path),
        "speedup_decode": sparse_metrics["tpot"] / dense_metrics["tpot"],
        "speedup_prefill": sparse_metrics["ttft"] / dense_metrics["ttft"],
        "ppl_delta": sparse_metrics["ppl"] - dense_metrics["ppl"],
        "memory_saving": (dense_metrics["mem"] - sparse_metrics["mem"]) / dense_metrics["mem"],
    }
    return report
```

______________________________________________________________________

## 实现路线图

### Week 11: 核心剪枝算法
- [ ] Magnitude/SparseGPT/Wanda 实现
- [ ] N:M 结构化稀疏化
- [ ] 稀疏矩阵格式转换（CSR/COO/BSR）

### Week 12: Kernel 集成与优化
- [ ] cuSPARSE/Triton Kernel 适配
- [ ] 2:4 硬件加速检测与回退
- [ ] 动态稀疏化（DejaVu）原型

### Week 13: 联合优化与验证
- [ ] 稀疏+量化联合优化
- [ ] Benchmark 与性能对比
- [ ] 精度验证（Wikitext/MMLU）

______________________________________________________________________

## 参考资源

### 论文
1. **SparseGPT**: "SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot" (2023)
2. **Wanda**: "A Simple and Effective Pruning Approach for Large Language Models" (2023)
3. **DejaVu**: "DejaVu: Contextual Sparsity for Efficient LLMs at Inference Time" (2023)
4. **N:M Sparsity**: NVIDIA Ampere Architecture Whitepaper

### 代码参考
- SparseGPT: https://github.com/IST-DASLab/sparsegpt
- Wanda: https://github.com/locuslab/wanda
- DejaVu: https://github.com/FMInference/DejaVu
- cuSPARSE: NVIDIA CUDA Toolkit

______________________________________________________________________

## FAQ

**Q: 稀疏化和量化哪个先做？**  
A: 推荐先稀疏化后量化。稀疏化减少参数量，量化减少单个参数的存储。反过来（先量化后稀疏）可能导致剪枝时难以评估重要性。

**Q: 2:4 稀疏需要特定硬件吗？**  
A: 是的。NVIDIA Ampere (A100/RTX 30x0) 及以上才有硬件加速。在其他硬件上会自动回退到软件实现（Triton），但加速效果较弱。

**Q: 动态稀疏化（DejaVu）开销大吗？**  
A: DejaVu 需要在线预测激活重要性，有额外开销（~5-10%）。但对于长上下文推理，跳过的计算收益可覆盖预测成本。

**Q: 稀疏化会影响 KV Cache 复用吗？**  
A: 不影响。稀疏化只改变权重，KV Cache 的生成和复用逻辑不变。但稀疏化后的模型可能生成略有不同的 KV，影响前缀匹配率（通常 <2%）。
