# 小方向 3.5：CoT 加速 (Chain-of-Thought Acceleration)

> **模块编号**: 3.5 = Phase 3（模型压缩与加速）第 5 个子模块  
> **Git Repo**: `sageLLM-cot-acceleration` | **Priority**: P2 (可选) | **Phase**: Week 12-13

______________________________________________________________________

## 模块定位

### 核心职责
- 为思维链（Chain-of-Thought）推理提供专用加速优化，减少冗余计算和中间步骤延迟
- 实现 CoT 步骤缓存、剪枝、压缩和跳过策略，提升推理效率
- 支持多种 CoT 模式：零样本 CoT、少样本 CoT、Self-Consistency、Tree-of-Thoughts
- 与前缀缓存（2.1）、投机解码（3.3）协同，实现 CoT 推理的端到端优化

### 为什么需要独立模块
- **效率瓶颈**：CoT 推理需要生成大量中间步骤（5-10x 普通推理），延迟高、成本高
- **冗余计算**：重复的推理路径、相似的中间步骤可以复用或跳过
- **质量保证**：Self-Consistency 需要多次采样，但可以通过早停和投票剪枝减少开销
- **研究价值**：CoT 优化是 LLM 推理的新兴方向，算法快速演进（ToT、GoT、PoT）

### Baseline 参考
1. **Zero-shot CoT**: "Let's think step by step" prompt engineering
2. **Self-Consistency**: 多次采样 + 投票，提升答案质量
3. **Tree-of-Thoughts (ToT)**: 树状探索推理路径，BFS/DFS 剪枝
4. **Graph-of-Thoughts (GoT)**: 图状推理，复用子问题
5. **Program-of-Thoughts (PoT)**: 代码生成 + 执行，减少推理步骤

______________________________________________________________________

## 技术规格

### 输入接口（CoT Acceleration Protocol）
```python
# core/protocols/cot_acceleration.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Literal, Optional, List, Dict, Tuple, Callable
import torch

CoTMode = Literal["zero_shot", "few_shot", "self_consistency", "tot", "got", "pot"]
PruningStrategy = Literal["confidence", "diversity", "cost", "hybrid"]
CacheStrategy = Literal["prefix", "step", "subgraph"]

@dataclass
class CoTConfig:
    # CoT 模式配置
    mode: CoTMode = "zero_shot"
    num_thoughts: int = 1                    # 推理路径数量（Self-Consistency 用）
    max_depth: int = 5                       # 推理深度（ToT/GoT 用）
    max_width: int = 3                       # 每步分支数（ToT 用）
    
    # 缓存配置
    enable_cache: bool = True                # 是否启用 CoT 步骤缓存
    cache_strategy: CacheStrategy = "step"   # 缓存粒度（prefix/step/subgraph）
    cache_similarity_threshold: float = 0.9  # 相似步骤匹配阈值
    
    # 剪枝配置
    enable_pruning: bool = True              # 是否启用推理路径剪枝
    pruning_strategy: PruningStrategy = "confidence"  # 剪枝策略
    confidence_threshold: float = 0.7        # 置信度阈值（低于此值的路径剪枝）
    max_tokens_per_step: int = 200           # 每步最大 token 数（防止冗长）
    early_stopping: bool = True              # 是否启用早停（达到一致答案即停止）
    
    # 压缩配置
    enable_compression: bool = False         # 是否压缩中间步骤
    compression_ratio: float = 0.5           # 压缩比例（保留最重要的 token）
    
    # 投票与聚合
    voting_method: Literal["majority", "weighted", "unanimous"] = "majority"
    vote_weight_by_confidence: bool = True   # 是否按置信度加权投票
    
    # 跳过策略
    skip_trivial_steps: bool = True          # 是否跳过简单步骤（如重复、确认性）
    trivial_step_threshold: float = 0.95     # 简单步骤检测阈值（高相似度 = 简单）
    
    # 并行化配置
    parallel_sampling: bool = True           # Self-Consistency 是否并行采样
    parallel_evaluation: bool = True         # ToT/GoT 是否并行评估分支
    
    # 与其他模块协同
    use_prefix_cache: bool = True            # 是否使用前缀缓存（2.1）
    use_speculative: bool = False            # 是否使用投机解码（3.3）
    
    # Telemetry
    log_reasoning_path: bool = True          # 是否记录推理路径
    log_pruning_stats: bool = True           # 是否记录剪枝统计

class CoTAccelerationProtocol(ABC):
    @abstractmethod
    def accelerate_cot(
        self,
        prompt: str,
        config: CoTConfig
    ) -> Tuple[str, Dict[str, any]]:
        """执行加速的 CoT 推理，返回答案和统计信息"""
        raise NotImplementedError
    
    @abstractmethod
    def cache_reasoning_step(
        self,
        step: str,
        embeddings: torch.Tensor,
        metadata: Dict
    ):
        """缓存推理步骤"""
        raise NotImplementedError
    
    @abstractmethod
    def retrieve_similar_step(
        self,
        query_step: str,
        threshold: float
    ) -> Optional[Tuple[str, torch.Tensor]]:
        """检索相似的推理步骤"""
        raise NotImplementedError
    
    @abstractmethod
    def prune_reasoning_paths(
        self,
        paths: List[Dict],
        strategy: PruningStrategy
    ) -> List[Dict]:
        """剪枝推理路径"""
        raise NotImplementedError
    
    @abstractmethod
    def aggregate_results(
        self,
        results: List[str],
        confidences: List[float],
        method: str
    ) -> str:
        """聚合多个推理结果（投票）"""
        raise NotImplementedError
    
    @abstractmethod
    def report(self) -> Dict[str, float]:
        """返回关键指标（延迟降低、剪枝率、缓存命中率）"""
        raise NotImplementedError
```

### 输出接口（使用示例）
```python
from sage.llm.engines.sagellm.accel.cot import CoTAccelerator, CoTConfig

# 示例 1: Zero-shot CoT + 步骤缓存
config_zero = CoTConfig(
    mode="zero_shot",
    enable_cache=True,
    cache_strategy="step",
    skip_trivial_steps=True,
)

accelerator = CoTAccelerator()
answer, stats = accelerator.accelerate_cot(
    prompt="What is 234 * 567?",
    config=config_zero
)
# answer: "132,678"
# stats: {"latency": 1.2s, "cache_hits": 0, "steps_skipped": 0}

# 示例 2: Self-Consistency + 剪枝
config_sc = CoTConfig(
    mode="self_consistency",
    num_thoughts=5,                   # 采样 5 条推理路径
    enable_pruning=True,
    pruning_strategy="confidence",
    confidence_threshold=0.7,         # 置信度 <0.7 的路径提前剪枝
    early_stopping=True,              # 3 条路径一致即停止
    parallel_sampling=True,           # 并行采样
)

answer, stats = accelerator.accelerate_cot(
    prompt="Is 17 a prime number? Let's think step by step.",
    config=config_sc
)
# stats: {"latency": 2.5s, "paths_sampled": 3, "paths_pruned": 0, "agreement": "unanimous"}

# 示例 3: Tree-of-Thoughts + 缓存 + 剪枝
config_tot = CoTConfig(
    mode="tot",
    max_depth=4,                      # 探索深度 4
    max_width=3,                      # 每步最多 3 个分支
    enable_cache=True,
    cache_strategy="subgraph",        # 缓存子图（复用相似推理路径）
    enable_pruning=True,
    pruning_strategy="cost",          # 按成本剪枝（优先保留短路径）
    parallel_evaluation=True,
)

answer, stats = accelerator.accelerate_cot(
    prompt="Find the shortest path from A to E in a graph...",
    config=config_tot
)
# stats: {"latency": 4.2s, "nodes_explored": 12, "nodes_pruned": 8, "cache_hits": 3}

# 示例 4: Graph-of-Thoughts（复用子问题）
config_got = CoTConfig(
    mode="got",
    max_depth=3,
    enable_cache=True,
    cache_strategy="subgraph",        # 子图缓存对 GoT 至关重要
    cache_similarity_threshold=0.9,
)

answer, stats = accelerator.accelerate_cot(
    prompt="Solve a complex math problem with overlapping subproblems...",
    config=config_got
)
# stats: {"latency": 3.1s, "subproblems_cached": 5, "cache_hit_rate": 0.62}
```

______________________________________________________________________

## 核心能力设计

### 1. CoT 步骤缓存

#### 1.1 步骤级缓存（最常用）
```python
# cache/step_cache.py
class CoTStepCache:
    """缓存独立的推理步骤"""
    def __init__(self, similarity_threshold: float = 0.9):
        self.cache = {}  # {step_hash: (step_text, embeddings, result)}
        self.threshold = similarity_threshold
    
    def store(self, step: str, embeddings: torch.Tensor, result: str):
        """存储推理步骤"""
        step_hash = self._compute_hash(step)
        self.cache[step_hash] = {
            "text": step,
            "embeddings": embeddings,
            "result": result,
            "count": 1,  # 使用次数
        }
    
    def retrieve(self, query_step: str) -> Optional[str]:
        """检索相似步骤的结果"""
        query_emb = self._embed(query_step)
        
        # 计算与所有缓存步骤的相似度
        best_match = None
        best_sim = -1.0
        
        for step_hash, cached in self.cache.items():
            sim = self._cosine_similarity(query_emb, cached["embeddings"])
            if sim > best_sim and sim >= self.threshold:
                best_sim = sim
                best_match = cached["result"]
        
        if best_match:
            self.cache[step_hash]["count"] += 1  # 增加命中计数
        
        return best_match
    
    def _compute_hash(self, step: str) -> int:
        """计算步骤的哈希（用于快速查找）"""
        return hash(step.strip().lower())
    
    def _embed(self, text: str) -> torch.Tensor:
        """生成文本嵌入（用于相似度计算）"""
        # 使用轻量级嵌入模型（如 sentence-transformers）
        return self.embedding_model.encode(text)
    
    def _cosine_similarity(self, a: torch.Tensor, b: torch.Tensor) -> float:
        """计算余弦相似度"""
        return torch.cosine_similarity(a, b, dim=0).item()
```

#### 1.2 前缀缓存（与 2.1 集成）
```python
# cache/prefix_cache_integration.py
from sage.llm.engines.sagellm.prefix_reuse import PrefixCache

class CoTPrefixCache:
    """利用前缀缓存加速 CoT（复用 prompt + 前 N 步）"""
    def __init__(self):
        self.prefix_cache = PrefixCache()
    
    def cache_cot_prefix(self, prompt: str, steps: List[str], kv_cache):
        """缓存 prompt + 已完成的 CoT 步骤"""
        full_text = prompt + "\n".join(steps)
        self.prefix_cache.store(full_text, kv_cache)
    
    def retrieve_cot_prefix(self, prompt: str, completed_steps: List[str]):
        """检索匹配的 CoT 前缀"""
        query = prompt + "\n".join(completed_steps)
        return self.prefix_cache.match(query)
```

#### 1.3 子图缓存（ToT/GoT 用）
```python
# cache/subgraph_cache.py
class SubgraphCache:
    """缓存推理子图（适用于 ToT/GoT）"""
    def __init__(self):
        self.cache = {}  # {subgraph_signature: (nodes, edges, result)}
    
    def store_subgraph(self, nodes: List[str], edges: List[Tuple], result: str):
        """存储推理子图"""
        signature = self._compute_signature(nodes, edges)
        self.cache[signature] = {"nodes": nodes, "edges": edges, "result": result}
    
    def retrieve_subgraph(self, query_nodes: List[str]) -> Optional[str]:
        """检索相似子图的结果"""
        query_sig = self._compute_signature(query_nodes, [])
        
        # 检查是否有匹配的子图
        for sig, cached in self.cache.items():
            if self._is_subgraph_match(query_sig, sig):
                return cached["result"]
        
        return None
    
    def _compute_signature(self, nodes: List[str], edges: List[Tuple]) -> str:
        """计算子图签名（基于节点和边的哈希）"""
        node_hashes = sorted([hash(node) for node in nodes])
        edge_hashes = sorted([hash(edge) for edge in edges])
        return f"{node_hashes}_{edge_hashes}"
```

### 2. 推理路径剪枝

#### 2.1 置信度剪枝
```python
# pruning/confidence_pruning.py
class ConfidencePruning:
    """基于置信度剪枝推理路径"""
    def __init__(self, threshold: float = 0.7):
        self.threshold = threshold
    
    def prune(self, paths: List[Dict]) -> List[Dict]:
        """剪枝低置信度路径"""
        pruned = []
        
        for path in paths:
            # 计算路径的平均置信度
            confidences = [step["confidence"] for step in path["steps"]]
            avg_conf = sum(confidences) / len(confidences)
            
            if avg_conf >= self.threshold:
                pruned.append(path)
        
        return pruned
```

#### 2.2 成本剪枝（ToT/GoT 用）
```python
# pruning/cost_pruning.py
class CostPruning:
    """基于成本剪枝（优先保留短路径、低延迟）"""
    def __init__(self, max_cost: float = 100.0):
        self.max_cost = max_cost
    
    def prune(self, paths: List[Dict]) -> List[Dict]:
        """剪枝高成本路径"""
        pruned = []
        
        for path in paths:
            # 计算路径成本（token 数 + 推理时间）
            cost = len(path["tokens"]) + path["latency"] * 10
            
            if cost <= self.max_cost:
                pruned.append(path)
        
        # 按成本排序，保留前 K 个
        pruned = sorted(pruned, key=lambda p: p["cost"])[:self.max_width]
        return pruned
```

#### 2.3 多样性剪枝
```python
# pruning/diversity_pruning.py
class DiversityPruning:
    """保持路径多样性（避免过早收敛）"""
    def __init__(self, diversity_threshold: float = 0.8):
        self.threshold = diversity_threshold
    
    def prune(self, paths: List[Dict]) -> List[Dict]:
        """剪枝过于相似的路径"""
        pruned = [paths[0]]  # 保留第一条路径
        
        for path in paths[1:]:
            # 检查与已保留路径的相似度
            is_diverse = True
            for kept_path in pruned:
                sim = self._compute_similarity(path, kept_path)
                if sim > self.threshold:
                    is_diverse = False
                    break
            
            if is_diverse:
                pruned.append(path)
        
        return pruned
```

### 3. Self-Consistency 优化

#### 3.1 并行采样 + 早停
```python
# self_consistency/parallel_sampling.py
class ParallelSelfConsistency:
    """并行采样 + 投票 + 早停"""
    def __init__(self, num_samples: int = 5, agreement_threshold: int = 3):
        self.num_samples = num_samples
        self.agreement_threshold = agreement_threshold
    
    def sample_and_vote(self, prompt: str, model) -> Tuple[str, Dict]:
        """并行采样并投票"""
        # 并行生成多条推理路径
        paths = []
        for i in range(self.num_samples):
            path = model.generate(prompt, temperature=0.7)
            paths.append(path)
            
            # 早停检查：是否已有 agreement_threshold 条路径一致
            if self._check_early_stop(paths):
                break
        
        # 投票选出最终答案
        answer = self._vote(paths)
        stats = {
            "paths_sampled": len(paths),
            "agreement": self._compute_agreement(paths),
        }
        
        return answer, stats
    
    def _check_early_stop(self, paths: List[str]) -> bool:
        """检查是否可以早停"""
        # 提取答案（假设答案在最后一行）
        answers = [path.split("\n")[-1] for path in paths]
        
        # 统计最常见答案的出现次数
        from collections import Counter
        most_common_count = Counter(answers).most_common(1)[0][1]
        
        return most_common_count >= self.agreement_threshold
    
    def _vote(self, paths: List[str]) -> str:
        """投票选出最终答案"""
        answers = [path.split("\n")[-1] for path in paths]
        from collections import Counter
        return Counter(answers).most_common(1)[0][0]
```

#### 3.2 加权投票（按置信度）
```python
# self_consistency/weighted_voting.py
class WeightedVoting:
    """按置信度加权投票"""
    def vote(self, paths: List[Dict]) -> str:
        """加权投票"""
        answer_scores = {}
        
        for path in paths:
            answer = path["answer"]
            confidence = path["confidence"]
            
            answer_scores[answer] = answer_scores.get(answer, 0) + confidence
        
        # 返回得分最高的答案
        return max(answer_scores, key=answer_scores.get)
```

### 4. Tree-of-Thoughts (ToT) 优化

#### 4.1 BFS + 剪枝
```python
# tot/bfs_pruning.py
class ToTBFSPruning:
    """Tree-of-Thoughts BFS 探索 + 剪枝"""
    def __init__(self, max_depth: int, max_width: int):
        self.max_depth = max_depth
        self.max_width = max_width
    
    def search(self, root_prompt: str, model, evaluator) -> str:
        """BFS 搜索推理树"""
        from collections import deque
        
        queue = deque([(root_prompt, 0, [])])  # (current_state, depth, path)
        best_path = None
        best_score = -1.0
        
        while queue:
            state, depth, path = queue.popleft()
            
            if depth >= self.max_depth:
                # 到达最大深度，评估当前路径
                score = evaluator.evaluate(path)
                if score > best_score:
                    best_score = score
                    best_path = path
                continue
            
            # 生成候选分支
            candidates = model.generate_candidates(state, k=self.max_width)
            
            # 评估并剪枝
            scored_candidates = [(c, evaluator.evaluate_step(c)) for c in candidates]
            scored_candidates = sorted(scored_candidates, key=lambda x: x[1], reverse=True)
            top_candidates = scored_candidates[:self.max_width]
            
            # 将 top 候选加入队列
            for candidate, score in top_candidates:
                new_path = path + [candidate]
                queue.append((candidate, depth + 1, new_path))
        
        return best_path
```

### 5. 跳过简单步骤

#### 5.1 冗余检测
```python
# skip/trivial_detector.py
class TrivialStepDetector:
    """检测并跳过简单/冗余步骤"""
    def __init__(self, threshold: float = 0.95):
        self.threshold = threshold
    
    def is_trivial(self, step: str, previous_steps: List[str]) -> bool:
        """判断步骤是否冗余"""
        # 检查与前面步骤的相似度
        step_emb = self._embed(step)
        
        for prev_step in previous_steps[-3:]:  # 只检查最近 3 步
            prev_emb = self._embed(prev_step)
            sim = self._cosine_similarity(step_emb, prev_emb)
            
            if sim >= self.threshold:
                return True  # 高度相似 = 冗余
        
        return False
    
    def skip_if_trivial(self, step: str, previous_steps: List[str]) -> bool:
        """决定是否跳过"""
        return self.is_trivial(step, previous_steps)
```

______________________________________________________________________

## 与其他模块协同

### 1. 与前缀缓存（2.1）集成
```python
# CoT 前缀复用
from sage.llm.engines.sagellm.prefix_reuse import PrefixCache

prefix_cache = PrefixCache()

# 缓存 CoT prompt + 前 N 步
cot_prefix = prompt + "\n".join(completed_steps)
prefix_kv = prefix_cache.match(cot_prefix)

if prefix_kv:
    # 复用前缀，继续生成后续步骤
    model.generate(next_step, past_kv=prefix_kv)
```

### 2. 与投机解码（3.3）协同
```python
# CoT + 投机解码（加速中间步骤生成）
from sage.llm.engines.sagellm.accel.speculative import SpeculativeDecoder

spec_decoder = SpeculativeDecoder()

# CoT 中间步骤用投机解码加速
for step in cot_steps:
    tokens = spec_decoder.decode_step(step, max_new_tokens=50)
```

### 3. 与调度器 IR（2.4）集成
```python
# 调度器下发 CoT 配置
ir_node = IRNode(
    type="cot_reasoning",
    config={
        "cot": {
            "mode": "self_consistency",
            "num_thoughts": 3,
            "enable_cache": True,
        }
    }
)
```

______________________________________________________________________

## 性能指标与验证

### 目标指标
| 指标 | 目标 | 测量方法 |
|------|------|---------|
| 延迟降低（Latency Reduction） | 30-50% | CoT_baseline / CoT_accelerated |
| 缓存命中率（Cache Hit Rate） | 40-60% | cache_hits / total_steps |
| 剪枝率（Pruning Rate） | 30-50% | paths_pruned / paths_generated |
| 质量保持（Quality Preservation） | ≥95% accuracy | 加速前后答案准确率对比 |
| 成本节省（Cost Saving） | 40-60% token reduction | total_tokens_saved / baseline_tokens |

### Benchmark 设计
```python
# benchmarks/cot_bench.py
def benchmark_cot(dataset: str, config: CoTConfig):
    """CoT 加速 Benchmark"""
    # 1. Baseline: 标准 CoT（无优化）
    baseline_metrics = run_standard_cot(dataset)
    
    # 2. Accelerated: 加速 CoT
    accelerator = CoTAccelerator()
    accel_metrics = run_accelerated_cot(accelerator, dataset, config)
    
    # 3. 对比报告
    report = {
        "latency_reduction": (baseline_metrics["latency"] - accel_metrics["latency"]) / baseline_metrics["latency"],
        "cache_hit_rate": accel_metrics["cache_hits"] / accel_metrics["total_steps"],
        "pruning_rate": accel_metrics["paths_pruned"] / accel_metrics["paths_generated"],
        "accuracy_preserved": accel_metrics["accuracy"] / baseline_metrics["accuracy"],
        "cost_saving": (baseline_metrics["tokens"] - accel_metrics["tokens"]) / baseline_metrics["tokens"],
    }
    return report
```

______________________________________________________________________

## 实现路线图

### Week 12: 核心缓存与剪枝
- [ ] 步骤级缓存实现
- [ ] 置信度剪枝
- [ ] 跳过冗余步骤

### Week 13: Self-Consistency 优化
- [ ] 并行采样 + 早停
- [ ] 加权投票
- [ ] 与前缀缓存集成

### Week 14: ToT/GoT 支持（可选）
- [ ] Tree-of-Thoughts BFS/DFS
- [ ] 子图缓存
- [ ] Graph-of-Thoughts 原型

______________________________________________________________________

## 参考资源

### 论文
1. **Chain-of-Thought Prompting**: "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" (Google, 2022)
2. **Self-Consistency**: "Self-Consistency Improves Chain of Thought Reasoning in Language Models" (Google, 2023)
3. **Tree-of-Thoughts**: "Tree of Thoughts: Deliberate Problem Solving with Large Language Models" (Princeton, 2023)
4. **Graph-of-Thoughts**: "Graph of Thoughts: Solving Elaborate Problems with Large Language Models" (ETH Zurich, 2023)
5. **Program-of-Thoughts**: "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks" (Tsinghua, 2023)

### 代码参考
- Tree-of-Thoughts: https://github.com/princeton-nlp/tree-of-thought-llm
- Graph-of-Thoughts: https://github.com/spcl/graph-of-thoughts
- Self-Consistency: https://github.com/google-research/self-consistency

______________________________________________________________________

## FAQ

**Q: CoT 加速会影响答案质量吗？**  
A: 如果配置得当（合理的剪枝阈值、缓存相似度），质量保持率可达 95%+。但激进的剪枝可能导致精度下降 5-10%。

**Q: CoT 缓存和前缀缓存有什么区别？**  
A: 前缀缓存基于 token 精确匹配，CoT 缓存基于语义相似度（嵌入）。CoT 缓存更灵活，可以匹配意思相同但表述不同的步骤。

**Q: Self-Consistency 的早停会错过更好的答案吗？**  
A: 可能。早停是延迟和质量的权衡。如果 3/5 路径一致即停止，可能错过后面 2 条路径的不同答案。可以通过调整 `agreement_threshold` 平衡。

**Q: ToT/GoT 的开销大吗？**  
A: 是的。ToT 需要探索多条路径（树状），GoT 需要维护子图。适合高价值任务（如数学竞赛、代码生成），不适合简单对话。

**Q: 这个模块是 P2（可选）的原因？**  
A: CoT 优化是特定场景（复杂推理任务）的优化，不是通用推理的必需组件。对于大多数对话、摘要等任务，标准推理已足够。
