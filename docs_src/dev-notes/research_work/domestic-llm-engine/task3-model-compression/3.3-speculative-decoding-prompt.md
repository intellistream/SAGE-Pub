# 小方向 3.3：投机解码 (Speculative Decoding)

> **模块编号**: 3.3 = Phase 3（模型压缩与加速）第 3 个子模块  
> **Git Repo**: `sageLLM-speculative-decoding` | **Priority**: P1 | **Phase**: Week 11-12

______________________________________________________________________

## 模块定位

### 核心职责
- 实现 Draft-Verify 双路投机解码架构，使用小模型（Draft）预测多个 token，大模型（Verify）并行验证
- 支持多种 Draft 策略：独立小模型、量化版本、前 N 层子模型、Medusa 多头预测
- 实现自适应投机深度（根据接受率动态调整预测长度）和树状投机（Tree Attention）
- 与 KV Cache 管理（2.2）、调度器 IR（2.4）深度集成，实现 Draft/Verify KV 共享和重用

### 为什么需要独立模块
- **性能突破**：投机解码可将 decode 延迟降低 2-3x（理想情况），是目前最有效的加速手段
- **复杂架构**：需要协调两个模型、管理两套 KV Cache、处理接受/拒绝逻辑
- **跨组件协同**：与调度器（2.4）、KV Pool（2.2）、前缀缓存（2.1）强耦合
- **研究价值**：Medusa、SpecInfer、BiLD 等算法快速演进，需要独立实验平台

### Baseline 参考
1. **Speculative Decoding (Google)**: 经典 Draft-Verify 框架
2. **Medusa**: 多头并行预测，无需独立 Draft 模型
3. **SpecInfer**: 树状投机 + 最优验证顺序
4. **BiLD (Best-of-Layer Distillation)**: 用前 N 层作为 Draft
5. **Lookahead Decoding**: N-gram 缓存 + 并行验证

______________________________________________________________________

## 技术规格

### 输入接口（Speculative Decoding Protocol）
```python
# core/protocols/speculative.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Literal, Optional, List, Dict, Tuple

DraftStrategy = Literal["small_model", "quantized", "prefix_layers", "medusa", "ngram"]
TreeStrategy = Literal["none", "linear", "tree"]  # 投机拓扑

@dataclass
class SpeculativeConfig:
    # Draft 模型配置
    draft_strategy: DraftStrategy = "small_model"
    draft_model_path: Optional[str] = None       # 独立小模型路径（如 Qwen2.5-0.5B）
    draft_num_layers: Optional[int] = None       # prefix_layers 策略：使用前 N 层
    draft_quant_dtype: Optional[str] = None      # 量化 Draft 模型（int4/fp8）
    
    # 投机深度配置
    num_speculative_tokens: int = 4              # 每次预测的 token 数（gamma）
    max_speculative_tokens: int = 8              # 最大投机深度
    min_speculative_tokens: int = 2              # 最小投机深度
    adaptive_depth: bool = True                  # 是否根据接受率自适应调整
    target_acceptance_rate: float = 0.7          # 目标接受率（用于自适应）
    
    # 树状投机配置
    tree_strategy: TreeStrategy = "none"         # 投机拓扑（linear/tree）
    tree_width: int = 2                          # 树状投机宽度（每个节点的候选数）
    tree_depth: int = 3                          # 树状投机深度
    
    # KV Cache 管理
    share_kv_cache: bool = True                  # Draft/Verify 是否共享 KV Cache
    reuse_draft_kv: bool = True                  # 接受的 token 是否复用 Draft KV
    kv_compression: bool = False                 # 是否压缩 Draft KV（FP8）
    
    # 验证策略
    verify_in_parallel: bool = True              # 是否并行验证（batched forward）
    early_rejection: bool = True                 # 是否提前拒绝（遇到首个不匹配即停止）
    temperature: float = 1.0                     # 采样温度
    top_p: float = 1.0                           # Nucleus 采样
    
    # 性能优化
    prefill_with_verify: bool = True             # Prefill 是否使用 Verify 模型（更准确）
    fuse_draft_verify: bool = False              # 是否融合 Draft/Verify forward（实验性）
    use_cuda_graph: bool = False                 # 是否使用 CUDA Graph（固定 gamma）
    
    # Telemetry
    log_acceptance_rate: bool = True             # 是否记录接受率
    log_speedup: bool = True                     # 是否记录加速比

class SpeculativeDecodingProtocol(ABC):
    @abstractmethod
    def setup(self, verify_model_path: str, config: SpeculativeConfig):
        """初始化 Draft/Verify 模型"""
        raise NotImplementedError
    
    @abstractmethod
    def decode_step(
        self, 
        prompt_tokens: List[int],
        max_new_tokens: int,
        config: SpeculativeConfig
    ) -> Tuple[List[int], Dict[str, float]]:
        """执行投机解码，返回 token 序列和统计信息"""
        raise NotImplementedError
    
    @abstractmethod
    def get_acceptance_rate(self) -> float:
        """获取当前接受率"""
        raise NotImplementedError
    
    @abstractmethod
    def adjust_speculative_depth(self, acceptance_rate: float):
        """根据接受率动态调整投机深度"""
        raise NotImplementedError
    
    @abstractmethod
    def report(self) -> Dict[str, float]:
        """返回关键指标（接受率、加速比、Draft/Verify 延迟）"""
        raise NotImplementedError
```

### 输出接口（使用示例）
```python
from sage.llm.sagellm.accel.speculative import SpeculativeDecoder, SpeculativeConfig

# 示例 1: 独立小模型 Draft（最常用）
config_small = SpeculativeConfig(
    draft_strategy="small_model",
    draft_model_path="Qwen/Qwen2.5-0.5B-Instruct",  # 小模型
    num_speculative_tokens=4,                        # 每次预测 4 个 token
    adaptive_depth=True,                             # 自适应调整
    target_acceptance_rate=0.7,
    share_kv_cache=True,
    reuse_draft_kv=True,
)

decoder = SpeculativeDecoder()
decoder.setup("Qwen/Qwen2.5-7B-Instruct", config_small)  # Verify 模型
tokens, stats = decoder.decode_step(
    prompt_tokens=[1, 2, 3],
    max_new_tokens=100,
    config=config_small
)
# stats: {"acceptance_rate": 0.68, "speedup": 2.3, "draft_latency": 5ms, "verify_latency": 15ms}

# 示例 2: Medusa 多头预测（无需独立 Draft）
config_medusa = SpeculativeConfig(
    draft_strategy="medusa",
    num_speculative_tokens=5,
    tree_strategy="tree",     # 树状投机
    tree_width=2,
    tree_depth=3,
)

decoder.setup("FasterDecoding/medusa-llama-7b", config_medusa)
tokens, stats = decoder.decode_step(prompt_tokens, max_new_tokens=100, config_medusa)

# 示例 3: 量化 Draft（节省显存）
config_quant_draft = SpeculativeConfig(
    draft_strategy="quantized",
    draft_model_path="Qwen/Qwen2.5-1.5B-Instruct",
    draft_quant_dtype="int4",  # Draft 用 INT4 量化
    num_speculative_tokens=6,
    kv_compression=True,       # Draft KV Cache 也用 FP8 压缩
)

decoder.setup("Qwen/Qwen2.5-14B", config_quant_draft)
tokens, stats = decoder.decode_step(prompt_tokens, max_new_tokens=100, config_quant_draft)

# 示例 4: 前 N 层作为 Draft（BiLD 风格）
config_prefix = SpeculativeConfig(
    draft_strategy="prefix_layers",
    draft_num_layers=12,       # 使用 Verify 模型的前 12 层作为 Draft
    num_speculative_tokens=3,
    share_kv_cache=True,       # 共享 KV Cache（Draft/Verify 同一个模型）
)

decoder.setup("Qwen/Qwen2.5-7B-Instruct", config_prefix)
tokens, stats = decoder.decode_step(prompt_tokens, max_new_tokens=100, config_prefix)
```

______________________________________________________________________

## 核心能力设计

### 1. Draft-Verify 流程

#### 1.1 经典投机解码算法
```python
# core/speculative_decoder.py
class SpeculativeDecoder:
    def decode_step(self, prompt_tokens, max_new_tokens, config):
        """单步投机解码"""
        generated_tokens = []
        current_tokens = prompt_tokens.copy()
        
        while len(generated_tokens) < max_new_tokens:
            # Step 1: Draft 预测 gamma 个 token
            draft_tokens = self._draft_forward(current_tokens, config.num_speculative_tokens)
            
            # Step 2: Verify 并行验证（一次 forward 验证所有 draft token）
            verify_probs = self._verify_forward(current_tokens + draft_tokens)
            
            # Step 3: 接受/拒绝判定
            accepted_tokens = []
            for i, draft_token in enumerate(draft_tokens):
                # 基于概率分布判断是否接受
                if self._accept(draft_token, verify_probs[i], config):
                    accepted_tokens.append(draft_token)
                    current_tokens.append(draft_token)
                else:
                    # 拒绝：从 Verify 分布重采样，停止后续验证
                    corrected_token = self._sample(verify_probs[i], config)
                    accepted_tokens.append(corrected_token)
                    current_tokens.append(corrected_token)
                    break  # Early rejection
            
            generated_tokens.extend(accepted_tokens)
            
            # Step 4: 自适应调整投机深度
            if config.adaptive_depth:
                acceptance_rate = len(accepted_tokens) / len(draft_tokens)
                self.adjust_speculative_depth(acceptance_rate)
        
        return generated_tokens
    
    def _accept(self, draft_token, verify_prob, config):
        """接受判定（基于概率比）"""
        draft_prob = self.draft_model.get_prob(draft_token)
        threshold = min(1.0, verify_prob / (draft_prob + 1e-10))
        return torch.rand(1).item() < threshold
```

#### 1.2 树状投机（SpecInfer）
```python
# strategies/tree_speculation.py
class TreeSpeculation:
    def build_tree(self, prompt_tokens, config):
        """构建树状候选 token 集合"""
        tree = {0: [prompt_tokens[-1]]}  # Root: 最后一个 prompt token
        
        for depth in range(config.tree_depth):
            for parent_id in tree[depth]:
                # 每个节点产生 tree_width 个候选
                candidates = self.draft_model.top_k(parent_id, k=config.tree_width)
                tree[depth + 1] = tree.get(depth + 1, []) + candidates
        
        return tree
    
    def verify_tree(self, tree, prompt_tokens):
        """Tree Attention：一次 forward 验证整棵树"""
        # 构建 Tree Attention Mask（每个节点只能看到其祖先）
        all_candidates = [node for level in tree.values() for node in level]
        attn_mask = self._build_tree_mask(tree)
        
        # 并行验证
        verify_probs = self.verify_model.forward(
            torch.cat([prompt_tokens, all_candidates]),
            attention_mask=attn_mask
        )
        
        # 从树的叶子节点找最长接受路径
        best_path = self._find_longest_accepted_path(tree, verify_probs)
        return best_path
```

### 2. Draft 策略实现

#### 2.1 独立小模型
```python
# strategies/small_model_draft.py
class SmallModelDraft:
    def __init__(self, draft_model_path: str):
        self.draft_model = AutoModelForCausalLM.from_pretrained(draft_model_path)
    
    def forward(self, input_ids: torch.Tensor, num_tokens: int) -> List[int]:
        """Draft 预测 num_tokens 个 token"""
        draft_tokens = []
        current_ids = input_ids
        
        for _ in range(num_tokens):
            logits = self.draft_model(current_ids).logits[:, -1, :]
            next_token = torch.argmax(logits, dim=-1)
            draft_tokens.append(next_token.item())
            current_ids = torch.cat([current_ids, next_token.unsqueeze(0)], dim=-1)
        
        return draft_tokens
```

#### 2.2 Medusa 多头预测
```python
# strategies/medusa_draft.py
class MedusaDraft:
    """Medusa: 在 Verify 模型上添加多个预测头"""
    def __init__(self, base_model, num_heads: int = 4):
        self.base_model = base_model
        # 每个头预测后续第 i 个 token
        self.medusa_heads = nn.ModuleList([
            nn.Linear(base_model.config.hidden_size, base_model.config.vocab_size)
            for _ in range(num_heads)
        ])
    
    def forward(self, input_ids: torch.Tensor) -> List[List[int]]:
        """并行预测多个位置的候选 token"""
        hidden_states = self.base_model(input_ids, output_hidden_states=True).hidden_states[-1]
        last_hidden = hidden_states[:, -1, :]
        
        # 每个头预测一个位置
        candidates = []
        for head in self.medusa_heads:
            logits = head(last_hidden)
            top_k = torch.topk(logits, k=2, dim=-1).indices  # 每个位置 top-2 候选
            candidates.append(top_k.tolist())
        
        return candidates  # [[tok1_1, tok1_2], [tok2_1, tok2_2], ...]
```

#### 2.3 前 N 层作为 Draft（BiLD）
```python
# strategies/prefix_layers_draft.py
class PrefixLayersDraft:
    """使用 Verify 模型的前 N 层作为 Draft"""
    def __init__(self, full_model, num_draft_layers: int):
        self.full_model = full_model
        self.num_draft_layers = num_draft_layers
        
        # 创建一个只包含前 N 层的浅层模型
        self.draft_model = self._create_shallow_model(full_model, num_draft_layers)
    
    def forward(self, input_ids, num_tokens):
        """Draft 用浅层模型预测"""
        draft_tokens = []
        current_ids = input_ids
        
        for _ in range(num_tokens):
            logits = self.draft_model(current_ids).logits[:, -1, :]
            next_token = torch.argmax(logits, dim=-1)
            draft_tokens.append(next_token.item())
            current_ids = torch.cat([current_ids, next_token.unsqueeze(0)], dim=-1)
        
        return draft_tokens
```

### 3. KV Cache 共享与复用

#### 3.1 Draft/Verify KV 共享
```python
# kv_cache/shared_kv.py
class SharedKVCache:
    """Draft 和 Verify 共享 KV Cache（适用于相同架构）"""
    def __init__(self, max_batch_size: int, max_seq_len: int):
        self.kv_cache = {}  # {layer_idx: (k_cache, v_cache)}
    
    def store_draft_kv(self, layer_idx: int, k: torch.Tensor, v: torch.Tensor):
        """存储 Draft 产生的 KV"""
        self.kv_cache[layer_idx] = (k, v)
    
    def reuse_for_verify(self, layer_idx: int, num_accepted: int):
        """Verify 复用已接受的 Draft KV"""
        k, v = self.kv_cache[layer_idx]
        # 只保留前 num_accepted 个 token 的 KV
        return k[:, :num_accepted, :], v[:, :num_accepted, :]
```

#### 3.2 接受/拒绝后的 KV 管理
```python
# kv_cache/acceptance_handler.py
def handle_acceptance(accepted_tokens, draft_kv_cache, verify_kv_cache):
    """处理接受后的 KV Cache"""
    num_accepted = len(accepted_tokens)
    
    for layer_idx in range(num_layers):
        draft_k, draft_v = draft_kv_cache[layer_idx]
        
        # 接受的部分：直接复用 Draft KV
        verify_kv_cache[layer_idx] = (
            draft_k[:, :num_accepted, :],
            draft_v[:, :num_accepted, :]
        )
    
    # 拒绝的部分：丢弃剩余 Draft KV
    # Verify 模型需要重新计算被拒绝位置的 KV
```

### 4. 自适应投机深度

#### 4.1 动态调整算法
```python
# adaptive/depth_adjuster.py
class AdaptiveDepthAdjuster:
    def __init__(self, target_rate: float = 0.7, min_depth: int = 2, max_depth: int = 8):
        self.target_rate = target_rate
        self.min_depth = min_depth
        self.max_depth = max_depth
        self.current_depth = (min_depth + max_depth) // 2
        self.acceptance_history = []
    
    def adjust(self, current_acceptance_rate: float):
        """根据接受率调整投机深度"""
        self.acceptance_history.append(current_acceptance_rate)
        
        # 使用滑动窗口平均
        avg_rate = np.mean(self.acceptance_history[-10:])
        
        if avg_rate > self.target_rate + 0.1:
            # 接受率高：增加投机深度
            self.current_depth = min(self.current_depth + 1, self.max_depth)
        elif avg_rate < self.target_rate - 0.1:
            # 接受率低：减少投机深度
            self.current_depth = max(self.current_depth - 1, self.min_depth)
        
        return self.current_depth
```

______________________________________________________________________

## 与其他模块协同

### 1. 与 KV Pool（2.2）集成
```python
# 通过 KV Pool 管理 Draft/Verify KV Cache
from sage.llm.sagellm.kv_runtime.kv_pool import KVPool

kv_pool = KVPool(max_blocks=1000)

# Draft forward: 分配临时 KV blocks
draft_blocks = kv_pool.allocate(num_tokens=config.num_speculative_tokens)
draft_kv = draft_model.forward(input_ids, kv_blocks=draft_blocks)

# Verify forward: 接受的 token 提升为永久 KV，拒绝的释放
accepted_blocks = draft_blocks[:num_accepted]
kv_pool.promote(accepted_blocks)  # 标记为永久
kv_pool.free(draft_blocks[num_accepted:])  # 释放拒绝的
```

### 2. 与前缀缓存（2.1）协同
```python
# Draft/Verify 都可以复用前缀 KV Cache
from sage.llm.sagellm.prefix_reuse import PrefixCache

prefix_cache = PrefixCache()
common_prefix_kv = prefix_cache.match(prompt_tokens)

# Draft 和 Verify 共享前缀 KV
draft_model.forward(input_ids, past_kv=common_prefix_kv)
verify_model.forward(input_ids, past_kv=common_prefix_kv)
```

### 3. 与调度器 IR（2.4）集成
```python
# 调度器下发投机解码配置
ir_node = IRNode(
    type="decode_speculative",
    config={
        "speculative": {
            "draft_model": "Qwen2.5-0.5B",
            "num_tokens": 4,
            "adaptive": True,
        }
    }
)
```

______________________________________________________________________

## 性能指标与验证

### 目标指标
| 指标 | 目标 | 测量方法 |
|------|------|---------|
| 接受率（Acceptance Rate） | 60-80% | 平均每次接受的 token 数 / gamma |
| 加速比（Speedup） | 2-3x (理想), 1.5-2x (实际) | TPOT_baseline / TPOT_speculative |
| Draft 延迟（Draft Latency） | <30% Verify 延迟 | 单次 Draft forward 时间 |
| 显存占用（Memory Overhead） | +20-40% | Draft 模型 + KV Cache |
| 吞吐量（Throughput） | ≥1.8x baseline | tokens/sec |

### Benchmark 设计
```python
# benchmarks/speculative_bench.py
def benchmark_speculative(verify_model, config: SpeculativeConfig):
    """投机解码 Benchmark"""
    # 1. Baseline: 无投机解码
    baseline_metrics = run_autoregressive(verify_model, test_prompts)
    
    # 2. Speculative: 投机解码
    decoder = SpeculativeDecoder()
    decoder.setup(verify_model, config)
    spec_metrics = run_speculative(decoder, test_prompts, config)
    
    # 3. 对比报告
    report = {
        "acceptance_rate": spec_metrics["acceptance_rate"],
        "speedup": baseline_metrics["tpot"] / spec_metrics["tpot"],
        "draft_overhead": spec_metrics["draft_latency"] / spec_metrics["verify_latency"],
        "memory_overhead": (spec_metrics["mem"] - baseline_metrics["mem"]) / baseline_metrics["mem"],
        "throughput_gain": spec_metrics["tokens_per_sec"] / baseline_metrics["tokens_per_sec"],
    }
    return report
```

______________________________________________________________________

## 实现路线图

### Week 11: 核心算法实现
- [ ] 经典 Draft-Verify 流程
- [ ] 独立小模型 Draft
- [ ] 接受/拒绝判定逻辑
- [ ] KV Cache 共享与复用

### Week 12: 高级策略
- [ ] Medusa 多头预测
- [ ] 树状投机（Tree Attention）
- [ ] 自适应投机深度
- [ ] 前 N 层 Draft（BiLD）

### Week 13: 集成与优化
- [ ] 与 KV Pool（2.2）集成
- [ ] 与前缀缓存（2.1）协同
- [ ] Benchmark 与性能验证
- [ ] CUDA Graph 优化（固定 gamma）

______________________________________________________________________

## 参考资源

### 论文
1. **Speculative Decoding**: "Fast Inference from Transformers via Speculative Decoding" (Google, 2023)
2. **Medusa**: "Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads" (2024)
3. **SpecInfer**: "SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification" (2023)
4. **BiLD**: "Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data" (2023)
5. **Lookahead Decoding**: "Break the Sequential Dependency of LLM Inference Using Lookahead Decoding" (2023)

### 代码参考
- Medusa: https://github.com/FasterDecoding/Medusa
- SpecInfer: https://github.com/flexflow/FlexFlow (SpecInfer 分支)
- Lookahead: https://github.com/hao-ai-lab/LookaheadDecoding

______________________________________________________________________

## FAQ

**Q: 投机解码一定能加速吗？**  
A: 不一定。如果接受率 <50%，Draft 的开销可能超过收益。通常在 Draft 模型足够小（<1/5 Verify 参数量）且任务可预测性强时效果最好。

**Q: Draft 和 Verify 必须是同一架构吗？**  
A: 不必须，但同架构（如都是 Qwen2.5）可以共享 KV Cache，显存效率更高。不同架构（如 TinyLlama draft + Llama-70B verify）需要独立管理 KV。

**Q: Medusa 比独立小模型好吗？**  
A: 各有优劣。Medusa 无需额外 Draft 模型（节省显存），但需要训练多个预测头。独立小模型灵活性高，可以任意替换 Draft 模型。

**Q: 树状投机比线性投机好多少？**  
A: 理论上树状投机可以覆盖更多候选路径，接受率提升 10-20%。但实现复杂度高（Tree Attention），且显存开销大。适合高价值场景（如代码生成）。

**Q: 投机解码和批处理冲突吗？**  
A: 可以共存。Batch 内每个请求可以独立决定投机深度（自适应）。但 batch 越大，Draft/Verify 的延迟差异越小，加速效果会减弱。
