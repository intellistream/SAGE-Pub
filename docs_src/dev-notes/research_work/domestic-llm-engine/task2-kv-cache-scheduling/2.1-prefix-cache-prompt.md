# 小方向 2.1：前缀缓存 (Prefix Cache)

> **模块编号**: 2.1 = Phase 2（KV 管理与调度）第 1 个子模块  
> **Git Repo**: `sageLLM-prefix-cache` | **Priority**: P1 | **Phase**: Week 6-8

## 模块定位

### 核心职责
管理和优化 LLM 推理中的前缀 KV Cache 缓存，通过 Radix Tree 实现高效的前缀共享，支持多租户场景下的 prompt 重用，显著降低 prefill 计算开销。

### 为什么需要独立模块
- **性能关键**：System Prompt / Few-shot Examples 通常占 30%-50% token，前缀缓存可节省大量 prefill 计算
- **多租户共享**：多个请求可能共享相同的前缀（如 System Prompt），需要跨会话共享
- **复杂数据结构**：Radix Tree 的实现和维护需要独立模块保证正确性
- **独立研究价值**：前缀缓存算法优化（命中率、内存利用率）可独立发表

### Baseline 参考
1. **vLLM Automatic Prefix Caching** ([vllm-project/vllm#3687](https://github.com/vllm-project/vllm/pull/3687))
   - Radix Tree 实现：`vllm/core/block_manager_v2.py`
   - 自动前缀检测和复用
2. **SGLang RadixAttention** ([sgl-project/sglang](https://github.com/sgl-project/sglang))
   - `radix_cache.py`：高性能 Radix Tree
   - 支持动态 eviction 和 LRU
3. **FlashAttention Prefix Sharing** (理论基础)
   - KV Cache 的物理存储与逻辑共享分离

______________________________________________________________________

## 技术规格

### 输入接口
```python
# core/protocols/prefix_cache.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import List, Optional, Tuple

@dataclass
class PrefixNode:
    """Radix Tree 节点"""
    token_ids: List[int]           # 该节点存储的 token 序列
    kv_block_ids: List[int]        # 对应的 KV Block IDs
    parent: Optional['PrefixNode'] # 父节点
    children: dict[int, 'PrefixNode']  # 子节点（key: 下一个 token）
    ref_count: int = 0             # 引用计数（正在使用的请求数）
    last_access_time: float = 0.0 # 最后访问时间（LRU）
    is_pinned: bool = False        # 是否固定（不可驱逐）

@dataclass
class PrefixMatch:
    """前缀匹配结果"""
    matched_tokens: List[int]      # 匹配的 token 序列
    matched_kv_blocks: List[int]   # 可复用的 KV Block IDs
    match_length: int              # 匹配长度
    remaining_tokens: List[int]    # 剩余需要 prefill 的 tokens

class PrefixCacheProtocol(ABC):
    """前缀缓存协议（所有实现必须遵守）"""
    
    @abstractmethod
    def match_prefix(self, token_ids: List[int]) -> PrefixMatch:
        """查找最长前缀匹配"""
        pass
    
    @abstractmethod
    def insert_prefix(
        self,
        token_ids: List[int],
        kv_block_ids: List[int],
        pin: bool = False
    ) -> PrefixNode:
        """插入新前缀（返回叶子节点）"""
        pass
    
    @abstractmethod
    def evict_lru(self, num_blocks: int) -> List[int]:
        """驱逐 LRU 节点（返回被驱逐的 block IDs）"""
        pass
    
    @abstractmethod
    def pin_prefix(self, token_ids: List[int]) -> bool:
        """固定前缀（防止被驱逐，用于 System Prompt）"""
        pass
    
    @abstractmethod
    def get_hit_rate(self) -> float:
        """获取缓存命中率"""
        pass
```

### 输出接口
```python
# 典型使用场景
from sage.llm.sagellm.prefix_reuse import RadixPrefixCache

cache = RadixPrefixCache(max_blocks=1000)

# 场景 1: 查询前缀匹配
token_ids = [1, 2, 3, 4, 5, 6, 7, 8]  # 新请求的 tokens
match = cache.match_prefix(token_ids)

if match.match_length > 0:
    print(f"复用 {match.match_length} 个 tokens")
    print(f"KV Blocks: {match.matched_kv_blocks}")
    # 只需 prefill remaining_tokens
    prefill_tokens = match.remaining_tokens
else:
    # 完整 prefill
    prefill_tokens = token_ids

# 场景 2: 插入新前缀
new_kv_blocks = [10, 11, 12]  # prefill 后分配的 KV blocks
cache.insert_prefix(token_ids, new_kv_blocks)

# 场景 3: 固定 System Prompt（防止被驱逐）
system_prompt_tokens = [1, 2, 3]  # System Prompt
cache.pin_prefix(system_prompt_tokens)

# 场景 4: 内存不足时驱逐
if memory_pressure:
    evicted_blocks = cache.evict_lru(num_blocks=10)
    kv_pool.free_blocks(evicted_blocks)
```

______________________________________________________________________

## 模块依赖关系

### 上游依赖（此模块需要）
- ✅ **core/protocols/prefix_cache.py** - 前缀缓存协议定义
- ✅ **kvmgr/kv_pool/** - 获取 KV Block 元数据（可选，用于验证）
- ✅ **无其他强依赖** - 独立的数据结构模块

### 下游使用者（依赖此模块的）
- **engines/lmdeploy** - Prefill 阶段前查询缓存
- **kvmgr/scheduler_ir/** - 调度器根据缓存命中率调整策略
- **kvmgr/eviction/** - 内存不足时触发前缀缓存驱逐

### 跨 Phase 依赖
- **comm/kv_transfer/** - 当前缀缓存需要跨节点同步时（可选，分布式场景）
  - 场景：多节点共享 System Prompt 缓存
  - 触发条件：跨节点请求调度时检测到本地无对应前缀
  - 数据流：Node A 的前缀 → kv_transfer 协议 → Node B 的 prefix_cache

### 独立性保证
```python
# 可以单独测试，无需其他模块
pytest kvmgr/prefix_cache/tests/

# 可以用 mock 替代 kv_pool
from unittest.mock import Mock
cache = RadixPrefixCache(max_blocks=100)
match = cache.match_prefix([1, 2, 3])  # 不依赖任何其他功能模块
```

______________________________________________________________________

## 实现方案

### 核心算法

#### 1. Radix Tree 查找（最长前缀匹配）
```python
def match_prefix(self, token_ids: List[int]) -> PrefixMatch:
    """
    从根节点开始，逐层匹配 token_ids
    返回最长匹配的前缀和对应的 KV blocks
    """
    node = self.root
    matched_tokens = []
    matched_kv_blocks = []
    
    i = 0
    while i < len(token_ids):
        token = token_ids[i]
        
        # 查找子节点
        if token not in node.children:
            break  # 匹配结束
        
        node = node.children[token]
        
        # 匹配该节点的 token 序列
        node_tokens = node.token_ids
        j = 0
        while j < len(node_tokens) and i < len(token_ids):
            if node_tokens[j] != token_ids[i]:
                break  # 部分匹配，需要分裂节点
            matched_tokens.append(token_ids[i])
            i += 1
            j += 1
        
        if j == len(node_tokens):
            # 完全匹配该节点
            matched_kv_blocks.extend(node.kv_block_ids)
            node.last_access_time = time.time()
        else:
            # 部分匹配，不复用（需要分裂节点，复杂实现）
            break
    
    return PrefixMatch(
        matched_tokens=matched_tokens,
        matched_kv_blocks=matched_kv_blocks,
        match_length=len(matched_tokens),
        remaining_tokens=token_ids[len(matched_tokens):]
    )
```

#### 2. Radix Tree 插入（支持路径压缩）
```python
def insert_prefix(
    self,
    token_ids: List[int],
    kv_block_ids: List[int],
    pin: bool = False
) -> PrefixNode:
    """
    插入新前缀，必要时分裂节点（路径压缩）
    """
    node = self.root
    i = 0
    
    while i < len(token_ids):
        token = token_ids[i]
        
        if token not in node.children:
            # 创建新子节点
            remaining_tokens = token_ids[i:]
            new_node = PrefixNode(
                token_ids=remaining_tokens,
                kv_block_ids=kv_block_ids,
                parent=node,
                children={},
                is_pinned=pin
            )
            node.children[token] = new_node
            return new_node
        
        child = node.children[token]
        
        # 检查是否需要分裂节点
        match_len = 0
        while (match_len < len(child.token_ids) and 
               i + match_len < len(token_ids) and
               child.token_ids[match_len] == token_ids[i + match_len]):
            match_len += 1
        
        if match_len < len(child.token_ids):
            # 需要分裂节点
            split_node = self._split_node(child, match_len)
            node = split_node
        else:
            node = child
        
        i += match_len
    
    # 到达或创建叶子节点
    node.kv_block_ids = kv_block_ids
    node.is_pinned = pin
    return node
```

#### 3. LRU 驱逐策略
```python
def evict_lru(self, num_blocks: int) -> List[int]:
    """
    驱逐最少使用的前缀节点（LRU）
    返回被驱逐的 KV block IDs
    """
    # 收集所有叶子节点（可驱逐的）
    candidates = []
    self._collect_leaves(self.root, candidates)
    
    # 过滤掉 pinned 和正在使用的
    candidates = [
        node for node in candidates
        if not node.is_pinned and node.ref_count == 0
    ]
    
    # 按 last_access_time 排序（LRU）
    candidates.sort(key=lambda n: n.last_access_time)
    
    evicted_blocks = []
    blocks_freed = 0
    
    for node in candidates:
        if blocks_freed >= num_blocks:
            break
        
        # 驱逐该节点
        evicted_blocks.extend(node.kv_block_ids)
        blocks_freed += len(node.kv_block_ids)
        
        # 从树中删除
        self._remove_node(node)
    
    return evicted_blocks
```

### 数据结构

#### Radix Tree 存储优化
```python
class RadixPrefixCache:
    """
    Radix Tree 实现的前缀缓存
    
    优化：
    1. 路径压缩：每个节点存储 token 序列（不是单个 token）
    2. LRU 快速查找：维护 last_access_time
    3. 引用计数：防止驱逐正在使用的前缀
    """
    def __init__(self, max_blocks: int):
        self.root = PrefixNode(
            token_ids=[],
            kv_block_ids=[],
            parent=None,
            children={}
        )
        self.max_blocks = max_blocks
        self.total_blocks = 0
        
        # 统计信息
        self.total_queries = 0
        self.cache_hits = 0
        self.total_matched_tokens = 0
    
    def get_hit_rate(self) -> float:
        """缓存命中率"""
        if self.total_queries == 0:
            return 0.0
        return self.cache_hits / self.total_queries
    
    def get_avg_match_length(self) -> float:
        """平均匹配长度（tokens）"""
        if self.cache_hits == 0:
            return 0.0
        return self.total_matched_tokens / self.cache_hits
```

______________________________________________________________________

## 性能指标

### 核心指标
| 指标 | 目标值 | 测量方法 |
|------|--------|---------|
| **缓存命中率** | ≥60% | 相同 System Prompt 场景 |
| **匹配延迟** | <10µs | 单次 `match_prefix()` |
| 指标 | 目标值 | vLLM Baseline | 说明 |
|------|--------|---------------|------|
| **缓存命中率** | ≥60% | ~40% | 相同 System Prompt 场景 |
| **查询延迟** | <10µs | ~15µs | 单次 `match_prefix()` 调用 |
| **插入延迟** | <50µs | ~80µs | 单次 `insert_prefix()` |
| **驱逐延迟** | <100µs | ~120µs | 驱逐 10 个节点 |
| **内存开销** | <2MB | ~3MB | 1000 个前缀节点 |
| **Token 节省率** | ≥30% | ~15% | vs 无缓存 Prefill token 数减少 |

### Benchmark 场景
```python
# kvmgr/prefix_cache/benchmarks/prefix_cache_benchmark.py
def benchmark_prefix_cache():
    """测试前缀缓存性能"""
    cache = RadixPrefixCache(max_blocks=1000)
    
    # 测试 1: 缓存命中率（相同 System Prompt）
    system_prompt = [1, 2, 3, 4, 5]  # 假设 System Prompt
    user_prompts = [
        system_prompt + [10, 11, 12],
        system_prompt + [20, 21, 22],
        system_prompt + [30, 31, 32],
    ]
    
    # 第一次请求：冷启动
    match = cache.match_prefix(user_prompts[0])
    assert match.match_length == 0
    cache.insert_prefix(user_prompts[0], [100, 101, 102])
    
    # 后续请求：应该命中
    for prompt in user_prompts[1:]:
        match = cache.match_prefix(prompt)
        assert match.match_length >= len(system_prompt)
    
    hit_rate = cache.get_hit_rate()
    assert hit_rate >= 0.6, f"Hit rate too low: {hit_rate}"
    
    # 测试 2: 查询性能
    start = time.time()
    for _ in range(1000):
        cache.match_prefix(user_prompts[0])
    query_time = (time.time() - start) / 1000
    assert query_time < 10e-6, f"Query too slow: {query_time*1e6:.1f}µs"
```

______________________________________________________________________

## 开发计划

### Week 1: 基础框架（MVP）
- [ ] 定义 `PrefixCacheProtocol` 和数据类
- [ ] 实现基础 Radix Tree（插入、查询，不支持分裂）
- [ ] LRU 驱逐策略
- [ ] 单元测试：简单场景验证

### Week 2: 高级特性
- [ ] 节点分裂（支持部分匹配）
- [ ] 路径压缩优化（减少节点数）
- [ ] Pin 机制（System Prompt 固定）
- [ ] 引用计数（防止驱逐正在使用的前缀）

### Week 3: 集成与优化
- [ ] 与 `engines/lmdeploy` 集成（Prefill Hook）
- [ ] 性能优化：批量查询、并发安全
- [ ] 完整 benchmark suite

______________________________________________________________________

## CLI 使用

### 启用前缀缓存
```bash
# 启动 LLM 服务时启用前缀缓存
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --prefix-cache-size 1000 \
  --prefix-cache-enable

# 配置 LRU 驱逐策略
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --prefix-cache-size 1000 \
  --eviction-policy lru
```

### 查看缓存统计
```bash
# 查看前缀缓存命中率
sage llm cache stats

# 输出示例：
# Prefix Cache Statistics:
#   Total Entries: 127
#   Hit Rate: 62.3%
#   Memory Usage: 1.2MB / 2.0MB
#   Avg Query Time: 8.5µs
```

### 固定 System Prompt
```bash
# 固定常用的 System Prompt（防止被驱逐）
sage llm cache pin --tokens "1,2,3,4,5" --description "Default System Prompt"

# 查看已固定的前缀
sage llm cache list-pinned

# 取消固定
sage llm cache unpin --tokens "1,2,3,4,5"
```

### 性能测试
```bash
# 运行前缀缓存 benchmark
pytest kvmgr/prefix_cache/benchmarks/ -v

# 对比有无前缀缓存的性能
sage llm benchmark --with-prefix-cache
sage llm benchmark --without-prefix-cache
```

______________________________________________________________________

## 集成示例

### 与 LMDeploy 集成
```python
# engines/lmdeploy/hooks/prefill_hook.py
from sage.llm.sagellm.prefix_reuse import RadixPrefixCache

class PrefillWithPrefixCache:
    def __init__(self, model, prefix_cache: RadixPrefixCache):
        self.model = model
        self.prefix_cache = prefix_cache
    
    def prefill(self, token_ids: List[int]) -> List[int]:
        # 查询前缀缓存
        match = self.prefix_cache.match_prefix(token_ids)
        
        if match.match_length > 0:
            # 复用缓存的 KV blocks
            kv_blocks = match.matched_kv_blocks
            self.model.kv_manager.reuse_blocks(kv_blocks)
            
            # 只 prefill 剩余 tokens
            new_kv_blocks = self.model.prefill(match.remaining_tokens)
            
            # 插入完整前缀
            all_kv_blocks = match.matched_kv_blocks + new_kv_blocks
            self.prefix_cache.insert_prefix(token_ids, all_kv_blocks)
        else:
            # 冷启动：完整 prefill
            new_kv_blocks = self.model.prefill(token_ids)
            self.prefix_cache.insert_prefix(token_ids, new_kv_blocks)
        
        return all_kv_blocks
```

### 与 Scheduler IR 协作
```python
# kvmgr/scheduler_ir/optimizer.py
from sage.llm.sagellm.prefix_reuse import RadixPrefixCache

class SchedulerOptimizer:
    def __init__(self, prefix_cache: RadixPrefixCache):
        self.prefix_cache = prefix_cache
    
    def should_batch(self, req1, req2) -> bool:
        """
        根据前缀缓存命中率决定是否批处理
        """
        # 如果两个请求共享前缀，优先批处理
        common_prefix_len = self._longest_common_prefix(
            req1.token_ids,
            req2.token_ids
        )
        
        if common_prefix_len > 10:  # 共享 ≥10 tokens
            return True
        
        # 如果前缀缓存命中率高，可以独立处理
        hit_rate = self.prefix_cache.get_hit_rate()
        if hit_rate > 0.8:
            return False  # 缓存够好，不需要强制批处理
        
        return True  # 默认批处理
```

______________________________________________________________________

## 质量检查清单

### 代码质量
- [ ] 所有公共接口有完整 docstring（Google style）
- [ ] 类型注解覆盖率 100%（`mypy --strict` 通过）
- [ ] 单元测试覆盖率 ≥80%（`pytest --cov`）
- [ ] 集成测试：模拟多租户场景

### 性能验证
- [ ] 缓存命中率 ≥60%（相同 System Prompt）
- [ ] 查询延迟 <10µs（Radix Tree 查找）
- [ ] Token 节省率 ≥30%（vs 无缓存）

### 文档完整性
- [ ] README.md：模块介绍、快速开始、API 文档
- [ ] examples/：多租户共享示例、System Prompt 固定
- [ ] CHANGELOG.md：版本更新记录

______________________________________________________________________

## 参考资源

### 论文
1. **vLLM: Automatic Prefix Caching** (vLLM Blog, 2024)
2. **SGLang: RadixAttention** (arXiv:2312.07104)
3. **FlashAttention-2** - KV Cache 物理/逻辑分离

### 开源项目
1. [vllm-project/vllm](https://github.com/vllm-project/vllm) - `vllm/core/block_manager_v2.py`
2. [sgl-project/sglang](https://github.com/sgl-project/sglang) - `radix_cache.py`
3. [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeed-MII Prefix Sharing

### 工具
- Graphviz - Radix Tree 可视化
- `pytest-benchmark` - 性能测试
- `memory_profiler` - 内存占用分析

______________________________________________________________________

## 常见问题

**Q1: 为什么不用简单的 HashMap 而要用 Radix Tree？**  
A: Radix Tree 支持**最长前缀匹配**，可以找到部分匹配的前缀（如 System Prompt），HashMap 只能全匹配。

**Q2: 如何处理 Token IDs 的哈希冲突？**  
A: Radix Tree 不基于哈希，而是基于 Token 序列的逐层匹配，不存在哈希冲突问题。

**Q3: 驱逐策略为什么用 LRU 而不是 LFU？**  
A: LRU 更适合长尾分布（少数热门前缀频繁访问）。LFU 可能留住早期高频但现在不再使用的前缀。

**Q4: Pin 机制是否会导致内存泄漏？**  
A: Pin 仅用于 System Prompt 等少量固定前缀。提供 `unpin()` 接口允许手动解除，或设置最大 pinned blocks 限制。

**Q5: 如何保证并发安全？**  
A: 使用读写锁（`threading.RLock`）保护 Radix Tree。查询操作用读锁，插入/驱逐用写锁。
