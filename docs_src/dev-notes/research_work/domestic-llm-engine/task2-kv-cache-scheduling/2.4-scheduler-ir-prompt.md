# 小方向 2.4：调度器 IR (Scheduler Intermediate Representation)

> **模块编号**: 2.4 = Phase 2（KV 管理与调度）第 4 个子模块  
> **Git Repo**: `sageLLM-scheduler-ir` | **Priority**: P1 | **Phase**: Week 9-11

## 模块定位

### 核心职责
定义统一的调度器中间表示（IR），抽象不同推理引擎（vLLM/TGI/LMDeploy/SGLang）的调度逻辑，支持跨引擎优化 Pass（Prefix Cache复用、批处理优化、内存感知调度），实现 sageLLM 的引擎无关调度层。

### 为什么需要独立模块
- **引擎异构性**：vLLM/TGI/LMDeploy 调度逻辑差异大，需要统一抽象
- **优化可组合性**：通过 IR Pass 实现优化（如 Prefix Cache Aware Scheduling），复用跨引擎
- **研究价值**：调度 IR 设计和优化 Pass 可独立发表（类似 MLIR/TVM IR）
- **可扩展性**：新的调度策略只需实现 IR Pass，无需修改引擎代码

### Baseline 参考
1. **vLLM Continuous Batching** ([vllm-project/vllm](https://github.com/vllm-project/vllm))
   - `vllm/core/scheduler.py`：基于优先级队列的调度
   - 动态批处理（迭代级调度）
2. **Orca Scheduler** ([osdi23-orca](https://www.usenix.org/conference/osdi23/presentation/yu))
   - 选择性批处理（Selective Batching）
   - 迭代级调度 + 公平性保证
3. **FastServe** ([arxiv.org/abs/2305.05920](https://arxiv.org/abs/2305.05920))
   - 预加载调度（Preemptive Scheduling）
   - Skip-Join Multi-Level Feedback Queue
4. **MLIR/TVM IR** (编译器 IR 设计参考)
   - Multi-level IR 设计
   - Pass Infrastructure

______________________________________________________________________

## 技术规格

### 输入接口
```python
from sagellm.kvmgr.scheduler_ir import (
    SchedulerIR, ScheduleRequest, ScheduleResult, OptimizationPass
)
from dataclasses import dataclass
from typing import List, Optional, Dict, Any
from abc import ABC, abstractmethod
from enum import Enum

class RequestPhase(Enum):
    """请求执行阶段"""
    PREFILL = "prefill"  # Prefill 阶段（编码输入）
    DECODE = "decode"    # Decode 阶段（自回归生成）
    PAUSED = "paused"    # 暂停（内存不足时）

@dataclass
class ScheduleRequest:
    """调度请求"""
    request_id: str  # 唯一请求 ID
    token_ids: List[int]  # 输入 tokens
    max_tokens: int  # 最大生成长度
    priority: int = 0  # 优先级（0=normal, 1=high, 2=urgent）
    arrived_time: float = 0.0  # 到达时间
    
    # KV Cache 相关
    prefix_cache_hit: Optional[int] = None  # 前缀缓存命中长度
    kv_blocks_needed: int = 0  # 需要的 KV Block 数量
    
    # 执行状态
    phase: RequestPhase = RequestPhase.PREFILL
    num_decoded_tokens: int = 0  # 已生成的 token 数

@dataclass
class ScheduleResult:
    """调度结果"""
    scheduled_requests: List[ScheduleRequest]  # 被调度的请求
    batch_size: int  # 批处理大小
    estimated_latency: float  # 预估延迟（ms）
    memory_usage: int  # 预估显存使用（MB）
    optimization_info: Dict[str, Any] = None  # 优化 Pass 信息

class SchedulerIR(ABC):
    """调度器 IR 协议"""
    
    @abstractmethod
    def schedule(
        self,
        pending_requests: List[ScheduleRequest],
        running_requests: List[ScheduleRequest],
        available_memory: int
    ) -> ScheduleResult:
        """
        核心调度接口
        
        Args:
            pending_requests: 等待中的请求
            running_requests: 正在运行的请求（Decode 阶段）
            available_memory: 可用显存（MB）
        
        Returns:
            调度结果（哪些请求被调度）
        """
        pass
    
    @abstractmethod
    def add_optimization_pass(self, pass_obj: 'OptimizationPass') -> None:
        """添加优化 Pass"""
        pass
    
    @abstractmethod
    def remove_optimization_pass(self, pass_name: str) -> None:
        """移除优化 Pass"""
        pass
    
    @abstractmethod
    def get_metrics(self) -> Dict[str, Any]:
        """获取调度指标"""
        pass

class OptimizationPass(ABC):
    """优化 Pass 基类"""
    
    @abstractmethod
    def name(self) -> str:
        """Pass 名称"""
        pass
    
    @abstractmethod
    def apply(
        self,
        pending: List[ScheduleRequest],
        running: List[ScheduleRequest],
        available_memory: int
    ) -> List[ScheduleRequest]:
        """
        应用优化，返回优化后的待调度请求列表
        """
        pass
```

### 输出接口
```python
@dataclass
class ScheduleMetrics:
    """调度指标"""
    total_requests: int  # 总请求数
    avg_latency: float  # 平均延迟（ms）
    p99_latency: float  # P99 延迟（ms）
    throughput: float  # 吞吐量（requests/s）
    batch_efficiency: float  # 批处理效率（0-1）
    memory_utilization: float  # 显存利用率（0-1）
```

______________________________________________________________________

## 模块依赖关系

### 上游依赖（此模块需要）
- ✅ **core/protocols/scheduler_ir.py** - Scheduler IR 协议定义
- ✅ **kvmgr/prefix_cache/** - 查询前缀缓存命中率（Prefix Cache Aware Pass）
- ✅ **kvmgr/kv_pool/** - 查询可用内存（Memory Aware Pass）
- ⚠️ **kvmgr/eviction/** - 内存不足时触发驱逐（可选）

### 下游使用者（依赖此模块的）
- **engines/lmdeploy** - 使用 IR 调度引擎请求
- **engines/vllm_adapter** - vLLM 适配器（将 vLLM 调度逻辑转换为 IR）
- **kvmgr/lifetime/** - 生命周期预测器可作为优化 Pass

### 跨 Phase 依赖
- **comm/topology/** - 跨节点调度时需要拓扑信息（可选，分布式场景）

### 独立性保证
```python
# 可以单独测试，无需真实引擎
pytest kvmgr/scheduler_ir/tests/

# 可以用 mock 请求测试调度逻辑
from unittest.mock import Mock
scheduler = BaseSchedulerIR()
requests = [
    ScheduleRequest(request_id="1", token_ids=[1, 2, 3], max_tokens=10),
    ScheduleRequest(request_id="2", token_ids=[4, 5, 6], max_tokens=20),
]
result = scheduler.schedule(pending_requests=requests, running_requests=[], available_memory=1000)
assert len(result.scheduled_requests) > 0
```

______________________________________________________________________

## 实现方案

### 核心算法

#### 1. Base Scheduler IR（FIFO）
```python
class BaseSchedulerIR(SchedulerIR):
    """基础调度器 IR（FIFO 策略）"""
    
    def __init__(self):
        self.optimization_passes: List[OptimizationPass] = []
        self.metrics = ScheduleMetrics(
            total_requests=0,
            avg_latency=0.0,
            p99_latency=0.0,
            throughput=0.0,
            batch_efficiency=0.0,
            memory_utilization=0.0,
        )
    
    def schedule(
        self,
        pending_requests: List[ScheduleRequest],
        running_requests: List[ScheduleRequest],
        available_memory: int
    ) -> ScheduleResult:
        """
        基础调度逻辑（FIFO + 内存约束）
        
        步骤：
        1. 应用所有优化 Passes
        2. 按到达时间排序（FIFO）
        3. 贪心选择直到内存不足
        """
        # 1. 应用优化 Passes
        optimized_pending = pending_requests.copy()
        for pass_obj in self.optimization_passes:
            optimized_pending = pass_obj.apply(
                optimized_pending,
                running_requests,
                available_memory
            )
        
        # 2. FIFO 排序
        optimized_pending.sort(key=lambda r: r.arrived_time)
        
        # 3. 贪心选择
        scheduled = []
        memory_used = 0
        
        for req in optimized_pending:
            # 计算内存需求
            req_memory = self._estimate_memory(req)
            
            if memory_used + req_memory <= available_memory:
                scheduled.append(req)
                memory_used += req_memory
            else:
                break  # 内存不足，停止调度
        
        # 4. 构建结果
        return ScheduleResult(
            scheduled_requests=scheduled,
            batch_size=len(scheduled),
            estimated_latency=self._estimate_latency(scheduled),
            memory_usage=memory_used,
            optimization_info=self._collect_pass_info()
        )
    
    def _estimate_memory(self, req: ScheduleRequest) -> int:
        """
        估算请求的显存需求
        
        公式：Memory = (input_len + max_tokens) * kv_block_size * 2 (K + V)
        """
        input_len = len(req.token_ids)
        total_len = input_len + req.max_tokens
        
        # 假设每个 token 的 KV Cache 占用 1MB（简化）
        return total_len * 1  # MB
    
    def _estimate_latency(self, scheduled: List[ScheduleRequest]) -> float:
        """
        估算批处理延迟
        
        简化模型：Latency = Prefill_time + Decode_time
        """
        if not scheduled:
            return 0.0
        
        # Prefill 时间与最长序列成正比
        max_input_len = max(len(r.token_ids) for r in scheduled)
        prefill_time = max_input_len * 0.01  # 10µs per token
        
        # Decode 时间与最大生成长度成正比
        max_output_len = max(r.max_tokens for r in scheduled)
        decode_time = max_output_len * 0.02  # 20µs per token
        
        return prefill_time + decode_time
    
    def add_optimization_pass(self, pass_obj: OptimizationPass) -> None:
        """添加优化 Pass"""
        self.optimization_passes.append(pass_obj)
    
    def remove_optimization_pass(self, pass_name: str) -> None:
        """移除优化 Pass"""
        self.optimization_passes = [
            p for p in self.optimization_passes
            if p.name() != pass_name
        ]
    
    def _collect_pass_info(self) -> Dict[str, Any]:
        """收集所有 Pass 的信息"""
        return {
            "active_passes": [p.name() for p in self.optimization_passes],
            "num_passes": len(self.optimization_passes),
        }
```

#### 2. Prefix Cache Aware Pass
```python
class PrefixCacheAwarePass(OptimizationPass):
    """
    前缀缓存感知优化 Pass
    
    策略：
    1. 优先调度有前缀缓存命中的请求（减少 Prefill 时间）
    2. 将相似前缀的请求聚合（提高缓存命中率）
    """
    
    def __init__(self, prefix_cache):
        self.prefix_cache = prefix_cache
    
    def name(self) -> str:
        return "PrefixCacheAware"
    
    def apply(
        self,
        pending: List[ScheduleRequest],
        running: List[ScheduleRequest],
        available_memory: int
    ) -> List[ScheduleRequest]:
        """
        重排请求顺序，优先有缓存命中的
        """
        # 查询每个请求的前缀缓存命中长度
        for req in pending:
            if req.prefix_cache_hit is None:
                match = self.prefix_cache.match_prefix(req.token_ids)
                req.prefix_cache_hit = match.match_length
        
        # 按缓存命中长度降序排序（命中越多越优先）
        pending.sort(key=lambda r: r.prefix_cache_hit or 0, reverse=True)
        
        return pending
```

#### 3. Priority-based Pass
```python
class PriorityBasedPass(OptimizationPass):
    """
    优先级感知调度 Pass
    
    策略：
    1. 高优先级请求先调度
    2. 相同优先级内按 FIFO
    """
    
    def name(self) -> str:
        return "PriorityBased"
    
    def apply(
        self,
        pending: List[ScheduleRequest],
        running: List[ScheduleRequest],
        available_memory: int
    ) -> List[ScheduleRequest]:
        """
        按优先级重排
        """
        # 按 (priority desc, arrived_time asc) 排序
        pending.sort(key=lambda r: (-r.priority, r.arrived_time))
        return pending
```

#### 4. Batching Efficiency Pass
```python
class BatchingEfficiencyPass(OptimizationPass):
    """
    批处理效率优化 Pass
    
    策略：
    1. 将长短序列混合批处理（避免长序列拖慢整体）
    2. 限制批次内的最大长度差异
    """
    
    def __init__(self, max_length_variance: int = 100):
        self.max_length_variance = max_length_variance
    
    def name(self) -> str:
        return "BatchingEfficiency"
    
    def apply(
        self,
        pending: List[ScheduleRequest],
        running: List[ScheduleRequest],
        available_memory: int
    ) -> List[ScheduleRequest]:
        """
        将相似长度的请求分组
        """
        # 按输入长度排序
        pending.sort(key=lambda r: len(r.token_ids))
        
        # 分组：长度差异 ≤ max_length_variance 的放一组
        groups = []
        current_group = []
        
        for req in pending:
            if not current_group:
                current_group.append(req)
            else:
                first_len = len(current_group[0].token_ids)
                req_len = len(req.token_ids)
                
                if abs(req_len - first_len) <= self.max_length_variance:
                    current_group.append(req)
                else:
                    groups.append(current_group)
                    current_group = [req]
        
        if current_group:
            groups.append(current_group)
        
        # 优先调度第一组（长度最接近）
        return groups[0] if groups else []
```

#### 5. Memory Aware Preemption Pass
```python
class MemoryAwarePreemptionPass(OptimizationPass):
    """
    内存感知抢占 Pass
    
    策略：
    1. 当内存不足时，暂停低优先级的 Decode 请求
    2. 为高优先级的 Prefill 请求腾出空间
    """
    
    def __init__(self, eviction_policy):
        self.eviction_policy = eviction_policy
    
    def name(self) -> str:
        return "MemoryAwarePreemption"
    
    def apply(
        self,
        pending: List[ScheduleRequest],
        running: List[ScheduleRequest],
        available_memory: int
    ) -> List[ScheduleRequest]:
        """
        检查是否需要抢占正在运行的请求
        """
        # 计算 pending 的总内存需求
        required_memory = sum(
            self._estimate_memory(r) for r in pending if r.priority >= 1
        )
        
        if required_memory > available_memory:
            # 需要抢占：暂停低优先级的 running 请求
            low_priority_running = [
                r for r in running if r.priority == 0
            ]
            
            # 调用驱逐策略选择受害者
            from sagellm.kvmgr.eviction import EvictionCandidate
            candidates = [
                EvictionCandidate(
                    sequence_id=int(r.request_id),
                    block_ids=list(range(r.kv_blocks_needed)),
                    last_access_time=r.arrived_time,
                    access_count=1,
                    priority=r.priority,
                    is_pinned=False,
                )
                for r in low_priority_running
            ]
            
            victims = self.eviction_policy.select_victims(
                candidates,
                required_blocks=required_memory // 100  # 简化
            )
            
            # 标记被抢占的请求为 PAUSED
            for r in running:
                if int(r.request_id) in victims:
                    r.phase = RequestPhase.PAUSED
        
        return pending
    
    def _estimate_memory(self, req: ScheduleRequest) -> int:
        """简化的内存估算"""
        return (len(req.token_ids) + req.max_tokens) * 1  # MB
```

______________________________________________________________________

## 性能目标

| 指标 | 目标值 | Baseline | 说明 |
|------|--------|----------|------|
| **调度延迟** | <200µs | ~500µs | 单次 schedule() 调用 |
| **吞吐量提升** | +20% | vLLM | vs 原始引擎调度 |
| **P99 延迟** | <500ms | ~800ms | 高优先级请求 P99 |
| **批处理效率** | ≥0.85 | ~0.7 | 实际吞吐 / 理论吞吐 |
| **Pass 开销** | <50µs | N/A | 单个 Pass apply() |

### Benchmark 场景
```python
# kvmgr/scheduler_ir/benchmarks/scheduler_ir_benchmark.py
def benchmark_scheduler_ir():
    """测试 Scheduler IR 性能"""
    
    # 1. 创建调度器和 Passes
    scheduler = BaseSchedulerIR()
    scheduler.add_optimization_pass(PrefixCacheAwarePass(prefix_cache=mock_cache))
    scheduler.add_optimization_pass(PriorityBasedPass())
    scheduler.add_optimization_pass(BatchingEfficiencyPass(max_length_variance=50))
    
    # 2. 模拟请求流
    requests = []
    for i in range(1000):
        req = ScheduleRequest(
            request_id=str(i),
            token_ids=list(range(random.randint(10, 100))),
            max_tokens=random.randint(10, 50),
            priority=random.randint(0, 2),
            arrived_time=time.time() + i * 0.01,
        )
        requests.append(req)
    
    # 3. 测试调度延迟
    start = time.time()
    for batch_start in range(0, len(requests), 32):
        batch = requests[batch_start:batch_start+32]
        result = scheduler.schedule(
            pending_requests=batch,
            running_requests=[],
            available_memory=1000
        )
    schedule_time = (time.time() - start) / (len(requests) // 32)
    
    assert schedule_time < 200e-6, f"Scheduling too slow: {schedule_time*1e6:.1f}µs"
    print(f"Avg schedule time: {schedule_time*1e6:.1f}µs")
    
    # 4. 测试吞吐量
    metrics = scheduler.get_metrics()
    print(f"Throughput: {metrics['throughput']:.1f} req/s")
    print(f"Batch efficiency: {metrics['batch_efficiency']:.2f}")
```

______________________________________________________________________

## CLI 使用

### 配置 Scheduler IR
```bash
# 启动 LLM 服务时配置调度器
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --scheduler-ir base \
  --enable-prefix-cache-aware \
  --enable-priority-based

# 配置批处理策略
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --scheduler-ir base \
  --max-batch-size 64 \
  --max-length-variance 100
```

### 查看调度指标
```bash
# 查看调度器统计
sage llm scheduler stats

# 输出示例：
# Scheduler IR Statistics:
#   Total Requests: 1000
#   Avg Latency: 125.3ms
#   P99 Latency: 487.2ms
#   Throughput: 245.3 req/s
#   Batch Efficiency: 0.87
#   Active Passes: PrefixCacheAware, PriorityBased
```

### 动态添加/移除 Pass
```bash
# 添加优化 Pass
sage llm scheduler add-pass --name BatchingEfficiency --max-variance 50

# 移除优化 Pass
sage llm scheduler remove-pass --name PrefixCacheAware

# 列出所有 Pass
sage llm scheduler list-passes
```

### A/B 测试不同调度策略
```bash
# 对比不同调度器配置
sage llm benchmark \
  --scheduler-configs base,with-prefix-aware,with-all-passes \
  --duration 60
```

______________________________________________________________________

## 开发计划

### Week 1: 基础框架（MVP）
- [ ] 定义 `SchedulerIR` 协议和数据类
- [ ] 实现 Base Scheduler（FIFO + 内存约束）
- [ ] Pass Infrastructure（add/remove/apply）
- [ ] 单元测试：模拟调度场景

### Week 2: 优化 Passes
- [ ] Prefix Cache Aware Pass
- [ ] Priority-based Pass
- [ ] Batching Efficiency Pass
- [ ] Memory Aware Preemption Pass

### Week 3: 集成与引擎适配
- [ ] 与 `engines/lmdeploy` 集成
- [ ] vLLM Adapter（将 vLLM 调度逻辑转换为 IR）
- [ ] 性能优化：快速 Pass 应用
- [ ] 完整 benchmark suite

______________________________________________________________________

## 集成示例

### 与 LMDeploy 集成
```python
# engines/lmdeploy/scheduler_wrapper.py
from sagellm.kvmgr.scheduler_ir import BaseSchedulerIR, ScheduleRequest

class LMDeployScheduler:
    def __init__(self, prefix_cache, kv_pool):
        self.scheduler_ir = BaseSchedulerIR()
        
        # 添加优化 Passes
        from sagellm.kvmgr.scheduler_ir.passes import (
            PrefixCacheAwarePass, PriorityBasedPass
        )
        self.scheduler_ir.add_optimization_pass(PrefixCacheAwarePass(prefix_cache))
        self.scheduler_ir.add_optimization_pass(PriorityBasedPass())
        
        self.kv_pool = kv_pool
    
    def schedule_step(self, pending_requests, running_requests):
        """
        调度一个时间步
        """
        # 查询可用内存
        available_memory = self.kv_pool.get_free_blocks() * 16  # 假设每个 Block 16MB
        
        # 调用 Scheduler IR
        result = self.scheduler_ir.schedule(
            pending_requests=pending_requests,
            running_requests=running_requests,
            available_memory=available_memory
        )
        
        return result.scheduled_requests
```

### vLLM Adapter
```python
# engines/vllm_adapter/scheduler_adapter.py
from vllm.core.scheduler import Scheduler as VLLMScheduler
from sagellm.kvmgr.scheduler_ir import BaseSchedulerIR, ScheduleRequest

class VLLMSchedulerAdapter:
    """
    将 vLLM 的调度逻辑转换为 Scheduler IR
    """
    
    def __init__(self, vllm_scheduler: VLLMScheduler):
        self.vllm_scheduler = vllm_scheduler
        self.scheduler_ir = BaseSchedulerIR()
    
    def schedule(self):
        """
        调用 vLLM 调度，转换为 IR 格式
        """
        # 获取 vLLM 的待调度请求
        vllm_requests = self.vllm_scheduler.waiting
        
        # 转换为 ScheduleRequest
        ir_requests = []
        for vreq in vllm_requests:
            ir_req = ScheduleRequest(
                request_id=str(vreq.request_id),
                token_ids=vreq.prompt_token_ids,
                max_tokens=vreq.max_tokens,
                priority=vreq.priority,
                arrived_time=vreq.arrival_time,
            )
            ir_requests.append(ir_req)
        
        # 调用 Scheduler IR
        result = self.scheduler_ir.schedule(
            pending_requests=ir_requests,
            running_requests=[],
            available_memory=self.vllm_scheduler.cache_config.gpu_memory_utilization
        )
        
        # 将结果转回 vLLM 格式
        return [r.request_id for r in result.scheduled_requests]
```

______________________________________________________________________

## 参考资源

### 论文
1. **Orca: A Distributed Serving System for Transformer-Based Generative Models**
   - 选择性批处理与迭代级调度
2. **vLLM: Efficient Memory Management for Large Language Model Serving**
   - Continuous Batching 实现
3. **FastServe: Fast Distributed Inference Serving for Large Language Models**
   - 预加载调度与 Skip-Join MLFQ
4. **MLIR: A Compiler Infrastructure for the End of Moore's Law**
   - Multi-level IR 设计参考

### 开源项目
1. **vLLM Scheduler**
   - `vllm/core/scheduler.py`
2. **TGI (Text Generation Inference)**
   - HuggingFace 的调度实现
3. **TVM Relay IR**
   - Pass Infrastructure 参考

### 工具
- **pytest-benchmark** - 性能测试
- **Graphviz** - 调度图可视化
- **TensorBoard** - 调度指标监控

______________________________________________________________________

## FAQ

### 1. 为什么需要 IR，直接调用引擎调度器不行吗？
**答**：不同引擎的调度 API 差异大，IR 提供统一抽象。更重要的是，IR 支持优化 Pass 组合，可以跨引擎复用优化策略。

### 2. Scheduler IR 与 MLIR/TVM IR 有什么区别？
**答**：MLIR/TVM 针对计算图，Scheduler IR 针对推理请求流。但设计思想类似：Multi-level 抽象 + Pass Infrastructure。

### 3. 优化 Pass 的执行顺序重要吗？
**答**：重要。例如 PrefixCacheAware Pass 应在 PriorityBased Pass 之前，否则优先级会覆盖缓存优化。可以通过 Pass Manager 管理顺序。

### 4. 如何添加自定义 Pass？
**答**：继承 `OptimizationPass`，实现 `name()` 和 `apply()` 方法。例如：
```python
class CustomPass(OptimizationPass):
    def name(self) -> str:
        return "Custom"
    
    def apply(self, pending, running, memory) -> List[ScheduleRequest]:
        # 自定义逻辑
        return pending
```

### 5. Scheduler IR 对吞吐量的提升有多少？
**答**：取决于工作负载。在有大量相似前缀的场景（如客服对话），PrefixCacheAware Pass 可提升 20%-30% 吞吐量。
