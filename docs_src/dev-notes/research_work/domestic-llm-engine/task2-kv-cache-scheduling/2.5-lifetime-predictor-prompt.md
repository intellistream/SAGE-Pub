# 小方向 2.5：生命周期预测器 (Lifetime Predictor)

> **模块编号**: 2.5 = Phase 2（KV 管理与调度）第 5 个子模块  
> **Git Repo**: `sageLLM-lifetime-predictor` | **Priority**: P2 | **Phase**: Week 11-12

## 模块定位

### 核心职责
使用机器学习（LSTM/Transformer）预测 Sequence 的剩余生命周期（还会生成多少 tokens），为驱逐策略、预取策略和调度优化提供预测信号，实现主动式内存管理和调度优化。

### 为什么需要独立模块
- **预测驱逐**：提前驱逐即将结束的 Sequences，避免驱逐仍需长时间运行的
- **预取优化**：预测长序列的 Sequences，提前分配内存
- **调度优化**：预测执行时间，优化批处理组合
- **研究价值**：LLM 推理的生命周期预测是新兴方向，可独立发表

### Baseline 参考
1. **Orca Scheduler** ([osdi23-orca](https://www.usenix.org/conference/osdi23/presentation/yu))
   - 基于历史统计的简单预测
   - 使用平均值 + 方差估计
2. **FastServe** ([arxiv.org/abs/2305.05920](https://arxiv.org/abs/2305.05920))
   - 基于 prompt 特征的预测
   - 简单线性回归模型
3. **LSTM Time Series Prediction**
   - 经典时间序列预测方法
4. **Transformer for Forecasting**
   - 现代时间序列预测方法

______________________________________________________________________

## 技术规格

### 输入接口
```python
from sage.llm.sagellm.kv_policy.lifetime import LifetimePredictorProtocol, PredictionRequest
from dataclasses import dataclass
from typing import List, Optional
from abc import ABC, abstractmethod

@dataclass
class PredictionRequest:
    """生命周期预测请求"""
    sequence_id: int
    token_ids: List[int]  # 当前输入 tokens
    current_length: int  # 当前生成的 tokens 数
    max_length: int  # 最大生成长度
    prompt_features: Optional[dict] = None  # Prompt 特征（可选）

@dataclass
class PredictionResult:
    """预测结果"""
    estimated_remaining_tokens: int  # 预测还会生成多少 tokens
    confidence: float  # 置信度（0-1）
    estimated_time: float  # 预测剩余时间（秒）

class LifetimePredictorProtocol(ABC):
    """生命周期预测器协议"""
    
    @abstractmethod
    def predict(self, request: PredictionRequest) -> PredictionResult:
        """
        预测 Sequence 的剩余生命周期
        """
        pass
    
    @abstractmethod
    def update(
        self,
        sequence_id: int,
        actual_length: int,
        actual_time: float
    ) -> None:
        """
        更新模型（在线学习）
        """
        pass
    
    @abstractmethod
    def get_metrics(self) -> dict:
        """获取预测指标（准确率、MAE 等）"""
        pass
```

### 输出接口
```python
@dataclass
class PredictionMetrics:
    """预测指标"""
    total_predictions: int  # 总预测次数
    mae: float  # Mean Absolute Error
    mse: float  # Mean Squared Error
    accuracy_within_10_percent: float  # 10% 误差内的准确率
    avg_confidence: float  # 平均置信度
```

______________________________________________________________________

## 模块依赖关系

### 上游依赖（此模块需要）
- ✅ **core/protocols/lifetime.py** - 生命周期预测器协议定义
- ⚠️ **kvmgr/scheduler_ir/** - 获取历史调度数据（可选，用于训练）
- ⚠️ **Prompt 特征提取器** - 提取 prompt 的语义特征（可选，提升准确率）

### 下游使用者（依赖此模块的）
- **kvmgr/eviction/** - 预测式驱逐策略
- **kvmgr/scheduler_ir/** - 调度器使用预测优化批处理
- **kvmgr/kv_pool/** - 预取策略（提前分配内存）

### 跨 Phase 依赖
- **无直接跨 Phase 依赖** - 独立的预测模块

### 独立性保证
```python
# 可以单独测试，无需真实推理
pytest kvmgr/lifetime/tests/

# 可以用历史数据训练和评估
from unittest.mock import Mock
predictor = LSTMLifetimePredictor()
req = PredictionRequest(
    sequence_id=1,
    token_ids=[1, 2, 3],
    current_length=10,
    max_length=100
)
result = predictor.predict(req)
assert result.estimated_remaining_tokens > 0
```

______________________________________________________________________

## 实现方案

### 核心算法

#### 1. Baseline Predictor（统计方法）
```python
class BaselineLifetimePredictor(LifetimePredictorProtocol):
    """
    基于历史统计的简单预测器
    
    策略：
    1. 记录每个 max_length 档位的平均生成长度
    2. 使用移动平均预测
    """
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        
        # 按 max_length 分档记录历史
        self.history: Dict[int, List[int]] = {}  # max_length → [actual_lengths]
        
        self.predictions: List[tuple] = []  # (predicted, actual) pairs
    
    def predict(self, request: PredictionRequest) -> PredictionResult:
        """
        基于历史平均值预测
        """
        max_len_bucket = self._bucket(request.max_length)
        
        if max_len_bucket in self.history and self.history[max_len_bucket]:
            # 使用该档位的历史平均值
            historical_lengths = self.history[max_len_bucket]
            avg_length = sum(historical_lengths) / len(historical_lengths)
            
            # 预测剩余 tokens
            estimated_total = int(avg_length)
            estimated_remaining = max(0, estimated_total - request.current_length)
            
            # 置信度：历史数据越多越高
            confidence = min(1.0, len(historical_lengths) / self.window_size)
        else:
            # 无历史数据：假设生成到 max_length 的 80%
            estimated_total = int(request.max_length * 0.8)
            estimated_remaining = max(0, estimated_total - request.current_length)
            confidence = 0.5  # 低置信度
        
        # 估算时间（假设每个 token 20ms）
        estimated_time = estimated_remaining * 0.02
        
        return PredictionResult(
            estimated_remaining_tokens=estimated_remaining,
            confidence=confidence,
            estimated_time=estimated_time
        )
    
    def update(
        self,
        sequence_id: int,
        actual_length: int,
        actual_time: float
    ) -> None:
        """
        更新历史统计
        """
        # 找到对应的 max_length 档位（通过 sequence_id 反查）
        # 简化：假设可以从 sequence_id 获取 max_length
        max_len_bucket = self._bucket(actual_length)  # 简化
        
        if max_len_bucket not in self.history:
            self.history[max_len_bucket] = []
        
        self.history[max_len_bucket].append(actual_length)
        
        # 保持窗口大小
        if len(self.history[max_len_bucket]) > self.window_size:
            self.history[max_len_bucket].pop(0)
    
    def _bucket(self, length: int) -> int:
        """将长度分桶（每 10 个一档）"""
        return (length // 10) * 10
    
    def get_metrics(self) -> dict:
        """计算预测指标"""
        if not self.predictions:
            return {"mae": 0.0, "mse": 0.0}
        
        errors = [abs(pred - actual) for pred, actual in self.predictions]
        mae = sum(errors) / len(errors)
        mse = sum(e**2 for e in errors) / len(errors)
        
        return {
            "total_predictions": len(self.predictions),
            "mae": mae,
            "mse": mse,
        }
```

#### 2. LSTM Predictor（深度学习）
```python
import torch
import torch.nn as nn

class LSTMLifetimePredictor(LifetimePredictorProtocol):
    """
    基于 LSTM 的生命周期预测器
    
    架构：
    - Input: [current_length, max_length, prompt_embedding]
    - LSTM: 2 layers, hidden_size=128
    - Output: estimated_remaining_tokens
    """
    
    def __init__(self, hidden_size: int = 128, num_layers: int = 2):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # LSTM 模型
        self.model = nn.LSTM(
            input_size=3,  # [current_length, max_length, normalized_time]
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        ).to(self.device)
        
        # 输出层
        self.fc = nn.Linear(hidden_size, 1).to(self.device)
        
        # 优化器
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        self.criterion = nn.MSELoss()
        
        # 训练历史
        self.training_buffer: List[tuple] = []  # (features, target)
        self.buffer_size = 1000
    
    def predict(self, request: PredictionRequest) -> PredictionResult:
        """
        使用 LSTM 预测
        """
        # 构建输入特征
        features = torch.tensor([
            [
                request.current_length,
                request.max_length,
                request.current_length / max(1, request.max_length),  # 完成度
            ]
        ], dtype=torch.float32).unsqueeze(0).to(self.device)  # [1, 1, 3]
        
        # 推理
        self.model.eval()
        with torch.no_grad():
            lstm_out, _ = self.model(features)
            prediction = self.fc(lstm_out[:, -1, :])  # [1, 1]
        
        estimated_remaining = max(0, int(prediction.item()))
        
        # 置信度：基于模型训练次数
        confidence = min(1.0, len(self.training_buffer) / self.buffer_size)
        
        # 估算时间
        estimated_time = estimated_remaining * 0.02
        
        return PredictionResult(
            estimated_remaining_tokens=estimated_remaining,
            confidence=confidence,
            estimated_time=estimated_time
        )
    
    def update(
        self,
        sequence_id: int,
        actual_length: int,
        actual_time: float
    ) -> None:
        """
        在线学习：用实际结果更新模型
        """
        # 构建训练样本（简化：假设可以获取 current_length 和 max_length）
        # 实际应该在预测时保存特征
        # 这里简化为用 actual_length 的 50% 作为 current_length
        current_length = actual_length // 2
        max_length = int(actual_length * 1.2)  # 假设 max_length
        
        features = torch.tensor([
            [
                current_length,
                max_length,
                current_length / max(1, max_length),
            ]
        ], dtype=torch.float32).unsqueeze(0).to(self.device)
        
        target = torch.tensor([[actual_length - current_length]], dtype=torch.float32).to(self.device)
        
        # 添加到训练缓冲区
        self.training_buffer.append((features, target))
        
        # 保持缓冲区大小
        if len(self.training_buffer) > self.buffer_size:
            self.training_buffer.pop(0)
        
        # 每 10 个样本训练一次
        if len(self.training_buffer) % 10 == 0:
            self._train_step()
    
    def _train_step(self):
        """
        单步训练
        """
        if len(self.training_buffer) < 10:
            return
        
        # 随机采样一个 batch
        import random
        batch = random.sample(self.training_buffer, min(32, len(self.training_buffer)))
        
        # 构建 batch
        features_batch = torch.cat([f for f, _ in batch], dim=0)
        targets_batch = torch.cat([t for _, t in batch], dim=0)
        
        # 训练
        self.model.train()
        self.optimizer.zero_grad()
        
        lstm_out, _ = self.model(features_batch)
        predictions = self.fc(lstm_out[:, -1, :])
        
        loss = self.criterion(predictions, targets_batch)
        loss.backward()
        self.optimizer.step()
```

#### 3. Transformer Predictor（现代方法）
```python
class TransformerLifetimePredictor(LifetimePredictorProtocol):
    """
    基于 Transformer 的预测器（使用 attention 机制）
    
    优势：
    - 可以利用 prompt 语义信息
    - 支持更长的历史序列
    """
    
    def __init__(self, d_model: int = 128, nhead: int = 4):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2).to(self.device)
        
        # 输入嵌入
        self.input_proj = nn.Linear(3, d_model).to(self.device)
        
        # 输出层
        self.fc = nn.Linear(d_model, 1).to(self.device)
        
        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.001)
        self.criterion = nn.MSELoss()
    
    def parameters(self):
        """返回所有可训练参数"""
        return list(self.transformer.parameters()) + \
               list(self.input_proj.parameters()) + \
               list(self.fc.parameters())
    
    def predict(self, request: PredictionRequest) -> PredictionResult:
        """使用 Transformer 预测"""
        # 构建输入
        features = torch.tensor([
            [
                request.current_length,
                request.max_length,
                request.current_length / max(1, request.max_length),
            ]
        ], dtype=torch.float32).unsqueeze(0).to(self.device)  # [1, 1, 3]
        
        # 嵌入
        embedded = self.input_proj(features)  # [1, 1, d_model]
        
        # Transformer
        self.transformer.eval()
        with torch.no_grad():
            output = self.transformer(embedded)  # [1, 1, d_model]
            prediction = self.fc(output[:, -1, :])  # [1, 1]
        
        estimated_remaining = max(0, int(prediction.item()))
        confidence = 0.8  # 假设较高置信度
        estimated_time = estimated_remaining * 0.02
        
        return PredictionResult(
            estimated_remaining_tokens=estimated_remaining,
            confidence=confidence,
            estimated_time=estimated_time
        )
    
    def update(self, sequence_id: int, actual_length: int, actual_time: float) -> None:
        """在线学习（简化实现）"""
        pass  # 与 LSTM 类似
```

______________________________________________________________________

## 性能目标

| 指标 | 目标值 | Baseline | 说明 |
|------|--------|----------|------|
| **预测 MAE** | <10 tokens | ~25 tokens | Mean Absolute Error |
| **10% 准确率** | ≥70% | ~50% | 误差在 10% 内的比例 |
| **推理延迟** | <1ms | ~5ms | 单次 predict() 调用 |
| **吞吐量提升** | +10% | N/A | vs 无预测的驱逐策略 |
| **内存节省** | +5% | N/A | vs 保守的内存分配 |

### Benchmark 场景
```python
# kvmgr/lifetime/benchmarks/lifetime_benchmark.py
def benchmark_lifetime_predictor():
    """测试生命周期预测器性能"""
    
    # 1. 生成测试数据（模拟真实分布）
    test_data = []
    for i in range(1000):
        max_length = random.randint(50, 500)
        # 实际长度：max_length 的 60%-90%
        actual_length = int(max_length * random.uniform(0.6, 0.9))
        
        test_data.append({
            "sequence_id": i,
            "max_length": max_length,
            "actual_length": actual_length,
        })
    
    # 2. 测试不同预测器
    predictors = {
        "Baseline": BaselineLifetimePredictor(),
        "LSTM": LSTMLifetimePredictor(),
        "Transformer": TransformerLifetimePredictor(),
    }
    
    results = {}
    for name, predictor in predictors.items():
        errors = []
        
        for data in test_data:
            # 模拟预测（在 50% 进度时预测）
            current_length = data["actual_length"] // 2
            
            req = PredictionRequest(
                sequence_id=data["sequence_id"],
                token_ids=list(range(current_length)),
                current_length=current_length,
                max_length=data["max_length"],
            )
            
            result = predictor.predict(req)
            
            # 计算误差
            actual_remaining = data["actual_length"] - current_length
            error = abs(result.estimated_remaining_tokens - actual_remaining)
            errors.append(error)
            
            # 更新模型（在线学习）
            predictor.update(
                sequence_id=data["sequence_id"],
                actual_length=data["actual_length"],
                actual_time=data["actual_length"] * 0.02
            )
        
        # 统计
        mae = sum(errors) / len(errors)
        within_10_percent = sum(1 for e, d in zip(errors, test_data) if e < d["actual_length"] * 0.1) / len(errors)
        
        results[name] = {
            "MAE": mae,
            "10% Accuracy": within_10_percent,
        }
        
        print(f"{name}: MAE={mae:.1f}, 10% Accuracy={within_10_percent:.1%}")
    
    assert results["LSTM"]["MAE"] < 10, "LSTM MAE too high"
```

______________________________________________________________________

## CLI 使用

### 启用生命周期预测
```bash
# 启动 LLM 服务时启用预测器
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --enable-lifetime-prediction \
  --predictor-type lstm

# 配置预测器参数
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --predictor-type transformer \
  --predictor-hidden-size 256
```

### 查看预测指标
```bash
# 查看预测器统计
sage llm lifetime stats

# 输出示例：
# Lifetime Predictor Statistics:
#   Predictor Type: LSTM
#   Total Predictions: 1000
#   MAE: 8.3 tokens
#   10% Accuracy: 72.5%
#   Avg Confidence: 0.85
```

### 训练预测器
```bash
# 使用历史数据训练
sage llm lifetime train --data-path /path/to/history.csv

# 在线学习模式（自动更新）
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --predictor-type lstm \
  --enable-online-learning
```

### 导出预测模型
```bash
# 导出训练好的模型
sage llm lifetime export --output-path /path/to/model.pt

# 加载预训练模型
sage llm engine start Qwen/Qwen2.5-7B-Instruct \
  --engine-kind llm \
  --predictor-type lstm \
  --predictor-checkpoint /path/to/model.pt
```

______________________________________________________________________

## 开发计划

### Week 1: 基础框架（MVP）
- [ ] 定义 `LifetimePredictorProtocol` 和数据类
- [ ] 实现 Baseline Predictor（统计方法）
- [ ] 数据收集接口（记录实际生命周期）
- [ ] 单元测试：模拟预测场景

### Week 2: 深度学习模型
- [ ] 实现 LSTM Predictor
- [ ] 在线学习框架
- [ ] 模型保存/加载
- [ ] Benchmark suite

### Week 3: 集成与优化
- [ ] 与 `kvmgr/eviction` 集成（预测式驱逐）
- [ ] 与 `kvmgr/scheduler_ir` 集成（预测优化 Pass）
- [ ] 实现 Transformer Predictor（可选）
- [ ] 性能优化：推理加速、批量预测

______________________________________________________________________

## 集成示例

### 与 Eviction Policy 集成
```python
# kvmgr/eviction/predictive_eviction.py
from sage.llm.sagellm.kv_policy.lifetime import LSTMLifetimePredictor, PredictionRequest

class PredictiveEvictionWithML:
    def __init__(self):
        self.lifetime_predictor = LSTMLifetimePredictor()
    
    def select_victims(
        self,
        candidates: List[EvictionCandidate],
        required_blocks: int
    ) -> List[int]:
        """
        使用预测器增强驱逐决策
        """
        # 为每个候选项预测剩余生命周期
        for candidate in candidates:
            req = PredictionRequest(
                sequence_id=candidate.sequence_id,
                token_ids=[],  # 简化
                current_length=candidate.sequence_length,
                max_length=candidate.max_length,
            )
            
            result = self.lifetime_predictor.predict(req)
            candidate.estimated_lifetime = result.estimated_time
        
        # 优先驱逐剩余生命短的
        candidates.sort(key=lambda c: c.estimated_lifetime or float('inf'))
        
        victims = []
        freed_blocks = 0
        
        for candidate in candidates:
            if candidate.is_pinned:
                continue
            
            victims.append(candidate.sequence_id)
            freed_blocks += len(candidate.block_ids)
            
            if freed_blocks >= required_blocks:
                break
        
        return victims
```

### 与 Scheduler IR 集成
```python
# kvmgr/scheduler_ir/passes/lifetime_aware_pass.py
from sage.llm.sagellm.kv_policy.lifetime import LifetimePredictorProtocol
from sage.llm.sagellm.scheduler_ir import OptimizationPass

class LifetimeAwarePass(OptimizationPass):
    """
    基于生命周期预测的调度优化 Pass
    
    策略：
    - 优先调度短序列（快速完成释放资源）
    - 长序列单独批处理（避免拖慢短序列）
    """
    
    def __init__(self, lifetime_predictor: LifetimePredictorProtocol):
        self.lifetime_predictor = lifetime_predictor
    
    def name(self) -> str:
        return "LifetimeAware"
    
    def apply(
        self,
        pending: List[ScheduleRequest],
        running: List[ScheduleRequest],
        available_memory: int
    ) -> List[ScheduleRequest]:
        """
        根据预测的生命周期重排请求
        """
        # 为每个请求预测剩余生命周期
        for req in pending:
            pred_req = PredictionRequest(
                sequence_id=int(req.request_id),
                token_ids=req.token_ids,
                current_length=req.num_decoded_tokens,
                max_length=req.max_tokens,
            )
            
            result = self.lifetime_predictor.predict(pred_req)
            req.estimated_remaining_tokens = result.estimated_remaining_tokens
        
        # 按预测的剩余 tokens 升序排序（短序列优先）
        pending.sort(key=lambda r: getattr(r, 'estimated_remaining_tokens', float('inf')))
        
        return pending
```

______________________________________________________________________

## 参考资源

### 论文
1. **Orca: A Distributed Serving System for Transformer-Based Generative Models**
   - 基于历史统计的生命周期预测
2. **FastServe: Fast Distributed Inference Serving for Large Language Models**
   - 基于 prompt 特征的预测
3. **LSTM for Time Series Forecasting**
   - 经典 LSTM 时间序列预测方法
4. **Attention Is All You Need**
   - Transformer 架构参考

### 开源项目
1. **PyTorch LSTM Examples**
   - 时间序列预测示例
2. **HuggingFace Transformers**
   - Transformer 模型库
3. **Scikit-learn Time Series**
   - 传统时间序列方法

### 工具
- **PyTorch** - 深度学习框架
- **TensorBoard** - 训练可视化
- **pytest-benchmark** - 性能测试

______________________________________________________________________

## FAQ

### 1. 生命周期预测的准确率有多高？
**答**：取决于工作负载的规律性。对于规律的批处理任务，MAE 可达 5-10 tokens；对于随机的交互式对话，MAE 可能 20+ tokens。

### 2. 预测错误会有什么影响？
**答**：
- **过高估计**：可能过早驱逐，导致 re-prefill
- **过低估计**：可能推迟驱逐，导致内存不足
- 可以通过置信度阈值控制风险（只有高置信度才使用预测）

### 3. LSTM vs Transformer，哪个更好？
**答**：
- **LSTM**：轻量，推理快（<1ms），适合实时预测
- **Transformer**：可利用 prompt 语义，准确率更高，但推理慢（2-5ms）
- 推荐：先用 LSTM，如果准确率不足再升级到 Transformer

### 4. 如何收集训练数据？
**答**：在推理过程中自动记录：
- 请求到达时：记录 current_length, max_length
- 请求完成时：记录 actual_length, actual_time
- 定期导出训练数据，离线训练或在线学习

### 5. 预测器对吞吐量的提升有多少？
**答**：取决于驱逐频率。如果驱逐频繁（内存紧张），预测式驱逐可减少 20%-30% 的 re-prefill，吞吐量提升 10%-15%。
