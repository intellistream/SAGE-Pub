# 小方向 1.4：计算通信重叠 (Compute-Communication Overlap)

> **Git Repo**: `sageLLM-overlap-pipeline` | **Priority**: P1 | **Phase**: Week 3-5

## 模块定位

### 核心职责
实现 Prefill/Decode 阶段的计算-通信重叠、CUDA Stream 管理、依赖调度，隐藏通信延迟。

### Baseline 参考
1. **Megatron-LM Pipeline Parallelism** ([NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM))
2. **FlexFlow Parallelization** ([flexflow/FlexFlow](https://github.com/flexflow/FlexFlow))
3. **DeepSpeed Pipeline Engine** ([microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed))

______________________________________________________________________

## 技术规格

### 接口定义
```python
# core/protocols/overlap_manager.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

class TaskType(Enum):
    COMPUTE = "compute"
    COMMUNICATION = "communication"

@dataclass
class Task:
    """计算或通信任务"""
    task_id: int
    task_type: TaskType
    func: callable
    args: tuple
    dependencies: list[int]  # 依赖的任务 ID
    stream: int | None = None

class OverlapManagerProtocol(ABC):
    """重叠管理器协议"""
    
    @abstractmethod
    def schedule(
        self,
        compute_tasks: list[Task],
        comm_tasks: list[Task]
    ) -> None:
        """调度计算和通信任务，最大化重叠"""
        pass
    
    @abstractmethod
    def get_overlap_efficiency(self) -> float:
        """获取重叠效率（重叠时间 / 总通信时间）"""
        pass
```

### 使用示例
```python
from sagellm.comm.overlap import OverlapManager

manager = OverlapManager(num_streams=3)

# 定义任务
compute_task = Task(
    task_id=1,
    task_type=TaskType.COMPUTE,
    func=model.forward,
    args=(input_tensor,),
    dependencies=[]
)

comm_task = Task(
    task_id=2,
    task_type=TaskType.COMMUNICATION,
    func=backend.all_reduce,
    args=(grad_tensor,),
    dependencies=[1]  # 依赖计算任务 1
)

# 调度执行（自动重叠）
manager.schedule([compute_task], [comm_task])

# 重叠效率 70% 表示 70% 的通信被计算隐藏
efficiency = manager.get_overlap_efficiency()
```

______________________________________________________________________

## 模块依赖关系

### 上游依赖（此模块需要）
- ✅ **core/protocols/overlap_manager.py** - 重叠管理器协议
- ✅ **collective_ops/** - 异步通信原语（async_op=True）
- ✅ **torch.cuda.Stream** - CUDA Stream 管理

### 下游使用者（依赖此模块的）
- **engines/lmdeploy** - Tensor Parallel 层重叠
- **accel/kernel_fusion/** - Attention kernel 与通信重叠

### 独立性保证
```python
# 可以用 mock 通信后端测试
from unittest.mock import Mock

mock_comm_backend = Mock()
mock_comm_backend.all_reduce.return_value = torch.randn(10)

manager = OverlapManager()
manager.schedule(compute_tasks, comm_tasks)  # 使用 mock 后端
```

______________________________________________________________________

## 实现方案

### 核心算法

#### 1. DAG 依赖调度
```python
def schedule_dag(tasks: list[Task]) -> list[list[Task]]:
    """
    基于依赖关系生成执行计划（拓扑排序）
    
    返回：分层的任务列表（每层可并行执行）
    """
    # 构建依赖图
    graph = defaultdict(list)
    in_degree = {t.task_id: 0 for t in tasks}
    
    for task in tasks:
        for dep in task.dependencies:
            graph[dep].append(task.task_id)
            in_degree[task.task_id] += 1
    
    # 拓扑排序（Kahn 算法）
    queue = deque([tid for tid, deg in in_degree.items() if deg == 0])
    schedule = []
    current_level = []
    
    while queue:
        tid = queue.popleft()
        current_level.append(tasks[tid])
        
        for neighbor in graph[tid]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)
        
        if not queue and in_degree:
            schedule.append(current_level)
            current_level = []
    
    return schedule
```

#### 2. CUDA Stream 管理
```python
class StreamManager:
    """CUDA Stream 池管理"""
    
    def __init__(self, num_streams: int = 3):
        self.compute_stream = torch.cuda.Stream()
        self.comm_streams = [torch.cuda.Stream() for _ in range(num_streams - 1)]
        self.current_comm_stream = 0
    
    def allocate_stream(self, task_type: TaskType) -> torch.cuda.Stream:
        """为任务分配 stream"""
        if task_type == TaskType.COMPUTE:
            return self.compute_stream
        else:
            # Round-robin 分配通信 stream
            stream = self.comm_streams[self.current_comm_stream]
            self.current_comm_stream = (self.current_comm_stream + 1) % len(self.comm_streams)
            return stream
    
    def synchronize_all(self):
        """同步所有 stream"""
        self.compute_stream.synchronize()
        for stream in self.comm_streams:
            stream.synchronize()
```

#### 3. Tensor Parallel 重叠示例
```python
class TensorParallelWithOverlap(nn.Module):
    """支持计算-通信重叠的 Tensor Parallel 层"""
    
    def __init__(self, in_features, out_features, world_size):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features // world_size, in_features))
        self.overlap_manager = OverlapManager()
        # 使用正确的命名空间
        from sagellm.comm.collective_ops import create_comm_backend
        self.comm_backend = create_comm_backend("nccl")
    
    def forward(self, x):
        # 分配 stream
        compute_stream = self.overlap_manager.get_compute_stream()
        comm_stream = self.overlap_manager.get_comm_stream()
        
        with torch.cuda.stream(compute_stream):
            # 计算本地输出
            local_output = F.linear(x, self.weight)
        
        # 立即启动通信（与下一层计算重叠）
        with torch.cuda.stream(comm_stream):
            handle = self.comm_backend.all_reduce(local_output, async_op=True)
        
        # 等待通信完成
        handle.wait()
        
        return local_output
```

______________________________________________________________________

## 性能指标

| 指标 | 目标值 | 测量方法 |
|------|--------|---------|
| **重叠效率** | ≥70% | 重叠时间 / 总通信时间 |
| **Idle Time** | <10% | GPU 空闲时间 / 总时间 |
| **调度延迟** | <100µs | DAG 调度算法执行时间 |
| **吞吐提升** | ≥20% | 有重叠 vs 无重叠 |

______________________________________________________________________

## 开发计划

### Week 1: Stream 管理
- [ ] `StreamManager` 实现
- [ ] 任务 DAG 调度器
- [ ] 单元测试：依赖正确性

### Week 2: 重叠策略
- [ ] Tensor Parallel 重叠
- [ ] Pipeline Parallel 重叠
- [ ] 自动依赖分析

### Week 3: 性能优化
- [ ] 动态 stream 数量调整
- [ ] 与 `collective_ops/` 集成
- [ ] Benchmark：重叠效率测量

______________________________________________________________________

## 参考资源

### 论文
1. **GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism** (NeurIPS 2019)
2. **Megatron-LM: Training Multi-Billion Parameter Language Models** (2020)
3. **FlexFlow: A Flexible Dataflow Accelerator Architecture** (2019)
