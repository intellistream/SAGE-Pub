# 小方向 1.5：国产互联适配 (Domestic Interconnect Adaptation)

> **Git Repo**: `sageLLM-domestic-interconnect` | **Priority**: P0 (核心) | **Phase**: Week 3-5

## ⚠️ 重要性说明

**本模块是"面向国产算力"的核心差异化能力**，决定了 sageLLM 能否真正在国产硬件上发挥性能。升级为 **P0 优先级**，与通信模块（1.1-1.4）并行开发。

## 模块定位

### 核心职责
适配国产加速器的专有互联协议（昇腾 HCCS、寒武纪 MLU-Link、海光 xGMI、昆仑 XPU-Link），实现与 NCCL **等效或超越**的性能。

### 为什么是 P0？
- **项目定位**：名称中的"面向国产算力"不是口号，必须有真实的国产硬件支持
- **市场需求**：政企客户国产化率要求（通常 ≥80%）
- **生态空白**：目前国产推理引擎（如 LightLLM 昇腾版）性能与 vLLM CUDA 版差距明显
- **技术门槛**：通信层是国产硬件性能的最大瓶颈，需要深度优化

### Baseline 参考
1. **Huawei CANN HCCL** (Huawei Collective Communication Library)
   - 昇腾官方集合通信库，基于 HCCS 互联
2. **Cambricon CNCL** (Cambricon Collective Communication Library)
   - 寒武纪官方集合通信库，基于 MLU-Link
3. **AMD RCCL** (ROCm Communication Collective Library)
   - 海光 DCU 使用的 ROCm 生态
4. **Baidu BKCL** (Baidu Kunlun Communication Library)
   - 昆仑芯官方通信库

______________________________________________________________________

## 技术规格

### 接口定义
```python
# core/protocols/domestic_backend.py
from sage.llm.engines.sagellm.comm_backend.collective_ops import CommBackendProtocol

class DomesticCommBackend(CommBackendProtocol):
    """国产互联后端（实现统一的 CommBackendProtocol）"""
    
    def __init__(self, vendor: str):
        """
        vendor: "ascend" | "cambricon" | "hygon" | "kunlun"
        """
        self.vendor = vendor
        self._init_vendor_backend()
    
    def _init_vendor_backend(self):
        """根据厂商初始化对应的 SDK"""
        if self.vendor == "ascend":
            import acl
            self.native_backend = HCCLBackend()
        elif self.vendor == "cambricon":
            import cnrt
            self.native_backend = CNCLBackend()
        # ... 其他厂商
    
    def all_reduce(self, tensor, op="sum", async_op=False):
        """委托给厂商原生实现"""
        return self.native_backend.all_reduce(tensor, op, async_op)
```

### 使用示例
```python
from sage.llm.engines.sagellm.comm_backend.domestic import create_domestic_backend

# 自动检测硬件类型
backend = create_domestic_backend(vendor="auto")

# 使用方式与 NCCL 完全一致
tensor = torch.randn(1024, device="npu:0")  # 昇腾 NPU
result = backend.all_reduce(tensor, op="sum")
```

______________________________________________________________________

## 模块依赖关系

### 上游依赖（此模块需要）
- ✅ **core/protocols/comm_backend.py** - 统一通信协议（实现 CommBackendProtocol）
- ✅ **厂商 SDK** - acl（昇腾）, torch_mlu（寒武纪）, ROCm（海光）
- ⚠️ **可选**: topology/ 用于硬件检测

### 可选依赖声明（包管理）
- 所有厂商 SDK/适配包必须写入对应包的 `pyproject.toml` 的可选 extra（如 `ascend`, `cambricon`），禁止手工
    `pip install`；默认安装不应强拉入厂商依赖。
- 典型映射：
    - 昇腾：`acl`, `torch==<ascend build>`
    - 寒武纪：`torch-mlu`/`torch_mlu`
    - 海光/ROCm：`torch==<rocm build>`
    - 昆仑：厂商 SDK 包（按官方命名）
- 在 README 中说明：仅当选择对应 backend 时才需要安装相关 extra。

### 下游使用者（依赖此模块的）
- **collective_ops/** - 作为可选后端（auto 模式下自动选择）
- **engines/lmdeploy** - 国产加速器部署场景

### 独立性保证
```python
# 可以单独测试（需要对应硬件）
if detect_hardware_vendor() == "ascend":
    backend = HCCLBackend(rank=0, world_size=8)
    tensor = torch.randn(1024, device="npu:0")
    result = backend.all_reduce(tensor)
    assert result.device.type == "npu"

# 也可以通过统一接口使用
from sage.llm.engines.sagellm.comm_backend.collective_ops import create_comm_backend
backend = create_comm_backend(backend_type="auto")  # 自动选择 HCCL/CNCL
```

______________________________________________________________________

## 实现方案

### 厂商适配层

#### 1. 昇腾 HCCL 适配
```python
import acl

class HCCLBackend(CommBackendProtocol):
    """昇腾 HCCL 后端"""
    
    def __init__(self, rank: int, world_size: int):
        # 初始化 HCCL
        acl.init()
        self.rank = rank
        self.world_size = world_size
        self.comm = acl.hccl.create_comm(rank, world_size)
    
    def all_reduce(self, tensor, op="sum", async_op=False):
        # 转换为 HCCL 操作符
        hccl_op = {"sum": acl.hccl.HcclReduceOp.SUM}[op]
        
        # 执行 all_reduce
        if async_op:
            stream = acl.rt.create_stream()
            acl.hccl.all_reduce(
                tensor.data_ptr(),
                tensor.numel(),
                hccl_op,
                self.comm,
                stream
            )
            return stream  # 返回 handle
        else:
            acl.hccl.all_reduce_sync(tensor.data_ptr(), tensor.numel(), hccl_op, self.comm)
            return tensor
```

#### 2. 寒武纪 CNCL 适配
```python
import torch_mlu  # 寒武纪 PyTorch 扩展

class CNCLBackend(CommBackendProtocol):
    """寒武纪 CNCL 后端"""
    
    def __init__(self, rank: int, world_size: int):
        torch_mlu.set_device(rank)
        self.rank = rank
        self.world_size = world_size
        # 初始化 CNCL communicator
        self.comm = torch_mlu.distributed.new_group(list(range(world_size)))
    
    def all_reduce(self, tensor, op="sum", async_op=False):
        # 使用 torch_mlu 的分布式原语
        op_map = {"sum": torch_mlu.distributed.ReduceOp.SUM}
        
        if async_op:
            return torch_mlu.distributed.all_reduce(
                tensor, op=op_map[op], group=self.comm, async_op=True
            )
        else:
            torch_mlu.distributed.all_reduce(tensor, op=op_map[op], group=self.comm)
            return tensor
```

#### 3. 自动硬件检测
```python
def detect_hardware_vendor() -> str:
    """自动检测硬件类型"""
    try:
        import acl
        acl.init()
        return "ascend"
    except ImportError:
        pass
    
    try:
        import torch_mlu
        if torch.mlu.is_available():
            return "cambricon"
    except ImportError:
        pass
    
    try:
        import torch
        if torch.cuda.is_available():
            device_name = torch.cuda.get_device_name(0)
            if "Hygon" in device_name:
                return "hygon"
    except:
        pass
    
    return "unknown"

def create_domestic_backend(vendor: str = "auto"):
    """工厂函数：创建国产后端"""
    if vendor == "auto":
        vendor = detect_hardware_vendor()
    
    backend_map = {
        "ascend": HCCLBackend,
        "cambricon": CNCLBackend,
        "hygon": HygonBackend,
        "kunlun": KunlunBackend,
    }
    
    return backend_map[vendor]()

______________________________________________________________________

## 国产卡快速自检与烟囱测试（建议）

1. **驱动/固件**：确认厂商驱动与固件版本与 SDK 匹配（如 `npu-smi info` / `cnmon` / `rocminfo`）。
2. **通信通道**：使用官方带宽测试工具（如 HCCL bandwidth、CNCL bench、ROCm rccl-tests）跑 1×1、8×8 all_reduce，带宽 ≥80% 峰值视为通过。
3. **PyTorch/SDK 版本匹配**：校验 `torch` 构建标识（ascend/rocm/mlu）与驱动一致。
4. **sageLLM 通路冒烟**：在 `comm_backend` mock 之外，运行最小 all_reduce/all_gather 样例，验证返回值、耗时、错误码（需在对应硬件环境执行）。
5. **日志与监控**：确保通信层暴露 HCCL/CNCL/RCCL 日志，接入统一 telemetry，便于回归基线对比。

______________________________________________________________________

## 性能指标

| 指标 | 目标值 | 测量方法 |
|------|--------|---------|
| **带宽利用率** | ≥85% 厂商声称带宽 | All-reduce benchmark |
| **延迟** | <50µs（节点内，1MB） | Micro-benchmark |
| **功能对齐** | 100%（与 NCCL 接口一致） | 单元测试 |
| **稳定性** | 无 crash（10000 次通信） | 压力测试 |
| **与 NCCL 对比** | ≤10% 性能差距 | 同场景对比测试 |

______________________________________________________________________

## 国产硬件详细适配方案

### 昇腾 910B (P0)

**硬件规格**：
- 互联：HCCS 400GB/s（双向）
- 算力：FP16 312 TFLOPS
- 显存：32GB HBM2e

**SDK 版本要求**：
```
CANN >= 7.0.0
torch_npu >= 2.1.0
driver >= 23.0.0
```

**Kernel 适配要点**：
- 使用 Ascend C 编写自定义算子
- 注意 NPU 与 CUDA 的 memory layout 差异
- HCCL 支持异步操作但需显式 stream 管理

**已知问题与 Workaround**：
- 问题：HCCL 在 batch_size=1 时有额外 overhead
- 解决：使用 micro-batching 或 padding

### 寒武纪 MLU370/590 (P0)

**硬件规格**：
- 互联：MLU-Link 200GB/s
- 算力：FP16 256 TFLOPS (MLU370)
- 显存：24GB GDDR6

**SDK 版本要求**：
```
CNToolkit >= 3.0.0
torch_mlu >= 1.10.0
driver >= 5.10.0
```

**Kernel 适配要点**：
- 使用 BANG 语言编写自定义算子
- MagicMind 提供图优化能力
- CNCL 接口与 NCCL 基本一致

### 海光 DCU Z100 (P1)

**硬件规格**：
- 互联：xGMI / Infinity Fabric
- 基于 ROCm 生态

**SDK 版本要求**：
```
ROCm >= 5.4.0
torch (ROCm build)
MIOpen >= 2.18.0
```

**Kernel 适配要点**：
- 使用 HIP 编写（与 CUDA 高度兼容）
- RCCL 是 NCCL 的 ROCm 移植
- 大部分 CUDA 代码可直接转换

### 昆仑芯 R300 (P1)

**硬件规格**：
- 互联：XPU-Link
- 百度自研架构

**SDK 版本要求**：
```
XPU SDK >= 2.6.0
paddle_custom_device
```

**Kernel 适配要点**：
- 使用 XPU SDK 编写算子
- 通过 PaddlePaddle 接入为主
- PyTorch 支持相对不成熟

______________________________________________________________________

## 开发计划（升级版）

### Week 1: 昇腾 910B 适配（P0）
- [ ] HCCL 后端完整实现（all_reduce/all_gather/reduce_scatter/send/recv）
- [ ] 单元测试：8 卡昇腾 910B
- [ ] 性能 benchmark：对比 vLLM CUDA 版

### Week 2: 寒武纪 MLU370 适配（P0）
- [ ] CNCL 后端实现
- [ ] 与 HCCL 性能对比
- [ ] 集成测试：LMDeploy + CNCL

### Week 3: 海光/昆仑 + 集成测试
- [ ] 海光 DCU RCCL 后端（如有硬件）
- [ ] 昆仑芯 BKCL 后端（如有硬件）
- [ ] 跨厂商混合部署测试
- [ ] 文档完善：各厂商安装指南

______________________________________________________________________

## 参考资源

### 官方文档
1. **Huawei CANN HCCL**: [Ascend Community](https://www.hiascend.com/)
   - HCCL API 参考: [HCCL Programmer's Guide](https://support.huawei.com/enterprise/zh/doc/EDOC1100336282)
2. **Cambricon CNCL**: [Cambricon Developer Docs](https://www.cambricon.com/)
   - MLU PyTorch 指南: [torch_mlu](https://developer.cambricon.com/index/pytorch/mlu-pytorch-intro.html)
3. **AMD ROCm**: [ROCm Documentation](https://rocm.docs.amd.com/)
   - RCCL API: [ROCm Communication Collectives Library](https://github.com/ROCmSoftwarePlatform/rccl)
4. **Baidu Kunlun**: [昆仑芯开发者中心](https://kunlunxin.com.cn/)

### 工具
- `npu-smi` - 昇腾设备管理工具
- `cnmon` - 寒武纪监控工具
- `rocm-smi` - 海光 DCU 工具
- `xpu-smi` - 昆仑芯设备管理工具

### 测试用例仓库
- SAGE 通信测试: `packages/sage-benchmark/tests/comm/domestic/`
- 各厂商官方 benchmark: 见上述文档链接

______________________________________________________________________

## 质量检查清单

### 功能验证
- [ ] All-reduce 结果正确性（与 NCCL 对比）
- [ ] 异步操作正确性
- [ ] 多种数据类型支持（FP32/FP16/BF16/INT8）
- [ ] 多种规约操作支持（SUM/MAX/MIN/PROD）

### 性能验证
- [ ] 带宽 ≥85% 厂商声称值
- [ ] 延迟 <50µs（节点内，1MB）
- [ ] 与 NCCL 性能差距 ≤10%
- [ ] 稳定性：无内存泄漏、无 crash（10000 次压测）

### 文档
- [ ] 厂商 SDK 版本要求矩阵
- [ ] 各厂商安装指南（驱动/固件/SDK）
- [ ] 已知限制和 workarounds
- [ ] 性能调优指南
