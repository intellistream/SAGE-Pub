# 小方向 1.1：拓扑感知与优化 (Topology Detection & Optimization)

> **Git Repo**: `sageLLM-topology` | **Priority**: P1 | **Phase**: Week 3-5

## 模块定位

### 核心职责
探测和管理硬件通信拓扑，为上层通信调度提供精确的拓扑信息（NVLink/PCIe/InfiniBand/国产互联），生成通信拓扑图和通信成本模型。

### 为什么需要独立模块
- **性能关键**：错误的拓扑假设会导致带宽浪费（如跨 PCIe bridge 的不必要通信）
- **硬件多样性**：CUDA GPU、国产加速器（昇腾/寒武纪/海光/昆仑）拓扑差异巨大
- **动态调整**：云环境下拓扑可能变化，需要实时探测和更新
- **独立研究价值**：拓扑感知算法本身是独立的研究课题（可发表）

### Baseline 参考
1. **Megatron-LM Topology Manager** ([NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM))
   - 拓扑探测：`megatron/core/tensor_parallel/topology.py`
   - 通信路径规划：支持 NVLink、PCIe、InfiniBand
2. **NCCL Topology Detection** ([NVIDIA/nccl](https://github.com/NVIDIA/nccl))
   - XML 拓扑文件解析（`/proc/`, `/sys/` 读取）
   - 自动带宽探测（micro-benchmark）
3. **DeepSpeed Topology Aware Routing** ([microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed))
   - `deepspeed/runtime/comm/topology.py`

______________________________________________________________________

## 技术规格

### 输入接口
```python
# core/protocols/topology.py
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum

class LinkType(Enum):
    NVLINK = "nvlink"        # 900GB/s (H100 NVLink 4.0)
    PCIE = "pcie"            # 64GB/s (PCIe 5.0 x16)
    INFINIBAND = "infiniband"  # 400Gb/s (NDR)
    HCCS = "hccs"            # 昇腾 HCCS (400GB/s)
    MLU_LINK = "mlu_link"    # 寒武纪 MLU-Link
    XGMI = "xgmi"            # 海光 xGMI
    UNKNOWN = "unknown"

@dataclass
class TopologyLink:
    """拓扑链路描述"""
    src_device: int
    dst_device: int
    link_type: LinkType
    bandwidth_gbps: float  # 理论带宽（GB/s）
    latency_us: float      # 延迟（微秒）
    is_bidirectional: bool = True

@dataclass
class TopologyInfo:
    """完整拓扑信息"""
    num_devices: int
    device_types: dict[int, str]  # {0: "cuda:0", 1: "cuda:1", ...}
    links: list[TopologyLink]
    adjacency_matrix: list[list[float]]  # [i][j] = bandwidth from i to j
    routing_table: dict[tuple[int, int], list[int]]  # (src, dst) -> [hop1, hop2, ...]

class TopologyManagerProtocol(ABC):
    """拓扑管理器协议（所有实现必须遵守）"""
    
    @abstractmethod
    def detect(self, force_redetect: bool = False) -> TopologyInfo:
        """探测硬件拓扑"""
        pass
    
    @abstractmethod
    def get_communication_cost(self, src: int, dst: int, size_bytes: int) -> float:
        """估算通信成本（单位：秒）"""
        pass
    
    @abstractmethod
    def find_optimal_path(self, src: int, dst: int) -> list[int]:
        """查找最优通信路径（节点序列）"""
        pass
    
    @abstractmethod
    def export_to_xml(self, path: str) -> None:
        """导出为 NCCL 兼容的 XML 拓扑文件"""
        pass
```

### 输出接口
```python
# 典型使用场景
from sage.llm.sagellm.comm_backend.topology import TopologyManager

manager = TopologyManager()
topo_info = manager.detect()  # 探测拓扑

# 获取设备间带宽
bw = topo_info.adjacency_matrix[0][1]  # GPU 0 -> GPU 1 的带宽

# 查询通信成本（发送 1GB 数据从 GPU 0 到 GPU 3）
cost_sec = manager.get_communication_cost(src=0, dst=3, size_bytes=1_000_000_000)

# 查找最优路径（可能经过中转节点）
path = manager.find_optimal_path(src=0, dst=7)  # [0, 2, 6, 7]（经过 GPU 2 和 6）

# 导出为 NCCL 可读格式
manager.export_to_xml("/tmp/topo.xml")
```

______________________________________________________________________

## 模块依赖关系

### 上游依赖（此模块需要）
- ✅ **core/protocols/topology.py** - 拓扑协议定义（TopologyManagerProtocol）
- ✅ **无其他依赖** - 完全独立的底层模块

### 下游使用者（依赖此模块的）
- **collective_ops/** - 根据拓扑选择最优通信算法
- **kv_transfer/** - 规划跨节点传输路径
- **overlap/** - 基于拓扑调度计算-通信重叠
- **engines/lmdeploy** - 初始化时探测硬件拓扑

### 独立性保证
```python
# 可以单独测试，无需其他模块
pytest comm/topology/tests/

# 可以用 mock 替代下游模块
from unittest.mock import Mock
topo_manager = TopologyManager()
topo_info = topo_manager.detect()  # 不依赖任何其他功能模块
```

______________________________________________________________________

## 实现方案

### 核心算法

#### 1. 拓扑探测流程
```python
# 伪代码
def detect_topology():
    # Step 1: 探测设备数量和类型
    devices = detect_devices()  # CUDA: nvidia-smi, 昇腾: npu-smi
    
    # Step 2: 探测点对点链路（NVLink/PCIe）
    links = []
    for src in devices:
        for dst in devices:
            if src == dst:
                continue
            link = probe_link(src, dst)  # micro-benchmark
            links.append(link)
    
    # Step 3: 构建邻接矩阵和路由表
    adj_matrix = build_adjacency_matrix(links)
    routing_table = floyd_warshall(adj_matrix)  # 全源最短路径
    
    return TopologyInfo(devices, links, adj_matrix, routing_table)
```

#### 2. Micro-Benchmark 带宽探测
```python
def probe_link(src: int, dst: int) -> TopologyLink:
    """通过实际通信测试探测带宽和延迟"""
    import torch
    
    # 发送小消息（latency 测试）
    small_tensor = torch.randn(1024, device=f"cuda:{src}")  # 4KB
    start = time.time()
    if src != dst:
        # 跨设备拷贝
        small_tensor.to(f"cuda:{dst}")
    torch.cuda.synchronize()
    latency = (time.time() - start) * 1e6  # 转换为微秒
    
    # 发送大消息（bandwidth 测试）
    large_tensor = torch.randn(25_000_000, device=f"cuda:{src}")  # 100MB
    start = time.time()
    if src != dst:
        large_tensor.to(f"cuda:{dst}")
    torch.cuda.synchronize()
    elapsed = time.time() - start
    bandwidth = (100 * 1024 * 1024) / elapsed / 1e9  # GB/s
    
    # 判断链路类型（基于带宽范围）
    link_type = infer_link_type(bandwidth)
    
    return TopologyLink(src, dst, link_type, bandwidth, latency)

def infer_link_type(bandwidth_gbps: float) -> LinkType:
    """根据测量带宽推断链路类型"""
    if bandwidth_gbps > 400:
        return LinkType.NVLINK  # H100 NVLink 4.0: ~900GB/s
    elif bandwidth_gbps > 200:
        return LinkType.HCCS  # 昇腾 HCCS: ~400GB/s
    elif bandwidth_gbps > 40:
        return LinkType.PCIE  # PCIe 5.0 x16: ~64GB/s
    elif bandwidth_gbps > 10:
        return LinkType.INFINIBAND  # NDR InfiniBand: ~50GB/s
    else:
        return LinkType.UNKNOWN
```

#### 3. 国产加速器适配
```python
class AscendTopologyDetector:
    """昇腾拓扑探测器（基于 HCCS）"""
    def detect(self):
        # 调用昇腾 SDK
        import acl
        device_count = acl.rt.get_device_count()
        # 读取 HCCS 拓扑信息
        topo_xml = acl.get_topology_xml()
        return parse_xml(topo_xml)

class CambriconTopologyDetector:
    """寒武纪拓扑探测器（基于 MLU-Link）"""
    def detect(self):
        import cnrt
        # 类似逻辑
```

### 数据结构

#### 拓扑图存储（邻接表）
```python
@dataclass
class TopologyGraph:
    """拓扑图数据结构"""
    nodes: dict[int, DeviceNode]  # 节点信息
    edges: dict[tuple[int, int], EdgeInfo]  # 边信息
    
    def get_neighbors(self, node_id: int) -> list[int]:
        """获取邻居节点"""
        return [dst for (src, dst) in self.edges if src == node_id]
    
    def shortest_path(self, src: int, dst: int) -> tuple[list[int], float]:
        """Dijkstra 最短路径（返回路径和总成本）"""
        ...
```

#### 通信成本模型
```python
def communication_cost_model(src: int, dst: int, size_bytes: int) -> float:
    """
    成本 = 延迟 + 数据大小 / 带宽
    
    考虑因素：
    1. 链路类型（NVLink vs PCIe）
    2. 多跳路径（中转节点）
    3. 拥塞（当前负载）
    """
    path = find_path(src, dst)
    total_cost = 0.0
    
    for i in range(len(path) - 1):
        hop_src, hop_dst = path[i], path[i+1]
        link = get_link(hop_src, hop_dst)
        
        # 延迟成本
        total_cost += link.latency_us / 1e6  # 转换为秒
        
        # 带宽成本
        total_cost += size_bytes / (link.bandwidth_gbps * 1e9)
    
    return total_cost
```

______________________________________________________________________

## 性能指标

### 核心指标
| 指标 | 目标值 | 测量方法 |
|------|--------|---------|
| **拓扑探测延迟** | <100ms | 首次探测时间（8 卡系统） |
| **探测准确率** | 100% | 与 nvidia-smi topo 对比 |
| **带宽预测误差** | <5% | micro-benchmark vs 实际通信 |
| **路径查询延迟** | <10µs | `find_optimal_path()` 响应时间 |
| **内存占用** | <1MB | 拓扑信息存储（8 卡） |

### Benchmark 场景
```python
# benchmark/topology_benchmark.py
def benchmark_topology_detection():
    """测试拓扑探测性能"""
    manager = TopologyManager()
    
    # 测试 1: 探测延迟
    start = time.time()
    topo_info = manager.detect(force_redetect=True)
    detect_time = time.time() - start
    assert detect_time < 0.1, f"Detection too slow: {detect_time}s"
    
    # 测试 2: 带宽预测准确性
    for src, dst in [(0, 1), (0, 7)]:
        predicted_bw = topo_info.adjacency_matrix[src][dst]
        actual_bw = measure_actual_bandwidth(src, dst)
        error = abs(predicted_bw - actual_bw) / actual_bw
        assert error < 0.05, f"Bandwidth error too large: {error*100:.1f}%"
    
    # 测试 3: 路径查询性能
    start = time.time()
    for _ in range(1000):
        path = manager.find_optimal_path(0, 7)
    query_time = (time.time() - start) / 1000
    assert query_time < 10e-6, f"Path query too slow: {query_time*1e6:.1f}µs"
```

______________________________________________________________________

## 开发计划

### Week 1: 基础框架（MVP）
- [ ] 定义 `TopologyManagerProtocol` 和数据类
- [ ] 实现 CUDA 拓扑探测（基于 `nvidia-smi topo -m`）
- [ ] 邻接矩阵构建 + Floyd-Warshall 最短路径
- [ ] 单元测试：8 卡 DGX 系统验证

### Week 2: 高级特性
- [ ] Micro-benchmark 带宽探测（替代静态配置）
- [ ] 支持 InfiniBand 跨节点拓扑
- [ ] 动态拓扑更新（热插拔设备）
- [ ] NCCL XML 格式导出

### Week 3: 国产加速器适配
- [ ] 昇腾 HCCS 拓扑探测
- [ ] 寒武纪 MLU-Link 适配
- [ ] 海光 xGMI 支持（如有硬件）

### Week 4: 集成与优化
- [ ] 与 `collective_ops/` 模块集成（提供拓扑信息）
- [ ] 性能优化：缓存探测结果、lazy evaluation
- [ ] 完整 benchmark suite

______________________________________________________________________

## 集成示例

### 与 collective_ops 模块协作
```python
# comm/collective_ops/nccl_backend.py
from sage.llm.sagellm.comm_backend.topology import TopologyManager

class NCCLBackend:
    def __init__(self):
        self.topo_manager = TopologyManager()
        self.topo_info = self.topo_manager.detect()
    
    def all_reduce(self, tensor, op="sum"):
        # 根据拓扑选择最优算法
        if self._is_nvlink_connected():
            return self._all_reduce_ring(tensor)  # Ring 算法（适合 NVLink）
        else:
            return self._all_reduce_tree(tensor)  # Tree 算法（适合 PCIe）
    
    def _is_nvlink_connected(self) -> bool:
        """检查所有设备是否通过 NVLink 全连接"""
        for link in self.topo_info.links:
            if link.link_type != LinkType.NVLINK:
                return False
        return True
```

### 与 kv_transfer 模块协作
```python
# comm/kv_transfer/transfer_scheduler.py
from sage.llm.sagellm.comm_backend.topology import TopologyManager

class KVTransferScheduler:
    def __init__(self):
        self.topo_manager = TopologyManager()
    
    def schedule_transfer(self, src_node: int, dst_node: int, size_bytes: int):
        # 查询最优路径
        path = self.topo_manager.find_optimal_path(src_node, dst_node)
        
        # 估算传输时间
        cost = self.topo_manager.get_communication_cost(src_node, dst_node, size_bytes)
        
        # 如果跨多跳，考虑分段传输
        if len(path) > 2:
            return self._staged_transfer(path, size_bytes)
        else:
            return self._direct_transfer(src_node, dst_node, size_bytes)
```

______________________________________________________________________

## 质量检查清单

### 代码质量
- [ ] 所有公共接口有完整 docstring（Google style）
- [ ] 类型注解覆盖率 100%（`mypy --strict` 通过）
- [ ] 单元测试覆盖率 ≥80%（`pytest --cov`）
- [ ] 集成测试：模拟 8 卡 DGX 拓扑

### 性能验证
- [ ] 探测延迟 <100ms（8 卡系统）
- [ ] 带宽预测误差 <5%（与 NCCL benchmark 对比）
- [ ] 路径查询 p99 <10µs（1000 次查询）

### 文档完整性
- [ ] README.md：模块介绍、快速开始、API 文档
- [ ] examples/：拓扑可视化脚本（生成 Graphviz DOT 文件）
- [ ] CHANGELOG.md：版本更新记录

______________________________________________________________________

## 参考资源

### 论文
1. **NCCL: Optimized Primitives for Collective Multi-GPU Communication** (NVIDIA, 2019)
2. **Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism** (NVIDIA, 2020)
3. **Topology-Aware Routing for Collective Communication** (Microsoft Research, 2021)

### 开源项目
1. [NVIDIA/Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - `megatron/core/tensor_parallel/topology.py`
2. [NVIDIA/nccl](https://github.com/NVIDIA/nccl) - XML 拓扑解析
3. [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed) - `deepspeed/runtime/comm/topology.py`
4. [horovod/horovod](https://github.com/horovod/horovod) - 拓扑感知通信

### 工具
- `nvidia-smi topo -m` - NVIDIA GPU 拓扑查询
- `ibv_devinfo` - InfiniBand 拓扑信息
- `npu-smi info -t topology` - 昇腾拓扑查询

______________________________________________________________________

## 常见问题

**Q1: 为什么不直接使用 NCCL 的拓扑探测？**  
A: NCCL 拓扑探测仅支持 NVIDIA GPU，且不暴露编程接口。我们需要统一的拓扑抽象层，支持国产加速器和自定义算法。

**Q2: Micro-benchmark 带宽探测会不会影响性能？**  
A: 仅在首次探测时执行（<100ms），结果会缓存。后续查询直接读取缓存（<10µs）。

**Q3: 如何处理动态拓扑（云环境下设备变化）？**  
A: 提供 `detect(force_redetect=True)` 接口强制重新探测。可通过定时器或事件触发（如检测到设备热插拔）。

**Q4: 是否支持异构拓扑（CUDA + 昇腾混合）？**  
A: 是的。`TopologyLink` 抽象层屏蔽了具体硬件，支持混合拓扑。但需要注意跨类型通信（可能需要 CPU 中转）。
