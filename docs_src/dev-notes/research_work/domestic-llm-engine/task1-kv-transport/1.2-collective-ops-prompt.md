# 小方向 1.2：集合通信优化 (Collective Communication Optimization)

> **Git Repo**: `sageLLM-collective-ops` | **Priority**: P1 | **Phase**: Week 3-5

## 模块定位

### 核心职责
优化分布式推理中的集合通信原语（all_reduce, all_gather, reduce_scatter），实现通信融合、重叠、自适应算法选择，支持 CUDA GPU 和国产加速器。

### 为什么需要独立模块
- **性能瓶颈**：集合通信占推理总延迟的 20-40%（模型并行场景）
- **算法多样性**：Ring, Tree, Recursive-Doubling 等算法适用于不同拓扑和数据大小
- **融合机会**：多个小通信可融合为一次大通信（减少 kernel launch overhead）
- **独立研究价值**：通信优化算法是独立课题，可发顶会（NSDI/EuroSys）

### Baseline 参考
1. **NCCL** ([NVIDIA/nccl](https://github.com/NVIDIA/nccl))
   - All_reduce Ring/Tree 算法
   - 拓扑感知路由
2. **Gloo** ([facebookincubator/gloo](https://github.com/facebookincubator/gloo))
   - CPU 友好的集合通信
   - 支持 TCP/InfiniBand
3. **DeepSpeed ZeRO Communication** ([microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed))
   - 通信融合（fused all_reduce）
   - Gradient bucketing

______________________________________________________________________

## 技术规格

### 输入接口
```python
# core/protocols/comm_backend.py
from abc import ABC, abstractmethod
from enum import Enum
import torch

class CommOp(Enum):
    ALL_REDUCE = "all_reduce"
    ALL_GATHER = "all_gather"
    REDUCE_SCATTER = "reduce_scatter"
    BROADCAST = "broadcast"
    SEND = "send"
    RECV = "recv"

class CommBackendProtocol(ABC):
    """集合通信后端协议"""
    
    @abstractmethod
    def all_reduce(
        self, 
        tensor: torch.Tensor, 
        op: str = "sum",  # sum, max, min, prod
        async_op: bool = False
    ) -> torch.Tensor | torch.futures.Future:
        """All-reduce 操作"""
        pass
    
    @abstractmethod
    def all_gather(
        self,
        tensor: torch.Tensor,
        output: torch.Tensor | None = None,
        async_op: bool = False
    ) -> torch.Tensor | torch.futures.Future:
        """All-gather 操作"""
        pass
    
    @abstractmethod
    def reduce_scatter(
        self,
        tensor: torch.Tensor,
        op: str = "sum",
        async_op: bool = False
    ) -> torch.Tensor | torch.futures.Future:
        """Reduce-scatter 操作"""
        pass
    
    @abstractmethod
    def fused_all_reduce(
        self,
        tensors: list[torch.Tensor],
        op: str = "sum"
    ) -> list[torch.Tensor]:
        """融合多个 all_reduce 操作"""
        pass
    
    @abstractmethod
    def get_stats(self) -> dict:
        """获取通信统计信息"""
        pass
```

### 输出接口
```python
# 典型使用场景
from sage.llm.engines.sagellm.comm_backend.collective_ops import create_comm_backend

# 创建后端（自动选择：NCCL for GPU, Gloo for CPU）
backend = create_comm_backend(backend_type="auto")

# 1. 同步 all_reduce
tensor = torch.randn(1024, 1024, device="cuda:0")
result = backend.all_reduce(tensor, op="sum")

# 2. 异步 all_reduce（与计算重叠）
future = backend.all_reduce(tensor, op="sum", async_op=True)
# ... do some compute ...
result = future.wait()

# 3. 融合多个小通信（减少 kernel launch overhead）
tensors = [torch.randn(512, device="cuda:0") for _ in range(10)]
results = backend.fused_all_reduce(tensors, op="sum")

# 4. 获取通信统计
stats = backend.get_stats()
# {'all_reduce_count': 100, 'total_bytes': 1048576000, 'avg_latency_ms': 0.5}
```

______________________________________________________________________

## 模块依赖关系

### 上游依赖（此模块需要）
- ✅ **core/protocols/comm_backend.py** - 通信后端协议定义
- ✅ **topology/** - 拓扑信息（可选，用于算法选择优化）
- ✅ **torch.distributed** - PyTorch 分布式通信原语

### 下游使用者（依赖此模块的）
- **overlap/** - 调度异步通信与计算重叠
- **kv_transfer/** - 使用底层通信原语传输 KV Cache
- **engines/lmdeploy** - Tensor Parallel 层使用 all_reduce

### 独立性保证
```python
# 可以不依赖 topology 模块工作（使用默认算法）
backend = create_comm_backend("nccl")
result = backend.all_reduce(tensor)  # 使用 NCCL 默认算法

# 也可以集成 topology 做优化
from sage.llm.engines.sagellm.comm_backend.topology import TopologyManager
backend = create_comm_backend("nccl", topology_manager=TopologyManager())
result = backend.all_reduce(tensor)  # 使用拓扑感知算法
```

______________________________________________________________________

## 实现方案

### 核心算法

#### 1. All-Reduce 算法选择策略
```python
def select_all_reduce_algorithm(
    data_size: int,
    num_devices: int,
    topology: TopologyInfo
) -> str:
    """
    根据数据大小、设备数量、拓扑选择最优算法
    
    规则：
    - 小消息 (<1MB) + 全 NVLink: Ring 算法（带宽利用率高）
    - 大消息 (>10MB) + 层次拓扑: Tree 算法（延迟低）
    - 2 的幂次设备数: Recursive-Doubling（log 步骤）
    """
    if data_size < 1_000_000:
        # 小消息：延迟敏感
        if is_power_of_2(num_devices):
            return "recursive_doubling"  # O(log N) 步骤
        else:
            return "ring"  # O(N) 但延迟稳定
    else:
        # 大消息：带宽敏感
        if topology.is_fully_connected_nvlink():
            return "ring"  # 最大化带宽利用
        else:
            return "tree"  # 减少跨 PCIe 通信
```

#### 2. 通信融合（Fused Communication）
```python
class CommFusionBuffer:
    """通信融合缓冲区"""
    def __init__(self, max_size_mb: int = 100):
        self.buffer = torch.empty(max_size_mb * 1024 * 1024, dtype=torch.uint8)
        self.offset = 0
        self.tensors = []
    
    def add_tensor(self, tensor: torch.Tensor) -> bool:
        """添加 tensor 到融合缓冲区"""
        tensor_bytes = tensor.numel() * tensor.element_size()
        if self.offset + tensor_bytes > self.buffer.numel():
            return False  # 缓冲区已满
        
        # 拷贝到融合缓冲区
        view = self.buffer[self.offset:self.offset+tensor_bytes].view_as(tensor)
        view.copy_(tensor)
        self.tensors.append((tensor, self.offset, tensor_bytes))
        self.offset += tensor_bytes
        return True
    
    def flush_and_all_reduce(self, backend):
        """一次性通信所有融合 tensor"""
        # 单次 all_reduce 融合缓冲区
        fused_buffer = self.buffer[:self.offset]
        backend.all_reduce(fused_buffer, op="sum")
        
        # 回写结果到原 tensor
        for tensor, offset, size in self.tensors:
            view = fused_buffer[offset:offset+size].view_as(tensor)
            tensor.copy_(view)
        
        self.reset()
```

#### 3. NCCL 后端实现
```python
import torch.distributed as dist

class NCCLBackend(CommBackendProtocol):
    """NCCL 后端（GPU 专用）"""
    
    def __init__(self, rank: int, world_size: int):
        self.rank = rank
        self.world_size = world_size
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
        self.stats = {"all_reduce_count": 0, "total_bytes": 0}
    
    def all_reduce(self, tensor, op="sum", async_op=False):
        op_map = {"sum": dist.ReduceOp.SUM, "max": dist.ReduceOp.MAX}
        
        # 记录统计
        self.stats["all_reduce_count"] += 1
        self.stats["total_bytes"] += tensor.numel() * tensor.element_size()
        
        # 执行通信
        handle = dist.all_reduce(tensor, op=op_map[op], async_op=async_op)
        
        if async_op:
            return handle  # 返回 Future
        else:
            return tensor
    
    def fused_all_reduce(self, tensors, op="sum"):
        """使用融合缓冲区批量通信"""
        fusion_buffer = CommFusionBuffer()
        for tensor in tensors:
            fusion_buffer.add_tensor(tensor)
        fusion_buffer.flush_and_all_reduce(self)
        return tensors
```

#### 4. Gloo 后端实现（CPU/跨节点）
```python
class GlooBackend(CommBackendProtocol):
    """Gloo 后端（CPU 或跨节点通信）"""
    
    def __init__(self, rank: int, world_size: int):
        dist.init_process_group("gloo", rank=rank, world_size=world_size)
    
    def all_reduce(self, tensor, op="sum", async_op=False):
        # Gloo 支持 CPU tensor
        if tensor.is_cuda:
            tensor = tensor.cpu()  # 拷贝到 CPU
        
        op_map = {"sum": dist.ReduceOp.SUM}
        dist.all_reduce(tensor, op=op_map[op], async_op=async_op)
        
        if tensor.device.type == "cpu":
            tensor = tensor.cuda()  # 拷贝回 GPU
        
        return tensor
```

______________________________________________________________________

## 性能指标

### 核心指标
| 指标 | 目标值 | 测量方法 |
|------|--------|---------|
| **All-reduce 延迟** | <20µs（节点内，1MB） | PyTorch benchmark |
| **带宽利用率** | ≥85% | 实际带宽 / 理论带宽 |
| **融合加速比** | ≥2x（10 个小通信） | 融合 vs 非融合 |
| **Overhead** | <5% | 通信时间 / 总时间 |

### Benchmark 场景
```python
# benchmark/comm_benchmark.py
def benchmark_all_reduce():
    """测试 all_reduce 性能"""
    backend = create_comm_backend("nccl")
    
    # 测试不同数据大小
    for size_mb in [1, 10, 100]:
        tensor = torch.randn(size_mb * 1024 * 1024 // 4, device="cuda:0")
        
        # 预热
        for _ in range(10):
            backend.all_reduce(tensor)
        
        # 测量
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        start.record()
        for _ in range(100):
            backend.all_reduce(tensor)
        end.record()
        torch.cuda.synchronize()
        
        avg_time_ms = start.elapsed_time(end) / 100
        bandwidth_gbps = (size_mb * 1024) / (avg_time_ms / 1000)
        
        print(f"Size: {size_mb}MB, Latency: {avg_time_ms:.2f}ms, BW: {bandwidth_gbps:.1f}GB/s")
```

______________________________________________________________________

## 开发计划

### Week 1: NCCL/Gloo 封装
- [ ] `NCCLBackend` 实现（all_reduce, all_gather, reduce_scatter）
- [ ] `GlooBackend` 实现（CPU fallback）
- [ ] 自动后端选择（`create_comm_backend("auto")`）
- [ ] 单元测试：4 卡 all_reduce 验证

### Week 2: 通信融合
- [ ] `CommFusionBuffer` 实现
- [ ] `fused_all_reduce()` 接口
- [ ] 自动融合调度器（动态决定何时 flush）
- [ ] Benchmark：融合 vs 非融合加速比

### Week 3: 算法优化
- [ ] Ring/Tree/Recursive-Doubling 算法实现
- [ ] 自适应算法选择（基于拓扑和数据大小）
- [ ] 与 `topology/` 模块集成

### Week 4: 国产加速器适配
- [ ] 昇腾 HCCL 后端（Huawei Collective Communication Library）
- [ ] 寒武纪 CNCL 后端
- [ ] 性能对齐（达到 NCCL 80% 性能）

______________________________________________________________________

## 集成示例

### 在 Tensor Parallel 中使用
```python
# engines/lmdeploy/tensor_parallel.py
from sage.llm.engines.sagellm.comm_backend.collective_ops import create_comm_backend

class TensorParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, world_size):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(out_features // world_size, in_features))
        self.comm_backend = create_comm_backend("nccl")
    
    def forward(self, x):
        # 本地计算
        local_output = F.linear(x, self.weight)
        
        # All-reduce 聚合结果
        global_output = self.comm_backend.all_reduce(local_output, op="sum")
        
        return global_output
```

### 与 overlap 模块协作
```python
# comm/overlap/scheduler.py
from sage.llm.engines.sagellm.comm_backend.collective_ops import create_comm_backend

class OverlapScheduler:
    def __init__(self):
        self.comm_backend = create_comm_backend("nccl")
    
    def schedule_layer(self, layer, input_tensor):
        # 计算当前层
        output = layer(input_tensor)
        
        # 异步启动通信（与下一层计算重叠）
        comm_handle = self.comm_backend.all_reduce(output, async_op=True)
        
        return output, comm_handle
```

______________________________________________________________________

## 参考资源

### 论文
1. **NCCL: Optimized Primitives for Collective Multi-GPU Communication** (NVIDIA, 2019)
2. **Gloo: High-Performance Generic Collective Communication Library** (Facebook, 2017)
3. **ZeRO: Memory Optimizations Toward Training Trillion Parameter Models** (Microsoft, 2020)

### 开源项目
1. [NVIDIA/nccl](https://github.com/NVIDIA/nccl)
2. [facebookincubator/gloo](https://github.com/facebookincubator/gloo)
3. [microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed) - `deepspeed/runtime/comm/`

______________________________________________________________________

## 质量检查清单

### 代码质量
- [ ] 类型注解覆盖率 100%
- [ ] 单元测试覆盖率 ≥80%
- [ ] 集成测试：4/8 卡 all_reduce 验证

### 性能验证
- [ ] All-reduce 延迟 <20µs（1MB, 节点内）
- [ ] 带宽利用率 ≥85%
- [ ] 融合加速比 ≥2x

### 文档完整性
- [ ] API 文档（docstring）
- [ ] 算法选择策略说明
- [ ] 性能调优指南
