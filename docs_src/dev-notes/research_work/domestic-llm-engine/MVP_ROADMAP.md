# sageLLM MVP è·¯çº¿å›¾ï¼šCUDA ä¼˜å…ˆé—­ç¯éªŒè¯

> **æ–‡æ¡£çŠ¶æ€**: MVP è§„åˆ’  
> **æ›´æ–°æ—¶é—´**: 2026-01-02  
> **ç›®æ ‡**: æ— å›½äº§ç¡¬ä»¶æ¡ä»¶ä¸‹å®Œæˆ sageLLM åŸºç¡€é—­ç¯  
> **ç­–ç•¥**: CUDA å®ç°ä¼˜å…ˆï¼Œé¢„ç•™å›½äº§ç¡¬ä»¶é€‚é…æ¥å£

---

## ğŸ¯ æ ¸å¿ƒç†å¿µ

**åœ¨æ²¡æœ‰å›½äº§ç¡¬ä»¶çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆç”¨ NVIDIA GPU å®Œæˆ sageLLM çš„æ ¸å¿ƒæ¶æ„å’Œç®—æ³•éªŒè¯ï¼Œä¸ºåç»­å›½äº§ç¡¬ä»¶é€‚é…æ‰“ä¸‹åŸºç¡€ã€‚**

### å…³é”®åŸåˆ™

1. **æ¶æ„ä¼˜å…ˆï¼Œç¡¬ä»¶å…¶æ¬¡**: å…ˆéªŒè¯æ¨¡å—åŒ–è®¾è®¡ã€Protocol å®šä¹‰ã€è°ƒåº¦ç­–ç•¥
2. **æŠ½è±¡å±‚è®¾è®¡**: é€šä¿¡/Kernel å±‚ä½¿ç”¨æŠ½è±¡æ¥å£ï¼Œä¾¿äºåç»­æ›¿æ¢
3. **CUDA as Reference**: CUDA/NCCL ä½œä¸ºå‚è€ƒå®ç°ï¼Œå›½äº§ç¡¬ä»¶å¯¹æ ‡
4. **å¢é‡æ¼”è¿›**: å…ˆ CUDA é—­ç¯ï¼Œå†é€æ­¥æ·»åŠ å›½äº§ç¡¬ä»¶æ”¯æŒ

---

## ğŸ“‹ MVP èŒƒå›´å®šä¹‰

### Phase 0: æœ€å°é—­ç¯ï¼ˆ2 å‘¨ï¼‰

**ç›®æ ‡**: è¯æ˜ sageLLM æ¶æ„å¯è¡Œï¼Œèƒ½åœ¨ NVIDIA GPU ä¸Šè¿è¡Œ

#### å¿…åšåŠŸèƒ½ï¼ˆP0ï¼‰

| æ¨¡å— | åŠŸèƒ½èŒƒå›´ | CUDA å®ç° | å›½äº§ç¡¬ä»¶ |
|------|---------|----------|---------|
| **core/** | Protocol/types å®šä¹‰ | âœ… å®Œæ•´ | â¸ï¸ é¢„ç•™æ¥å£ |
| **scheduler_ir/** | Prefill/Decode åˆ†ç¦» IR | âœ… å®Œæ•´ | â¸ï¸ é¢„ç•™æ¥å£ |
| **kv_runtime/** | KV block æ± åŒ–ç®¡ç† | âœ… CUDA å†…å­˜ç®¡ç† | â¸ï¸ é¢„ç•™ NPU æ¥å£ |
| **comm_backend/** | é€šä¿¡æŠ½è±¡å±‚ | âœ… NCCL å®ç° | â¸ï¸ é¢„ç•™ HCCL/CNCL æ¥å£ |
| **engines/** | å¼•æ“æ³¨å†Œåˆ° Control Plane | âœ… å®Œæ•´ | N/A |

#### æš‚ç¼“åŠŸèƒ½ï¼ˆP1-P2ï¼‰

- âŒ prefix_reuseï¼ˆå‰ç¼€å¤ç”¨ï¼‰- å¯åç»­æ·»åŠ 
- âŒ kv_policyï¼ˆæ·˜æ±°ç­–ç•¥ï¼‰- å…ˆç”¨ç®€å• LRU
- âŒ accel/quantizationï¼ˆé‡åŒ–ï¼‰- å…ˆç”¨ FP16/BF16
- âŒ accel/sparsityï¼ˆç¨€ç–åŒ–ï¼‰- å¯é€‰ä¼˜åŒ–
- âŒ accel/speculativeï¼ˆæŠ•æœºè§£ç ï¼‰- å¯é€‰ä¼˜åŒ–
- âŒ comm_backend/domesticï¼ˆå›½äº§äº’è”ï¼‰- ç­‰ç¡¬ä»¶åˆ°ä½

---

## ğŸ—ï¸ MVP æ¶æ„è®¾è®¡

### ç›®å½•ç»“æ„ï¼ˆæœ€å°åŒ–ï¼‰

```
packages/sage-llm-core/src/sage/llm/engines/sagellm/
â”œâ”€â”€ __init__.py                  # å¼•æ“å…¥å£
â”œâ”€â”€ engine.py                    # SageLLMEngine ä¸»ç±»ï¼ˆå®ç° BaseInferenceEngineï¼‰
â”œâ”€â”€ core/                        # æ ¸å¿ƒåè®®å±‚ï¼ˆç¡¬ä»¶æ— å…³ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ protocols.py             # Protocol å®šä¹‰
â”‚   â”œâ”€â”€ types.py                 # æ•°æ®ç±»å‹ï¼ˆKVBlock, RequestMetadata, etc.ï¼‰
â”‚   â””â”€â”€ config.py                # é…ç½®ç®¡ç†
â”œâ”€â”€ scheduler_ir/                # è°ƒåº¦ IRï¼ˆç¡¬ä»¶æ— å…³ï¼‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ir_builder.py            # IR æ„å»ºå™¨
â”‚   â”œâ”€â”€ ir_executor.py           # IR æ‰§è¡Œå™¨
â”‚   â””â”€â”€ pd_separation.py         # Prefill/Decode åˆ†ç¦»é€»è¾‘
â”œâ”€â”€ kv_runtime/                  # KV Cache è¿è¡Œæ—¶
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ pool_manager.py          # KV block æ± åŒ–ç®¡ç†
â”‚   â”œâ”€â”€ backends/                # åç«¯æŠ½è±¡
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py              # KVBackendProtocol
â”‚   â”‚   â”œâ”€â”€ cuda_backend.py      # âœ… CUDA å®ç°ï¼ˆMVPï¼‰
â”‚   â”‚   â””â”€â”€ npu_backend.py       # â¸ï¸ å›½äº§ NPU æ¥å£ï¼ˆé¢„ç•™ï¼‰
â”‚   â””â”€â”€ memory_allocator.py      # å†…å­˜åˆ†é…å™¨ï¼ˆCUDA firstï¼‰
â”œâ”€â”€ comm_backend/                # é€šä¿¡å±‚
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                  # CommBackendProtocol
â”‚   â”œâ”€â”€ nccl_backend.py          # âœ… NCCL å®ç°ï¼ˆMVPï¼‰
â”‚   â””â”€â”€ domestic/                # â¸ï¸ å›½äº§äº’è”ï¼ˆé¢„ç•™ç›®å½•ï¼‰
â”‚       â””â”€â”€ __init__.py
â””â”€â”€ benchmarks/                  # æ€§èƒ½æµ‹è¯•
    â”œâ”€â”€ __init__.py
    â””â”€â”€ bench_mvp.py             # MVP åŸºå‡†æµ‹è¯•
```

---

## ğŸ“ æ ¸å¿ƒè®¾è®¡ï¼šç¡¬ä»¶æŠ½è±¡å±‚

### 1. KV Runtime Backend Protocol

```python
# packages/sage-llm-core/src/sage/llm/engines/sagellm/kv_runtime/backends/base.py

from abc import ABC, abstractmethod
from typing import Any

class KVBackendProtocol(ABC):
    """KV Cache åç«¯æŠ½è±¡åè®®ï¼ˆç¡¬ä»¶æ— å…³ï¼‰"""
    
    @abstractmethod
    def allocate_blocks(self, num_blocks: int, block_size: int) -> list[int]:
        """åˆ†é… KV blocksï¼ˆè¿”å› block IDsï¼‰"""
        pass
    
    @abstractmethod
    def free_blocks(self, block_ids: list[int]) -> None:
        """é‡Šæ”¾ KV blocks"""
        pass
    
    @abstractmethod
    def copy_blocks(self, src_ids: list[int], dst_ids: list[int]) -> None:
        """è·¨è®¾å¤‡/èŠ‚ç‚¹æ‹·è´ blocks"""
        pass
    
    @abstractmethod
    def get_memory_usage(self) -> dict[str, Any]:
        """è·å–æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
        pass
```

### 2. CUDA Backend å®ç°ï¼ˆMVPï¼‰

```python
# packages/sage-llm-core/src/sage/llm/engines/sagellm/kv_runtime/backends/cuda_backend.py

import torch
from .base import KVBackendProtocol

class CUDAKVBackend(KVBackendProtocol):
    """CUDA ç‰ˆæœ¬çš„ KV Cache åç«¯ï¼ˆMVP å®ç°ï¼‰"""
    
    def __init__(self, device: str = "cuda:0", block_size: int = 16):
        self.device = torch.device(device)
        self.block_size = block_size
        self.blocks = {}  # block_id -> Tensor
        self.free_list = []
        self.next_id = 0
    
    def allocate_blocks(self, num_blocks: int, block_size: int) -> list[int]:
        block_ids = []
        for _ in range(num_blocks):
            if self.free_list:
                block_id = self.free_list.pop()
            else:
                block_id = self.next_id
                self.next_id += 1
                # åˆ†é… CUDA å†…å­˜
                self.blocks[block_id] = torch.empty(
                    (block_size, 128, 128),  # (seq_len, num_heads, head_dim)
                    device=self.device,
                    dtype=torch.float16
                )
            block_ids.append(block_id)
        return block_ids
    
    def free_blocks(self, block_ids: list[int]) -> None:
        self.free_list.extend(block_ids)
    
    def copy_blocks(self, src_ids: list[int], dst_ids: list[int]) -> None:
        for src, dst in zip(src_ids, dst_ids):
            self.blocks[dst].copy_(self.blocks[src])
    
    def get_memory_usage(self) -> dict[str, Any]:
        return {
            "allocated_blocks": len(self.blocks),
            "free_blocks": len(self.free_list),
            "memory_mb": torch.cuda.memory_allocated(self.device) / 1024 / 1024
        }
```

### 3. å›½äº§ NPU Backend æ¥å£ï¼ˆé¢„ç•™ï¼‰

```python
# packages/sage-llm-core/src/sage/llm/engines/sagellm/kv_runtime/backends/npu_backend.py

from .base import KVBackendProtocol

class AscendKVBackend(KVBackendProtocol):
    """æ˜‡è…¾ NPU ç‰ˆæœ¬çš„ KV Cache åç«¯ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
    
    def __init__(self, device: str = "npu:0", block_size: int = 16):
        # TODO: ç­‰ç¡¬ä»¶åˆ°ä½åå®ç°
        raise NotImplementedError("Ascend NPU backend not implemented yet")
    
    def allocate_blocks(self, num_blocks: int, block_size: int) -> list[int]:
        # TODO: ä½¿ç”¨ torch_npu å®ç°
        pass
    
    # ... å…¶ä»–æ–¹æ³•ç±»ä¼¼


class MLUKVBackend(KVBackendProtocol):
    """å¯’æ­¦çºª MLU ç‰ˆæœ¬çš„ KV Cache åç«¯ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
    
    def __init__(self, device: str = "mlu:0", block_size: int = 16):
        # TODO: ç­‰ç¡¬ä»¶åˆ°ä½åå®ç°
        raise NotImplementedError("MLU backend not implemented yet")
```

---

## ğŸ”§ Comm Backend æŠ½è±¡è®¾è®¡

### 1. Comm Backend Protocol

```python
# packages/sage-llm-core/src/sage/llm/engines/sagellm/comm_backend/base.py

from abc import ABC, abstractmethod
import torch

class CommBackendProtocol(ABC):
    """é€šä¿¡åç«¯æŠ½è±¡åè®®"""
    
    @abstractmethod
    def all_reduce(self, tensor: torch.Tensor, op: str = "sum") -> torch.Tensor:
        """All-reduce æ“ä½œ"""
        pass
    
    @abstractmethod
    def all_gather(self, tensor: torch.Tensor) -> list[torch.Tensor]:
        """All-gather æ“ä½œ"""
        pass
    
    @abstractmethod
    def send_kv_blocks(self, blocks: list[torch.Tensor], dst_rank: int) -> None:
        """å‘é€ KV blocks åˆ°ç›®æ ‡èŠ‚ç‚¹"""
        pass
    
    @abstractmethod
    def recv_kv_blocks(self, src_rank: int) -> list[torch.Tensor]:
        """ä»æºèŠ‚ç‚¹æ¥æ”¶ KV blocks"""
        pass
    
    @abstractmethod
    def get_bandwidth_stats(self) -> dict[str, float]:
        """è·å–é€šä¿¡å¸¦å®½ç»Ÿè®¡"""
        pass
```

### 2. NCCL Backend å®ç°ï¼ˆMVPï¼‰

```python
# packages/sage-llm-core/src/sage/llm/engines/sagellm/comm_backend/nccl_backend.py

import torch
import torch.distributed as dist
from .base import CommBackendProtocol

class NCCLCommBackend(CommBackendProtocol):
    """NCCL ç‰ˆæœ¬çš„é€šä¿¡åç«¯ï¼ˆMVP å®ç°ï¼‰"""
    
    def __init__(self, rank: int, world_size: int):
        self.rank = rank
        self.world_size = world_size
        # åˆå§‹åŒ– NCCL
        if not dist.is_initialized():
            dist.init_process_group(backend="nccl")
    
    def all_reduce(self, tensor: torch.Tensor, op: str = "sum") -> torch.Tensor:
        op_map = {"sum": dist.ReduceOp.SUM, "max": dist.ReduceOp.MAX}
        dist.all_reduce(tensor, op=op_map[op])
        return tensor
    
    def all_gather(self, tensor: torch.Tensor) -> list[torch.Tensor]:
        tensor_list = [torch.empty_like(tensor) for _ in range(self.world_size)]
        dist.all_gather(tensor_list, tensor)
        return tensor_list
    
    def send_kv_blocks(self, blocks: list[torch.Tensor], dst_rank: int) -> None:
        for block in blocks:
            dist.send(block, dst=dst_rank)
    
    def recv_kv_blocks(self, src_rank: int) -> list[torch.Tensor]:
        # ç®€åŒ–ç‰ˆæœ¬ï¼šé¢„å…ˆçŸ¥é“ block æ•°é‡
        blocks = []
        # TODO: å®ç°åŠ¨æ€æ¥æ”¶é€»è¾‘
        return blocks
    
    def get_bandwidth_stats(self) -> dict[str, float]:
        # TODO: å®ç°å¸¦å®½ç»Ÿè®¡
        return {"avg_bandwidth_gbps": 0.0}
```

### 3. å›½äº§äº’è” Backendï¼ˆé¢„ç•™ï¼‰

```python
# packages/sage-llm-core/src/sage/llm/engines/sagellm/comm_backend/domestic/hccl_backend.py

from ..base import CommBackendProtocol

class HCCLCommBackend(CommBackendProtocol):
    """æ˜‡è…¾ HCCL ç‰ˆæœ¬çš„é€šä¿¡åç«¯ï¼ˆé¢„ç•™æ¥å£ï¼‰"""
    
    def __init__(self, rank: int, world_size: int):
        # TODO: ç­‰ç¡¬ä»¶åˆ°ä½åå®ç°
        raise NotImplementedError("HCCL backend not implemented yet")
    
    # ... å…¶ä»–æ–¹æ³•
```

---

## ğŸš€ MVP å®ç°æ­¥éª¤

### Week 1: åŸºç¡€æ¶æ„æ­å»º

**Day 1-2: ç›®å½•ä¸ Protocol**
```bash
# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p packages/sage-llm-core/src/sage/llm/engines/sagellm/{core,scheduler_ir,kv_runtime,comm_backend}

# å®ç°æ ¸å¿ƒ Protocol
- core/protocols.py: KVBackendProtocol, CommBackendProtocol
- core/types.py: KVBlock, RequestMetadata, SchedulingDecision
- core/config.py: SageLLMConfig
```

**Day 3-4: KV Runtimeï¼ˆCUDAï¼‰**
```python
# å®ç° CUDA KV Backend
- kv_runtime/backends/cuda_backend.py: CUDAKVBackend
- kv_runtime/pool_manager.py: KVPoolManagerï¼ˆä½¿ç”¨ CUDAKVBackendï¼‰
- å•å…ƒæµ‹è¯•: tests/test_kv_runtime_cuda.py
```

**Day 5-7: Scheduler IR**
```python
# å®ç°è°ƒåº¦ IR
- scheduler_ir/ir_builder.py: IRBuilder
- scheduler_ir/ir_executor.py: IRExecutor
- scheduler_ir/pd_separation.py: PrefillDecodeScheduler
- å•å…ƒæµ‹è¯•: tests/test_scheduler_ir.py
```

---

### Week 2: å¼•æ“é›†æˆä¸éªŒè¯

**Day 8-10: Comm Backendï¼ˆNCCLï¼‰**
```python
# å®ç° NCCL é€šä¿¡åç«¯
- comm_backend/nccl_backend.py: NCCLCommBackend
- é›†æˆæµ‹è¯•: tests/test_comm_nccl.pyï¼ˆ2 å¡æµ‹è¯•ï¼‰
```

**Day 11-12: SageLLMEngine ä¸»ç±»**
```python
# å®ç°å¼•æ“ä¸»ç±»
- engine.py: SageLLMEngine (ç»§æ‰¿ BaseInferenceEngine)
  - setup(): åˆå§‹åŒ– KV runtime + Comm backend
  - generate(): æ¨ç†å…¥å£
  - get_capability(): æŠ¥å‘Šå¼•æ“èƒ½åŠ›
- æ³¨å†Œåˆ° Control Plane
```

**Day 13-14: ç«¯åˆ°ç«¯æµ‹è¯•**
```bash
# æœ€å°é—­ç¯éªŒè¯
sage llm engine start Qwen/Qwen2.5-1.5B-Instruct \
  --engine-kind sagellm \
  --device cuda:0

# Python æµ‹è¯•
python -m pytest packages/sage-llm-core/tests/test_sagellm_mvp.py -v

# æ€§èƒ½åŸºçº¿æµ‹è¯•
python packages/sage-llm-core/src/sage/llm/engines/sagellm/benchmarks/bench_mvp.py
```

---

## ğŸ“Š MVP éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½éªŒæ”¶

| æµ‹è¯•é¡¹ | éªŒæ”¶æ ‡å‡† | æµ‹è¯•æ–¹æ³• |
|--------|---------|---------|
| **å¼•æ“æ³¨å†Œ** | sageLLM èƒ½æ³¨å†Œåˆ° Control Plane | `sage llm engine list` æ˜¾ç¤º sagellm |
| **å•å¡æ¨ç†** | èƒ½å®Œæˆ chat/generate è¯·æ±‚ | `UnifiedInferenceClient` è°ƒç”¨æˆåŠŸ |
| **KV ç®¡ç†** | KV block åˆ†é…/é‡Šæ”¾æ­£å¸¸ | å†…å­˜æ³„æ¼æµ‹è¯•ï¼ˆ100 æ¬¡è¯·æ±‚ï¼‰ |
| **Prefill/Decode åˆ†ç¦»** | IR æ„å»ºå’Œæ‰§è¡ŒæˆåŠŸ | æ—¥å¿—æ˜¾ç¤º PD åˆ†ç¦» |
| **å¤šå¡é€šä¿¡** | NCCL all_reduce æ­£å¸¸ | 2 å¡ tensor parallel æµ‹è¯• |

### æ€§èƒ½éªŒæ”¶ï¼ˆvs vLLM Baselineï¼‰

| æŒ‡æ ‡ | ç›®æ ‡ | æµ‹è¯•åœºæ™¯ |
|------|------|---------|
| **TTFT** | â‰¥ vLLM Ã— 0.95 | Qwen2.5-1.5B, prompt=512 tokens |
| **TPOT** | â‰¥ vLLM Ã— 0.95 | output=256 tokens |
| **ååé‡** | â‰¥ vLLM Ã— 0.90 | batch_size=8, concurrent=16 |
| **å†…å­˜æ•ˆç‡** | KV cache ç¢ç‰‡ç‡ <10% | è¿è¡Œ 100 æ¬¡è¯·æ±‚ |

**å…è®¸çš„æ€§èƒ½æŸå¤±**: MVP é˜¶æ®µå…è®¸ 5-10% çš„æ€§èƒ½æŸå¤±ï¼ˆæœªä¼˜åŒ–ï¼‰

---

## ğŸ”„ å›½äº§ç¡¬ä»¶é€‚é…è·¯å¾„

### ç¡¬ä»¶åˆ°ä½åçš„è¿ç§»æ­¥éª¤

#### Step 1: æ˜‡è…¾ 910B é€‚é…ï¼ˆWeek 3-4ï¼‰

```python
# 1. å®ç° Ascend KV Backend
class AscendKVBackend(KVBackendProtocol):
    def __init__(self, device: str = "npu:0"):
        import torch_npu  # æ˜‡è…¾ PyTorch é€‚é…
        self.device = torch.device(device)
    
    def allocate_blocks(self, num_blocks: int, block_size: int):
        # ä½¿ç”¨ torch_npu åˆ†é… NPU å†…å­˜
        return torch_npu.npu_alloc(...)

# 2. å®ç° HCCL Comm Backend
class HCCLCommBackend(CommBackendProtocol):
    def __init__(self):
        import hccl_binding  # æ˜‡è…¾ HCCL ç»‘å®š
        self.hccl_comm = hccl_binding.init()
    
    def all_reduce(self, tensor):
        return self.hccl_comm.all_reduce(tensor)

# 3. é…ç½®åˆ‡æ¢
sagellm_config = {
    "kv_backend": "ascend",  # ä» "cuda" åˆ‡æ¢åˆ° "ascend"
    "comm_backend": "hccl",  # ä» "nccl" åˆ‡æ¢åˆ° "hccl"
}
```

#### Step 2: å¯’æ­¦çºª MLU é€‚é…ï¼ˆWeek 5-6ï¼‰

ç±»ä¼¼æ˜‡è…¾æµç¨‹ï¼Œå®ç° `MLUKVBackend` å’Œ `CNCLCommBackend`

#### Step 3: æ€§èƒ½å¯¹é½ï¼ˆWeek 7-8ï¼‰

- å¯¹æ¯” CUDA vs æ˜‡è…¾ vs å¯’æ­¦çºªæ€§èƒ½
- ä¼˜åŒ– Kernelï¼ˆå¦‚éœ€è¦ï¼‰
- é€šä¿¡å¸¦å®½è°ƒä¼˜

---

## ğŸ’° MVP ç¡¬ä»¶éœ€æ±‚ï¼ˆæœ€å°åŒ–ï¼‰

### æ¨èé…ç½®

| ç¡¬ä»¶ | è§„æ ¼ | æ•°é‡ | ç”¨é€” | é¢„ç®— |
|------|------|------|------|------|
| **NVIDIA GPU** | RTX 3090/4090 (24GB) | 2 å¼  | MVP å¼€å‘ä¸æµ‹è¯• | Â¥20,000 - Â¥30,000 |
| æˆ– | A100 40GB | 1 å¼  | æ€§èƒ½åŸºçº¿å¯¹æ¯” | Â¥50,000ï¼ˆç§Ÿç”¨äº‘æœåŠ¡å™¨ï¼‰|

### äº‘æœåŠ¡å™¨æ–¹æ¡ˆï¼ˆæ›´ç»æµï¼‰

| å¹³å° | å®ä¾‹ç±»å‹ | é…ç½® | ä»·æ ¼ | é€‚ç”¨é˜¶æ®µ |
|------|---------|------|------|---------|
| **é˜¿é‡Œäº‘** | ecs.gn7i-c16g1.4xlarge | 1 Ã— A10 (24GB) | Â¥10/å°æ—¶ | MVP å¼€å‘ |
| **è…¾è®¯äº‘** | GN7.2XLARGE32 | 1 Ã— V100 (16GB) | Â¥8/å°æ—¶ | åŠŸèƒ½æµ‹è¯• |
| **AWS** | p3.2xlarge | 1 Ã— V100 (16GB) | $3/å°æ—¶ | æ€§èƒ½åŸºçº¿ |

**MVP é˜¶æ®µæ€»æˆæœ¬**: Â¥5,000 - Â¥10,000ï¼ˆäº‘æœåŠ¡å™¨ 2 ä¸ªæœˆï¼‰

---

## ğŸ“ å¼€å‘æ¸…å•ï¼ˆChecklistï¼‰

### Phase 0: MVP æ ¸å¿ƒï¼ˆ2 å‘¨ï¼‰

- [ ] **Week 1: åŸºç¡€æ¶æ„**
  - [ ] Day 1-2: åˆ›å»ºç›®å½• + Protocol å®šä¹‰
  - [ ] Day 3-4: CUDA KV Backend å®ç°
  - [ ] Day 5-7: Scheduler IR å®ç°
  
- [ ] **Week 2: å¼•æ“é›†æˆ**
  - [ ] Day 8-10: NCCL Comm Backend å®ç°
  - [ ] Day 11-12: SageLLMEngine ä¸»ç±»
  - [ ] Day 13-14: ç«¯åˆ°ç«¯æµ‹è¯• + æ€§èƒ½éªŒæ”¶

### Phase 1: å›½äº§ç¡¬ä»¶é€‚é…ï¼ˆç¡¬ä»¶åˆ°ä½åï¼‰

- [ ] **Week 3-4: æ˜‡è…¾ 910B**
  - [ ] Ascend KV Backend
  - [ ] HCCL Comm Backend
  - [ ] æ€§èƒ½å¯¹æ¯”æµ‹è¯•
  
- [ ] **Week 5-6: å¯’æ­¦çºª MLU370**
  - [ ] MLU KV Backend
  - [ ] CNCL Comm Backend
  - [ ] æ€§èƒ½å¯¹æ¯”æµ‹è¯•

### Phase 2: é«˜çº§åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰

- [ ] **Week 7-8: å‰ç¼€å¤ç”¨**
  - [ ] Radix Attention å®ç°
  - [ ] å‘½ä¸­ç‡ä¼˜åŒ–
  
- [ ] **Week 9-10: æ·˜æ±°ç­–ç•¥**
  - [ ] LRU/LFU/S3FIFO å®ç°
  - [ ] æ”¶ç›Š-ä»£ä»·æ¨¡å‹

---

## ğŸ¯ é‡Œç¨‹ç¢‘ä¸äº¤ä»˜ç‰©

### M1: MVP æ ¸å¿ƒå®Œæˆï¼ˆWeek 2ï¼‰

**äº¤ä»˜ç‰©**:
- âœ… sageLLM å¼•æ“ä»£ç ï¼ˆCUDA ç‰ˆæœ¬ï¼‰
- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥80%
- âœ… ç«¯åˆ°ç«¯æµ‹è¯•é€šè¿‡
- âœ… æ€§èƒ½åŸºçº¿æŠ¥å‘Šï¼ˆvs vLLMï¼‰

**éªŒæ”¶æ ‡å‡†**:
```bash
# 1. å¼•æ“æ³¨å†ŒæˆåŠŸ
sage llm engine start Qwen/Qwen2.5-1.5B-Instruct --engine-kind sagellm

# 2. æ¨ç†æ­£å¸¸
python -c "
from sage.llm import UnifiedInferenceClient
client = UnifiedInferenceClient.create()
response = client.chat([{'role': 'user', 'content': 'Hello'}])
print(response)
"

# 3. æ€§èƒ½æµ‹è¯•
python benchmarks/bench_mvp.py --model Qwen/Qwen2.5-1.5B-Instruct --batch-size 8
```

### M2: å›½äº§ç¡¬ä»¶é€‚é…ï¼ˆç¡¬ä»¶åˆ°ä½å 4-6 å‘¨ï¼‰

**äº¤ä»˜ç‰©**:
- âœ… Ascend/MLU KV Backend
- âœ… HCCL/CNCL Comm Backend
- âœ… æ€§èƒ½å¯¹æ¯”æŠ¥å‘Šï¼ˆCUDA vs æ˜‡è…¾ vs å¯’æ­¦çºªï¼‰

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

### å†…éƒ¨æ–‡æ¡£
- [SAGE æ¶æ„æ€»è§ˆ](../../package-architecture.md)
- [Control Plane è®¾è®¡](../../l1-common/control-plane.md)
- [ç¡¬ä»¶é‡‡è´­æ¸…å•](./HARDWARE_PROCUREMENT.md)

### å¤–éƒ¨å‚è€ƒ
- [vLLM æ¶æ„](https://github.com/vllm-project/vllm)
- [NCCL æ–‡æ¡£](https://docs.nvidia.com/deeplearning/nccl/)
- [æ˜‡è…¾ CANN](https://www.hiascend.com/document)
- [å¯’æ­¦çºª CNToolkit](https://www.cambricon.com/docs)

---

## â“ FAQ

### Q1: ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ vLLMï¼Ÿ
**A**: sageLLM çš„ç›®æ ‡æ˜¯ï¼š
1. **æ¨¡å—åŒ–è®¾è®¡**: æ¯ä¸ªå­ç³»ç»Ÿç‹¬ç«‹å¯æ›¿æ¢
2. **å›½äº§ä¼˜å…ˆ**: å†…ç½®å›½äº§ç¡¬ä»¶é€‚é…ï¼ˆvLLM ä»…æ”¯æŒ NVIDIAï¼‰
3. **ç ”ç©¶å¯¼å‘**: éªŒè¯æ–°çš„è°ƒåº¦/ç¼“å­˜/é€šä¿¡ç­–ç•¥

### Q2: MVP å’Œ vLLM æ€§èƒ½å·®è·æœ‰å¤šå¤§ï¼Ÿ
**A**: MVP é˜¶æ®µé¢„æœŸæ€§èƒ½ï¼š
- TTFT/TPOT: vLLM Ã— 0.95ï¼ˆ5% æŸå¤±å¯æ¥å—ï¼‰
- ååé‡: vLLM Ã— 0.90ï¼ˆ10% æŸå¤±å¯æ¥å—ï¼‰
- ä¼˜åŒ–åç›®æ ‡: vLLM Ã— 1.0ï¼ˆæ€§èƒ½æŒå¹³ï¼‰

### Q3: ç¡¬ä»¶åˆ°ä½åå¤šä¹…èƒ½å®Œæˆé€‚é…ï¼Ÿ
**A**: 
- æ˜‡è…¾ 910B: 2 å‘¨ï¼ˆHCCL æ–‡æ¡£å®Œå–„ï¼‰
- å¯’æ­¦çºª MLU: 2 å‘¨ï¼ˆCNCL ç±»ä¼¼ NCCLï¼‰
- æµ·å…‰ DCU: 1 å‘¨ï¼ˆHIP å…¼å®¹ CUDAï¼‰
- æ˜†ä»‘èŠ¯: 2 å‘¨ï¼ˆéœ€ç†Ÿæ‚‰ XPU SDKï¼‰

### Q4: MVP èƒ½ç”¨äºç”Ÿäº§å—ï¼Ÿ
**A**: MVP ä¸»è¦ç”¨äºï¼š
- âœ… æ¶æ„éªŒè¯
- âœ… ç®—æ³•ç ”ç©¶
- âœ… æ€§èƒ½åŸºçº¿å»ºç«‹
- âŒ ä¸å»ºè®®ç›´æ¥ç”¨äºç”Ÿäº§ï¼ˆéœ€å……åˆ†æµ‹è¯•å’Œä¼˜åŒ–ï¼‰

---

## ğŸ‰ æ€»ç»“

### MVP æ ¸å¿ƒä»·å€¼

1. **å¿«é€ŸéªŒè¯**: 2 å‘¨å®Œæˆæ¶æ„é—­ç¯
2. **æˆæœ¬å¯æ§**: äº‘æœåŠ¡å™¨ Â¥5,000 - Â¥10,000
3. **é£é™©é™ä½**: ç¡¬ä»¶åˆ°ä½å‰éªŒè¯è®¾è®¡
4. **å¹³æ»‘è¿‡æ¸¡**: é¢„ç•™å›½äº§ç¡¬ä»¶æ¥å£

### å…³é”®æˆåŠŸå› ç´ 

- âœ… **æŠ½è±¡è®¾è®¡**: Protocol é©±åŠ¨ï¼Œä¾¿äºåç»­æ›¿æ¢
- âœ… **å¢é‡æ¼”è¿›**: CUDA å…ˆè¡Œï¼Œå›½äº§ç¡¬ä»¶è·Ÿè¿›
- âœ… **æ€§èƒ½åŸºçº¿**: ä¸ vLLM å¯¹æ¯”ï¼Œé‡åŒ–æ”¹è¿›
- âœ… **æ–‡æ¡£å®Œå–„**: é™ä½åç»­å¼€å‘è€…æ¥å…¥æˆæœ¬

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2026-01-02  
**ä½œè€…**: SAGE é¡¹ç›®ç»„  
**è”ç³»æ–¹å¼**: [å¡«å†™é‚®ç®±]
