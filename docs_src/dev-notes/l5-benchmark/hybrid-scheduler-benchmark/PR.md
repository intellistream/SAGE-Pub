# Pull Request: Control Plane Scheduler Benchmark Framework

## ğŸ“‹ æ¦‚è¿°

æœ¬ PR ä¸º sageLLM Control Plane çš„è°ƒåº¦ç­–ç•¥å®ç°å®Œæ•´çš„ Benchmark è¯„æµ‹æ¡†æ¶ï¼Œæ”¯æŒ LLM è°ƒåº¦å’Œæ··åˆè°ƒåº¦ï¼ˆLLM +
Embeddingï¼‰ä¸¤ç§æ¨¡å¼ï¼Œå¹¶æä¾›è‡ªåŠ¨å¯è§†åŒ–å’Œé¢„å®šä¹‰å®éªŒåŠŸèƒ½ã€‚

## ğŸ”— å…³è”

- **åˆ†æ”¯**: `feature/embedding_lmm_mixed_scheduler`
- **å®ç°è¯´æ˜**: [README.md](./README.md)

______________________________________________________________________

## âœ¨ æ–°å¢åŠŸèƒ½

### 1. Benchmark æ¡†æ¶é‡æ„

- å°†åŸæœ‰ `benchmark_control_plane/` ä»£ç é‡æ„ä¸º `llm_scheduler/` å­æ¨¡å—
- æŠ½å–å…±äº«ç»„ä»¶åˆ° `common/` ç›®å½•ï¼ˆbase_config, base_metrics, gpu_monitor, strategy_adapterï¼‰
- ä¿æŒ API å‘åå…¼å®¹ï¼ˆ`BenchmarkConfig` ç­‰åˆ«åï¼‰

### 2. æ··åˆè°ƒåº¦ Benchmark (`hybrid_scheduler/`)

æ–°å¢æ”¯æŒ LLM + Embedding æ··åˆè´Ÿè½½çš„ Benchmarkï¼š

- `HybridBenchmarkConfig`: æ”¯æŒ `llm_ratio`, `embedding_model`, `embedding_slo_deadline_ms` ç­‰é…ç½®
- `HybridWorkloadGenerator`: ç”Ÿæˆæ··åˆè¯·æ±‚åºåˆ—ï¼ˆLLM_CHAT / LLM_GENERATE / EMBEDDINGï¼‰
- `HybridBenchmarkClient`: åŒæ—¶æ”¯æŒ `/v1/chat/completions` å’Œ `/v1/embeddings`
- `HybridMetricsCollector`: åˆ†åˆ«æ”¶é›† LLM å’Œ Embedding æŒ‡æ ‡
- `HybridBenchmarkRunner`: æ‰§è¡Œæ··åˆ Benchmark å¹¶é›†æˆ GPU ç›‘æ§

### 3. å¯è§†åŒ–æ¨¡å— (`visualization/`)

Benchmark è¿è¡Œå®Œæˆåè‡ªåŠ¨ç”Ÿæˆå›¾è¡¨å’ŒæŠ¥å‘Šï¼š

- **å›¾è¡¨ç±»å‹**:
  - ååé‡: comparison, vs_rate
  - å»¶è¿Ÿ: distribution, percentiles, CDF
  - SLO: compliance, by_priority
  - GPU: utilization, memory
  - æ··åˆ: ratio_impact, type_breakdown, batch_efficiency
- **æŠ¥å‘Šæ ¼å¼**: HTMLï¼ˆJinja2 æ¨¡æ¿ï¼‰ã€Markdown
- **Runner é›†æˆ**: `auto_visualize=True` å‚æ•°

### 4. é¢„å®šä¹‰å®éªŒ (`experiments/`)

| å®éªŒ                    | æè¿°                                     |
| ----------------------- | ---------------------------------------- |
| `throughput_exp.py`     | æ‰«æè¯·æ±‚é€Ÿç‡ï¼Œæ‰¾åˆ°æœ€å¤§ååé‡             |
| `latency_exp.py`        | å›ºå®šè´Ÿè½½ä¸‹åˆ†æå»¶è¿Ÿåˆ†å¸ƒ                   |
| `slo_compliance_exp.py` | å¯¹æ¯”å„ç­–ç•¥çš„ SLO è¾¾æˆç‡                  |
| `mixed_ratio_exp.py`    | æµ‹è¯•ä¸åŒ LLM/Embedding æ¯”ä¾‹ï¼ˆä»… hybridï¼‰ |

### 5. CLI å·¥å…· (`sage-cp-bench`)

```bash
# LLM Benchmark
sage-cp-bench run --mode llm --policy fifo --requests 100 --rate 10

# Hybrid Benchmark
sage-cp-bench run --mode hybrid --policy hybrid_slo --llm-ratio 0.7 --requests 100

# ç­–ç•¥å¯¹æ¯”
sage-cp-bench compare --mode hybrid --policies fifo,priority,hybrid_slo

# è¿è¡Œå®éªŒ
sage-cp-bench experiment --name throughput --policies fifo,priority

# ä»ç»“æœç”Ÿæˆå¯è§†åŒ–
sage-cp-bench visualize --input results.json --output ./charts
```

### 6. æµ‹è¯•æ•°æ®ç›®å½•

```
sage/data/sources/control_plane_benchmark/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ llm_workloads/     # light, medium, heavy
â”‚   â”œâ”€â”€ hybrid_workloads/  # balanced, llm_heavy, embed_heavy, burst
â”‚   â””â”€â”€ prompts/           # llm_prompts.jsonl, embed_texts.jsonl
â”œâ”€â”€ dataloader.py          # æ•°æ®åŠ è½½å™¨
â””â”€â”€ dataset.yaml           # æ•°æ®é›†é…ç½®
```

______________________________________________________________________

## ğŸ“ æ–‡ä»¶å˜æ›´

### æ–°å¢æ–‡ä»¶ (~36 ä¸ªæºæ–‡ä»¶)

```
packages/sage-benchmark/src/sage/benchmark/benchmark_control_plane/
â”œâ”€â”€ common/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_config.py          # åŸºç¡€é…ç½®ç±»
â”‚   â”œâ”€â”€ base_metrics.py         # åŸºç¡€æŒ‡æ ‡ç±»
â”‚   â”œâ”€â”€ gpu_monitor.py          # GPU ç›‘æ§ï¼ˆpynvml/nvidia-smi/mockï¼‰
â”‚   â””â”€â”€ strategy_adapter.py     # ç­–ç•¥é€‚é…å™¨
â”œâ”€â”€ llm_scheduler/              # é‡æ„åçš„ LLM Benchmark
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ workload.py
â”‚   â”œâ”€â”€ client.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ runner.py
â”‚   â””â”€â”€ reporter.py
â”œâ”€â”€ hybrid_scheduler/           # æ–°å¢æ··åˆ Benchmark
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ workload.py
â”‚   â”œâ”€â”€ client.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ runner.py
â”‚   â””â”€â”€ reporter.py
â”œâ”€â”€ visualization/              # æ–°å¢å¯è§†åŒ–
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ charts.py
â”‚   â”œâ”€â”€ report_generator.py
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ benchmark_report.html
â”‚       â””â”€â”€ comparison_report.html
â”œâ”€â”€ experiments/                # æ–°å¢é¢„å®šä¹‰å®éªŒ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_experiment.py
â”‚   â”œâ”€â”€ throughput_exp.py
â”‚   â”œâ”€â”€ latency_exp.py
â”‚   â”œâ”€â”€ slo_compliance_exp.py
â”‚   â””â”€â”€ mixed_ratio_exp.py
â”œâ”€â”€ __init__.py                 # æ›´æ–°ï¼šç»Ÿä¸€å¯¼å‡º
â”œâ”€â”€ cli.py                      # æ›´æ–°ï¼šæ”¯æŒ hybrid æ¨¡å¼
â”œâ”€â”€ README.md
â”œâ”€â”€ DATA_PATHS.md
â””â”€â”€ VISUALIZATION.md

packages/sage-benchmark/src/sage/data/sources/control_plane_benchmark/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ dataloader.py
â”œâ”€â”€ dataset.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ llm_workloads/*.jsonl
â”‚   â”œâ”€â”€ hybrid_workloads/*.jsonl
â”‚   â””â”€â”€ prompts/*.jsonl
â””â”€â”€ metadata/
```

### ä¿®æ”¹æ–‡ä»¶

- `packages/sage-benchmark/src/sage/benchmark/benchmark_control_plane/__init__.py`
  - æ·»åŠ  commonã€llm_schedulerã€hybrid_schedulerã€visualizationã€experiments å¯¼å‡º
  - ä¿æŒå‘åå…¼å®¹çš„åˆ«å
- `packages/sage-benchmark/src/sage/benchmark/benchmark_control_plane/cli.py`
  - æ”¯æŒ `--mode llm|hybrid`
  - æ–°å¢ `experiment`ã€`visualize` å‘½ä»¤

### æ–°å¢æµ‹è¯• (~3400 è¡Œ)

```
packages/sage-benchmark/tests/benchmark_control_plane/
â”œâ”€â”€ test_benchmark_control_plane.py  # åŸæœ‰æµ‹è¯•
â”œâ”€â”€ test_cli.py                      # CLI æµ‹è¯•
â”œâ”€â”€ test_common.py                   # å…±äº«ç»„ä»¶æµ‹è¯•
â”œâ”€â”€ test_experiments.py              # å®éªŒæµ‹è¯•
â”œâ”€â”€ test_hybrid_scheduler.py         # æ··åˆè°ƒåº¦æµ‹è¯•
â”œâ”€â”€ test_llm_scheduler.py            # LLM è°ƒåº¦æµ‹è¯•
â””â”€â”€ test_visualization.py            # å¯è§†åŒ–æµ‹è¯•
```

______________________________________________________________________

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰ benchmark_control_plane æµ‹è¯•
cd /home/yjy/SAGE/packages/sage-benchmark
pytest tests/benchmark_control_plane/ -v

# å¿«é€Ÿæµ‹è¯•
pytest tests/benchmark_control_plane/ -v --tb=short
```

### æµ‹è¯•è¦†ç›–

- âœ… é…ç½®éªŒè¯ï¼ˆLLM/Hybridï¼‰
- âœ… è´Ÿè½½ç”Ÿæˆï¼ˆå‡åŒ€/æ³Šæ¾/çªå‘ï¼‰
- âœ… æŒ‡æ ‡æ”¶é›†ä¸èšåˆ
- âœ… GPU ç›‘æ§ï¼ˆå« Mock æ¨¡å¼ï¼‰
- âœ… ç­–ç•¥é€‚é…å™¨
- âœ… å›¾è¡¨ç”Ÿæˆ
- âœ… æŠ¥å‘Šç”Ÿæˆï¼ˆHTML/Markdownï¼‰
- âœ… CLI å‘½ä»¤
- âœ… é¢„å®šä¹‰å®éªŒ

______________________________________________________________________

## ğŸ“Š ä»£ç ç»Ÿè®¡

| ç±»åˆ«     | æ–‡ä»¶æ•° |  ä»£ç è¡Œæ•°   |
| -------- | :----: | :---------: |
| æºä»£ç    |   36   |   ~14,000   |
| æµ‹è¯•ä»£ç  |   7    |   ~3,400    |
| æ–‡æ¡£     |   4    |   ~1,800    |
| æ•°æ®æ–‡ä»¶ |   9    |    ~600     |
| **æ€»è®¡** | **56** | **~19,800** |

______________________________________________________________________

## ğŸ“ ä½¿ç”¨ç¤ºä¾‹

### Python API

```python
# LLM Benchmark
from sage.benchmark.benchmark_control_plane import (
    LLMBenchmarkConfig,
    LLMBenchmarkRunner,
)

config = LLMBenchmarkConfig(
    control_plane_url="http://localhost:8080",
    num_requests=1000,
    request_rate=100.0,
    policies=["fifo", "priority", "slo_aware"],
)
runner = LLMBenchmarkRunner(config)
result = await runner.run()

# Hybrid Benchmark
from sage.benchmark.benchmark_control_plane.hybrid_scheduler import (
    HybridBenchmarkConfig,
    HybridBenchmarkRunner,
)

config = HybridBenchmarkConfig(
    control_plane_url="http://localhost:8080",
    num_requests=500,
    llm_ratio=0.7,
    embedding_model="BAAI/bge-m3",
    policies=["hybrid_slo"],
)
runner = HybridBenchmarkRunner(config)
result = await runner.run()  # è‡ªåŠ¨ç”Ÿæˆå›¾è¡¨
```

### CLI å¿«é€Ÿå¼€å§‹

```bash
# 1. è¿è¡Œç®€å• LLM Benchmark
sage-cp-bench run --mode llm --policy fifo --requests 100 --rate 10

# 2. è¿è¡Œæ··åˆ Benchmark
sage-cp-bench run --mode hybrid --policy hybrid_slo --llm-ratio 0.7 --requests 100

# 3. å¯¹æ¯”å¤šä¸ªç­–ç•¥
sage-cp-bench compare --mode llm --policies fifo,priority,slo_aware --requests 500

# 4. è¿è¡Œååé‡å®éªŒ
sage-cp-bench experiment --name throughput --policies fifo,priority

# 5. ä»å·²æœ‰ç»“æœç”Ÿæˆå¯è§†åŒ–
sage-cp-bench visualize --input ./results/benchmark_result.json --output ./charts
```

______________________________________________________________________

## âœ… æ£€æŸ¥æ¸…å•

- [x] ä»£ç é€šè¿‡ Ruff æ ¼å¼æ£€æŸ¥
- [x] ä»£ç é€šè¿‡ Mypy ç±»å‹æ£€æŸ¥ï¼ˆwarning æ¨¡å¼ï¼‰
- [x] å•å…ƒæµ‹è¯•å…¨éƒ¨é€šè¿‡
- [x] å‘åå…¼å®¹ï¼ˆåŸæœ‰ API ä¸å˜ï¼‰
- [x] æ–‡æ¡£å®Œæ•´ï¼ˆREADME, DATA_PATHS, VISUALIZATIONï¼‰
- [x] CLI å¸®åŠ©ä¿¡æ¯å®Œæ•´

______________________________________________________________________

## ğŸ“ é™„ä»¶

- [ROADMAP.md](./ROADMAP.md) - å®Œæ•´å¼€å‘è·¯çº¿å›¾
- [TASKS.md](./TASKS.md) - å›¢é˜Ÿä»»åŠ¡åˆ†è§£ï¼ˆ6 ä¸ª Taskï¼Œ18 ä¸ªå­ä»»åŠ¡ï¼‰
- [README.md](./README.md) - å®ç°æ€»ç»“

______________________________________________________________________

## ğŸ”œ åç»­è®¡åˆ’

1. **æ€§èƒ½ä¼˜åŒ–**: å¤§è§„æ¨¡è´Ÿè½½ä¸‹çš„å†…å­˜ä¼˜åŒ–
1. **æ›´å¤šå›¾è¡¨**: æ·»åŠ  Plotly äº¤äº’å¼å›¾è¡¨æ”¯æŒ
1. **CI é›†æˆ**: æ·»åŠ  benchmark å›å½’æµ‹è¯•åˆ° CI
1. **å®éªŒæ‰©å±•**: æ·»åŠ æ›´å¤šé¢„å®šä¹‰å®éªŒï¼ˆfairness, cost, tail latencyï¼‰
