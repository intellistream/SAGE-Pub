# Deployment Guide

Deploy SAGE applications and the sageLLMæœåŠ¡æ ˆ (LLM / Embedding / Gateway) in a variety of environments.

---

## Quick Start: sage llm serve

`sage` CLI å†…ç½®äº†ä¸€é”®å¯åŠ¨/åœæ­¢ LLM æœåŠ¡çš„å‘½ä»¤ï¼Œé€‚åˆå¼€å‘å’Œå°è§„æ¨¡éƒ¨ç½²ï¼š

```bash
# å¯åŠ¨é»˜è®¤æ¨¡å‹ï¼ˆLLMï¼‰
sage llm serve

# æ˜¾å¼æŒ‡å®šæ¨¡å‹ä¸ç«¯å£
sage llm serve \
  --model Qwen/Qwen2.5-7B-Instruct \
  --port 8901

# åŒæ—¶å¯åŠ¨ LLM + Embedding
sage llm serve --with-embedding \
  --model Qwen/Qwen2.5-7B-Instruct \
  --embedding-model BAAI/bge-m3

# æŸ¥çœ‹çŠ¶æ€ / æ—¥å¿— / åœæ­¢
sage llm status
sage llm logs --follow
sage llm stop
```

`sage llm serve` å†…éƒ¨ä¼šç»Ÿä¸€ä½¿ç”¨ `SagePorts`ï¼Œå› æ­¤**ä¸¥ç¦**åœ¨ä»£ç ä¸­ç¡¬ç¼–ç ç«¯å£å·ã€‚ç›¸å…³ç«¯å£å¦‚ä¸‹ï¼š

> ğŸ’¡ ä½¿ç”¨ `sage llm model list-remote` å¯ä»¥æŸ¥çœ‹å®˜æ–¹æ¨èçš„å¸¸ç”¨æ¨¡å‹ï¼Œå¹¶ç»“åˆ `sage llm model download` é¢„çƒ­ç¼“å­˜ã€‚

| å¸¸é‡ | ç«¯å£ | ç”¨é€” |
|------|------|------|
| `SagePorts.GATEWAY_DEFAULT` | 8000 | OpenAI å…¼å®¹ Gateway |
| `SagePorts.LLM_DEFAULT` | 8001 | vLLM æ¨ç†æœåŠ¡ |
| `SagePorts.BENCHMARK_LLM` | 8901 | WSL2 / Benchmark å¤‡ç”¨ |
| `SagePorts.EMBEDDING_DEFAULT` | 8090 | Embedding æœåŠ¡ |
| `SagePorts.STUDIO_BACKEND` | 8080 | Studio åç«¯ |
| `SagePorts.STUDIO_FRONTEND` | 5173 | Studio å‰ç«¯ |

---

## åŠ¨æ€å¼•æ“ç®¡ç†

Control Plane æ”¯æŒè¿è¡Œæ—¶åŠ¨æ€å¯åŠ¨/åœæ­¢æ¨ç†å¼•æ“ï¼Œå¹¶è‡ªåŠ¨è¿½è¸ª GPU æ˜¾å­˜ï¼š

### å¼•æ“å‘½ä»¤

```bash
# åˆ—å‡ºå½“å‰è¿è¡Œçš„å¼•æ“
sage llm engine list

# å¯åŠ¨ LLM å¼•æ“ï¼ˆæ”¯æŒ tensor/pipeline å¹¶è¡Œï¼‰
sage llm engine start Qwen/Qwen2.5-7B-Instruct --tensor-parallel 2

# å¯åŠ¨ Embedding å¼•æ“
sage llm engine start BAAI/bge-m3 --engine-kind embedding --port 8095

# åœæ­¢æŒ‡å®šå¼•æ“
sage llm engine stop <engine_id>

# æŸ¥çœ‹ GPU çŠ¶æ€
sage llm gpu
```

### é¢„è®¾ç¼–æ’

ä½¿ç”¨é¢„è®¾ä¸€é”®éƒ¨ç½²å¤šä¸ªå¼•æ“ç»„åˆï¼Œé¿å…æ‰‹åŠ¨é€ä¸ªå¯åŠ¨ï¼š

```bash
# åˆ—å‡ºå†…ç½®é¢„è®¾
sage llm preset list

# æŸ¥çœ‹é¢„è®¾è¯¦æƒ…
sage llm preset show -n qwen-mini-with-embeddings

# åº”ç”¨é¢„è®¾ï¼ˆå¯åŠ  --dry-run é¢„è§ˆï¼‰
sage llm preset apply -n qwen-mini-with-embeddings

# ä½¿ç”¨è‡ªå®šä¹‰ YAML
sage llm preset apply --file ./my-preset.yaml
```

é¢„è®¾ YAML ç¤ºä¾‹ï¼š

```yaml
version: 1
name: qwen-mini-with-embeddings
engines:
  - name: chat
    kind: llm
    model: Qwen/Qwen2.5-1.5B-Instruct
    tensor_parallel: 1
    label: chat-qwen15b
  - name: embed
    kind: embedding
    model: BAAI/bge-small-zh-v1.5
    label: embedding-bge
```

---

## Deploy Individual Services

### 1. LLM æœåŠ¡ï¼ˆvLLMï¼‰

```bash
SAGE_MODEL="Qwen/Qwen2.5-7B-Instruct"

# ä½¿ç”¨ sage llm serveï¼ˆæ¨èï¼‰
sage llm serve --model "$SAGE_MODEL" --port 8901

# å¥åº·æ£€æŸ¥
curl http://localhost:8901/v1/models
```

### 2. Embedding æœåŠ¡

```bash
# é€šè¿‡ sage llm serve åŒæ—¶å¯åŠ¨
sage llm serve --with-embedding --embedding-model BAAI/bge-m3 --embedding-port 8090

# æˆ–å•ç‹¬å¯åŠ¨ Embedding æœåŠ¡
python -m sage.common.components.sage_embedding.embedding_server \
  --model BAAI/bge-m3 \
  --port 8090

# å¥åº·æ£€æŸ¥
curl http://localhost:8090/v1/models
```

### 3. ä½¿ç”¨å®¢æˆ·ç«¯

```python
from sage.common.components.sage_llm import UnifiedInferenceClient

# è‡ªåŠ¨æ£€æµ‹æœ¬åœ°æœåŠ¡ï¼ˆæ¨èï¼‰
client = UnifiedInferenceClient.create()

# æˆ–æ˜¾å¼é…ç½®è¿æ¥åˆ°ç‰¹å®šæœåŠ¡
client = UnifiedInferenceClient.create(
    control_plane_url="http://localhost:8901/v1",
    default_llm_model="Qwen/Qwen2.5-7B-Instruct",
)
```

---

## Deployment Options

### 1. Local Development

For development and testing:

```bash
# Install SAGE
./quickstart.sh

# Run your application
python my_app.py
```

**Best for**: Development, testing, small-scale experiments

### 2. Single Server Deployment

Deploy on a single machine with multiple workers:

```bash
# Use Ray for distributed execution on single machine
export SAGE_EXECUTION_MODE=ray
export RAY_NUM_CPUS=8

python my_app.py
```

**Best for**: Medium-scale workloads, production with limited resources

### 3. Distributed Cluster

Deploy across multiple machines:

```bash
# On head node
ray start --head --port=6379

# On worker nodes
ray start --address='head_node_ip:6379'

# Run application
export SAGE_EXECUTION_MODE=distributed
python my_app.py
```

**Best for**: Large-scale production workloads

### 4. Kubernetes Deployment

Deploy SAGE on Kubernetes:

```yaml
# sage-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sage-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: sage
  template:
    metadata:
      labels:
        app: sage
    spec:
      containers:
      - name: sage
        image: sage:latest
        env:
        - name: SAGE_EXECUTION_MODE
          value: "distributed"
        - name: RAY_ADDRESS
          value: "ray-head:6379"
```

**Best for**: Cloud-native deployments, auto-scaling

### 5. Docker Container

Containerize your SAGE application:

```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install SAGE
COPY . /app
RUN pip install -e .

# Run application
CMD ["python", "my_app.py"]
```

Build and run:

```bash
docker build -t my-sage-app .
docker run -p 8000:8000 my-sage-app
```

**Best for**: Reproducible deployments, CI/CD

## Configuration

### Environment Variables

```bash
# Execution mode
export SAGE_EXECUTION_MODE=local|ray|distributed

# API Keys
export OPENAI_API_KEY=sk-...
export JINA_API_KEY=jina_...

# Ray configuration
export RAY_ADDRESS=localhost:6379
export RAY_NUM_CPUS=8
export RAY_NUM_GPUS=1

# sageLLM stack
export SAGE_CHAT_BASE_URL=http://localhost:8901/v1
export SAGE_EMBEDDING_BASE_URL=http://localhost:8090/v1
export SAGE_UNIFIED_BASE_URL=http://localhost:8000/v1  # Gateway
export SAGE_CHAT_MODEL=Qwen/Qwen2.5-7B-Instruct
export SAGE_EMBEDDING_MODEL=BAAI/bge-m3

# Logging
export SAGE_LOG_LEVEL=INFO
export SAGE_LOG_DIR=./logs

# Performance
export SAGE_MAX_WORKERS=16
export SAGE_BATCH_SIZE=32
```

### Configuration Files

Create a `.env` file:

```ini
# .env
SAGE_EXECUTION_MODE=distributed
OPENAI_API_KEY=sk-...
RAY_ADDRESS=ray-cluster:6379
SAGE_LOG_LEVEL=INFO
```

Load in your application:

```python
from sage.common.config import load_env

load_env(".env")
```

## Production Considerations

### 1. Monitoring

Monitor SAGE applications:

```python
from sage.kernel.api.local_environment import LocalStreamEnvironment

env = LocalStreamEnvironment(
    "production_app",
    config={"monitoring": {"enabled": True, "metrics_port": 9090, "log_level": "INFO"}},
)
```

### 2. Fault Tolerance

Enable checkpointing:

```python
env = LocalStreamEnvironment(
    "fault_tolerant_app",
    config={
        "fault_tolerance": {
            "strategy": "checkpoint",
            "checkpoint_interval": 60.0,
            "checkpoint_dir": "/data/checkpoints",
        }
    },
)
```

### 3. Resource Management

Configure resources:

```python
env = LocalStreamEnvironment(
    "resource_managed_app",
    config={
        "resources": {"max_workers": 16, "memory_limit": "32GB", "gpu_enabled": True}
    },
)
```

### 4. Security

Secure API keys and credentials:

```python
# Use environment variables
import os

api_key = os.getenv("OPENAI_API_KEY")

# Or use secret management
from sage.common.config import SecretManager

secrets = SecretManager()
api_key = secrets.get("openai_api_key")
```

## Scaling

### Horizontal Scaling

Add more worker nodes:

```bash
# Add Ray workers
for i in {1..5}; do
  ray start --address='head:6379' &
done
```

### Vertical Scaling

Increase resources per worker:

```python
config = {"resources": {"cpus_per_worker": 4, "memory_per_worker": "8GB"}}
```

## Cloud Platforms

### AWS

Deploy on AWS using ECS or EKS:

```yaml
# AWS ECS task definition
{
  "family": "sage-app",
  "containerDefinitions": [{
    "name": "sage",
    "image": "sage:latest",
    "memory": 8192,
    "cpu": 4096,
    "environment": [
      {"name": "SAGE_EXECUTION_MODE", "value": "distributed"}
    ]
  }]
}
```

### Google Cloud Platform

Deploy on GKE:

```bash
gcloud container clusters create sage-cluster \
  --num-nodes=3 \
  --machine-type=n1-standard-4

kubectl apply -f sage-deployment.yaml
```

### Azure

Deploy on AKS:

```bash
az aks create \
  --resource-group sage-rg \
  --name sage-cluster \
  --node-count 3 \
  --node-vm-size Standard_D4s_v3

kubectl apply -f sage-deployment.yaml
```

## Performance Optimization

### 1. Batch Processing

```python
config = {"batch_size": 64, "prefetch_size": 128}
```

### 2. Parallel Execution

```python
stream = env.from_source(source).map(operator, parallelism=8)  # Parallel instances
```

### 3. GPU Acceleration

```python
config = {"gpu_enabled": True, "gpu_memory_fraction": 0.8}
```

## Troubleshooting

### æœåŠ¡æ ˆå¸¸è§é—®é¢˜

- **LLM ç«¯å£å¯åŠ¨ä½†æ— æ³•è¿æ¥ï¼ˆç‰¹åˆ«æ˜¯ WSL2ï¼‰**ï¼šä½¿ç”¨ `SagePorts.get_recommended_llm_port()` æˆ– `sage llm serve --port 8901`ã€‚
- **Embedding ç”Ÿæˆ 404**ï¼šç¡®è®¤ `sage llm status` æ˜¾ç¤ºæœåŠ¡è¿è¡Œä¸­ï¼Œå¹¶ä½¿ç”¨ `/v1/embeddings` ç«¯ç‚¹ã€‚
- **Gateway è¿”å› 502**ï¼šGateway æ— æ³•è¿æ¥ä¸‹æ¸¸ LLMï¼Œæ£€æŸ¥ `--llm-port` å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚
- **æ¨¡å‹ä¸‹è½½ç¼“æ…¢**ï¼šè®¾ç½® `HF_ENDPOINT=https://hf-mirror.com` ä»¥ä½¿ç”¨å›½å†…é•œåƒã€‚

### Common Issues

**Ray cluster not connecting**:

```bash
# Check Ray status
ray status

# Check network connectivity
telnet head_node 6379
```

**Out of memory**:

```python
# Reduce batch size
config = {"batch_size": 16}

# Limit workers
config = {"max_workers": 4}
```

**Slow performance**:

```python
# Enable profiling
config = {"profiling": {"enabled": True}}

# Check bottlenecks
env.get_profiler().print_report()
```

## See Also

- [Getting Started](../../getting-started/quickstart.md)
- [Best Practices](../best-practices/index.md)
- [Architecture](../../concepts/architecture/overview.md)
