# é«˜çº§ RAG æŠ€æœ¯

> **ç›®æ ‡**ï¼šå­¦ä¹ å¦‚ä½•æ„å»ºä¼ä¸šçº§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ

## æ¦‚è¿°

ä¼ä¸šçº§ RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿéœ€è¦è€ƒè™‘æ£€ç´¢è´¨é‡ã€æ€§èƒ½ã€å¯æ‰©å±•æ€§ç­‰å¤šæ–¹é¢å› ç´ ã€‚SAGE æä¾›äº† `UnifiedInferenceClient` ç»Ÿä¸€æ¥å£ï¼Œå¯ä»¥æ— ç¼é›†æˆ LLM å’Œ Embedding æœåŠ¡ï¼Œç®€åŒ– RAG ç³»ç»Ÿçš„å¼€å‘ã€‚

## ç¤ºä¾‹ä¸Šæ‰‹ä¸‰ä»¶å¥—

| é¡¹ | å†…å®¹ |
| --- | --- |
| **æºç å…¥å£** | `examples/tutorials/L3-libs/rag/usage_4_complete_rag.py`ï¼ˆå«å®Œæ•´ RAG + DP Unlearning Pipelineï¼‰ |
| **è¿è¡Œå‘½ä»¤** | `python examples/tutorials/L3-libs/rag/usage_4_complete_rag.py` |
| **é¢„æœŸæ—¥å¿—** | ç»ˆç«¯ä¼šå…ˆæ‰“å° `RAGUnlearningSystem initialized`ï¼Œåç»­ `âœ“ Initialized RAG corpus ...`ã€`ğŸ“ Forget request ...`ã€`âœ… Completed unlearning` ç­‰æ­¥éª¤ï¼›å¦‚å¯ç”¨è°ƒè¯•æ¨¡å¼è¿˜ä¼šæ˜¾ç¤ºå®¡è®¡æ—¥å¿—å†™å…¥ |

> å»ºè®®åœ¨è¿è¡Œå‰ï¼š
> 1. å¯åŠ¨åŸºç¡€æ¨ç†æœåŠ¡ï¼š`sage llm serve --with-embedding --model Qwen/Qwen2.5-7B-Instruct --embedding-model BAAI/bge-m3`
> 2. è®¾ç½® `.env` ä¸­çš„ `OPENAI_API_KEY` / `HF_TOKEN`ï¼ˆè‹¥éœ€è®¿é—®äº‘ç«¯æ¨¡å‹ï¼‰ã€‚
> 3. æ‰§è¡Œ `sage-dev quality --check-only`ï¼Œç¡®ä¿è„šæœ¬ä¾èµ–çš„ `sage-libs`ã€`sage-middleware` å­åŒ…å·²é€šè¿‡é™æ€æ£€æŸ¥ã€‚

## UnifiedInferenceClient å¿«é€Ÿå…¥é—¨

### åŸºæœ¬ç”¨æ³•

`UnifiedInferenceClient` æ˜¯ SAGE æ¨èçš„ç»Ÿä¸€æ¨ç†å®¢æˆ·ç«¯ï¼Œæ”¯æŒ LLM Chat/Generate å’Œ Embedding ä¸¤ç§èƒ½åŠ›ï¼š

```python
from sage.llm import UnifiedInferenceClient

# æ–¹å¼ 1: Control Plane Firstï¼ˆè‡ªåŠ¨æ¢æµ‹æœ¬åœ°/äº‘ç«¯ï¼‰
client = UnifiedInferenceClient.create()

# æ–¹å¼ 2: æ˜¾å¼è¿æ¥ Gateway / Control Plane
client = UnifiedInferenceClient.create(
    control_plane_url="http://localhost:8000/v1",
    default_llm_model="Qwen/Qwen2.5-7B-Instruct",
    default_embedding_model="BAAI/bge-m3",
)

# Chat å¯¹è¯
response = client.chat([
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯ RAG?"}
])
print(response)

# Generate æ–‡æœ¬ç”Ÿæˆ
text = client.generate("RAG çš„æ ¸å¿ƒæ€æƒ³æ˜¯")
print(text)

# Embed å‘é‡åµŒå…¥
vectors = client.embed(["æ£€ç´¢å¢å¼ºç”Ÿæˆ", "å‘é‡æ•°æ®åº“"])
print(f"åµŒå…¥ç»´åº¦: {len(vectors[0])}")
```

### å¯åŠ¨åç«¯æœåŠ¡

```bash
# å¯åŠ¨ LLM æœåŠ¡ï¼ˆåå°è¿è¡Œï¼Œè‡ªåŠ¨æŒ‘é€‰ SagePorts.get_recommended_llm_port()ï¼‰
sage llm serve --model Qwen/Qwen2.5-7B-Instruct

# åŒæ—¶å¯åŠ¨ LLM + Embedding æœåŠ¡ï¼ˆEmbedding ç«¯å£é»˜è®¤ SagePorts.EMBEDDING_DEFAULT=8090ï¼‰
sage llm serve --with-embedding \
    --model Qwen/Qwen2.5-7B-Instruct \
    --embedding-model BAAI/bge-m3 \
    --port $(python -c "from sage.common.config.ports import SagePorts; print(SagePorts.get_recommended_llm_port())") \
    --embedding-port 8090

# æŸ¥çœ‹çŠ¶æ€ / åœæ­¢æœåŠ¡
sage llm status
sage llm stop
```

> WSL2 ä¸Š `SagePorts.LLM_DEFAULT (8001)` å¯èƒ½æ‹’ç»è¿æ¥ï¼Œ`sage llm serve` ä¼šè‡ªåŠ¨å›é€€åˆ° `SagePorts.LLM_WSL_FALLBACK (8901)`ï¼Œ`UnifiedInferenceClient.create()` åŒæ ·æŒ‰æ­¤é¡ºåºæ¢æµ‹ã€‚

### ç¯å¢ƒå˜é‡ä¸æœ¬åœ°æ¨¡å‹

```bash
# .envï¼ˆæ‘˜å½•ï¼‰
SAGE_CHAT_BASE_URL=http://localhost:${SAGE_LLM_PORT:-8901}/v1
SAGE_CHAT_MODEL=Qwen/Qwen2.5-7B-Instruct
SAGE_CHAT_API_KEY=              # æœ¬åœ°æœåŠ¡å¯ç•™ç©ºï¼Œäº‘ç«¯å›é€€æ‰éœ€è¦
SAGE_EMBEDDING_BASE_URL=http://localhost:8090/v1
SAGE_EMBEDDING_MODEL=BAAI/bge-m3
HF_TOKEN=hf_xxx
# detect_china_mainland()/ensure_hf_mirror_configured() ä¼šåœ¨ä¸­å›½å¤§é™†è‡ªåŠ¨è®¾ç½®ï¼š
HF_ENDPOINT=https://hf-mirror.com
```

- CLI ä¼šåœ¨å¯åŠ¨/ä¸‹è½½æ¨¡å‹å‰è°ƒç”¨ `ensure_hf_mirror_configured()`ï¼›è‹¥æƒ³æ‰‹åŠ¨æ£€æµ‹ï¼Œå¯åœ¨è„šæœ¬ä¸­è°ƒç”¨ `detect_china_mainland()`ã€‚
- ä¸æƒ³è¿è¡ŒæœåŠ¡æ—¶ï¼Œå¯ä½¿ç”¨ `EmbeddingFactory.create("hf", model=...)` å¹¶ç”¨ `EmbeddingClientAdapter` åŒ…è£…æˆæ‰¹é‡æ¥å£ï¼Œå†ä¸ `UnifiedInferenceClient` æ­é…ä»…è´Ÿè´£ LLM è°ƒç”¨ã€‚

## æ„å»º RAG Pipeline

### åŸºç¡€ RAG æµç¨‹

```python
from sage.kernel.api.local_environment import LocalEnvironment
from sage.common.core.functions.batch_function import BatchFunction
from sage.common.core.functions.map_function import MapFunction
from sage.common.core.functions.sink_function import SinkFunction
from sage.llm import UnifiedInferenceClient

# åˆå§‹åŒ–ç»Ÿä¸€å®¢æˆ·ç«¯ï¼ˆè‡ªåŠ¨æ¢æµ‹ï¼‰
client = UnifiedInferenceClient.create()


class QuerySource(BatchFunction):
    """æŸ¥è¯¢æ•°æ®æº"""
    def __init__(self, queries, **kwargs):
        super().__init__(**kwargs)
        self.queries = queries
        self.index = 0
    
    def execute(self):
        if self.index >= len(self.queries):
            return None
        query = self.queries[self.index]
        self.index += 1
        return {"query": query}


class EmbeddingOperator(MapFunction):
    """å‘é‡åµŒå…¥ç®—å­"""
    def execute(self, data):
        query = data["query"]
        # ä½¿ç”¨ UnifiedInferenceClient ç”Ÿæˆå‘é‡
        vectors = client.embed([query])
        return {
            "query": query,
            "embedding": vectors[0]
        }


class RetrievalOperator(MapFunction):
    """æ£€ç´¢ç®—å­ï¼ˆç¤ºä¾‹ï¼šæ¨¡æ‹Ÿæ£€ç´¢ï¼‰"""
    def __init__(self, knowledge_base, **kwargs):
        super().__init__(**kwargs)
        self.knowledge_base = knowledge_base
    
    def execute(self, data):
        # å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šæŸ¥è¯¢å‘é‡æ•°æ®åº“
        # ä¾‹å¦‚ ChromaDB, Milvus, Qdrant ç­‰
        query = data["query"]
        relevant_docs = self.knowledge_base[:3]  # ç®€åŒ–ç¤ºä¾‹
        return {
            "query": query,
            "context": "\n".join(relevant_docs)
        }


class GenerationOperator(MapFunction):
    """ç”Ÿæˆç®—å­"""
    def execute(self, data):
        query = data["query"]
        context = data["context"]
        
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""
        
        # ä½¿ç”¨ UnifiedInferenceClient ç”Ÿæˆå›ç­”
        response = client.chat([
            {"role": "user", "content": prompt}
        ])
        
        return {
            "query": query,
            "answer": response
        }


class ResultSink(SinkFunction):
    """ç»“æœè¾“å‡º"""
    def execute(self, data):
        print(f"é—®é¢˜: {data['query']}")
        print(f"å›ç­”: {data['answer']}")
        print("-" * 50)


def main():
    env = LocalEnvironment("RAG_Pipeline")
    
    queries = [
        "SAGE æ”¯æŒå“ªäº› LLM?",
        "å¦‚ä½•éƒ¨ç½²åˆ†å¸ƒå¼ Pipeline?"
    ]
    
    knowledge_base = [
        "SAGE æ”¯æŒ vLLMã€OpenAI API ç­‰å¤šç§ LLM åç«¯ã€‚",
        "SAGE åŸºäº Ray æ„å»ºåˆ†å¸ƒå¼æ‰§è¡Œèƒ½åŠ›ã€‚",
        "ä½¿ç”¨ RemoteEnvironment å¯ä»¥åœ¨é›†ç¾¤ä¸Šè¿è¡Œ Pipelineã€‚"
    ]
    
    (
        env.from_batch(QuerySource, queries=queries)
        .map(EmbeddingOperator)
        .map(RetrievalOperator, knowledge_base=knowledge_base)
        .map(GenerationOperator)
        .sink(ResultSink)
    )
    
    env.submit(autostop=True)


if __name__ == "__main__":
    main()
```

## å¤šæºæ£€ç´¢

### å¹¶è¡Œæ£€ç´¢å¤šä¸ªçŸ¥è¯†åº“

```python
from sage.llm import UnifiedInferenceClient

client = UnifiedInferenceClient.create_auto()


class MultiSourceRetriever(MapFunction):
    """å¤šæºæ£€ç´¢ç®—å­"""
    def __init__(self, sources, **kwargs):
        super().__init__(**kwargs)
        self.sources = sources  # å¤šä¸ªçŸ¥è¯†åº“é…ç½®
    
    def execute(self, data):
        query = data["query"]
        query_embedding = client.embed([query])[0]
        
        all_results = []
        for source_name, source_config in self.sources.items():
            # ä»æ¯ä¸ªæºæ£€ç´¢
            results = self._retrieve_from_source(
                query_embedding, 
                source_config,
                top_k=3
            )
            for r in results:
                r["source"] = source_name
            all_results.extend(results)
        
        # ç»“æœèåˆï¼šæŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
        all_results.sort(key=lambda x: x["score"], reverse=True)
        
        return {
            "query": query,
            "results": all_results[:5]  # å– Top-5
        }
    
    def _retrieve_from_source(self, embedding, config, top_k):
        # å®é™…å®ç°ä¸­æŸ¥è¯¢å¯¹åº”çš„å‘é‡æ•°æ®åº“
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿç»“æœ
        return [{"text": "ç¤ºä¾‹æ–‡æ¡£", "score": 0.9}]
```

## åˆ†å±‚æ£€ç´¢

### ä¸¤é˜¶æ®µæ£€ç´¢ç­–ç•¥

```python
class HierarchicalRetriever(MapFunction):
    """åˆ†å±‚æ£€ç´¢ï¼šç²—æ£€ç´¢ -> ç»†æ£€ç´¢"""
    
    def execute(self, data):
        query = data["query"]
        
        # é˜¶æ®µ 1: ç²—ç²’åº¦æ£€ç´¢ï¼ˆæ–‡æ¡£çº§ï¼‰
        coarse_results = self._coarse_retrieve(query, top_k=10)
        
        # é˜¶æ®µ 2: ç»†ç²’åº¦æ£€ç´¢ï¼ˆæ®µè½çº§ï¼‰
        fine_results = self._fine_retrieve(query, coarse_results, top_k=3)
        
        return {
            "query": query,
            "context": "\n".join([r["text"] for r in fine_results])
        }
    
    def _coarse_retrieve(self, query, top_k):
        """ç²—ç²’åº¦æ£€ç´¢ï¼šåŸºäºæ–‡æ¡£æ‘˜è¦æˆ–æ ‡é¢˜"""
        query_embedding = client.embed([query])[0]
        # ä½¿ç”¨æ–‡æ¡£çº§ç´¢å¼•æ£€ç´¢
        return []  # è¿”å›å€™é€‰æ–‡æ¡£
    
    def _fine_retrieve(self, query, candidates, top_k):
        """ç»†ç²’åº¦æ£€ç´¢ï¼šåœ¨å€™é€‰æ–‡æ¡£å†…æ£€ç´¢æ®µè½"""
        query_embedding = client.embed([query])[0]
        # åœ¨å€™é€‰æ–‡æ¡£çš„æ®µè½ä¸­æ£€ç´¢
        return []  # è¿”å›æœ€ç›¸å…³æ®µè½
```

## é‡æ’åºï¼ˆRe-rankingï¼‰

### Cross-Encoder é‡æ’åº

```python
class RerankOperator(MapFunction):
    """ä½¿ç”¨ Cross-Encoder é‡æ’åºæ£€ç´¢ç»“æœ"""
    
    def execute(self, data):
        query = data["query"]
        candidates = data["candidates"]
        
        # æ„å»º query-document å¯¹
        pairs = [(query, doc["text"]) for doc in candidates]
        
        # ä½¿ç”¨ LLM è¿›è¡Œç›¸å…³æ€§è¯„åˆ†
        scored_results = []
        for doc, (q, d) in zip(candidates, pairs):
            score = self._compute_relevance(q, d)
            scored_results.append({**doc, "rerank_score": score})
        
        # æŒ‰é‡æ’åºå¾—åˆ†æ’åº
        scored_results.sort(key=lambda x: x["rerank_score"], reverse=True)
        
        return {
            "query": query,
            "results": scored_results[:3]
        }
    
    def _compute_relevance(self, query, document):
        """ä½¿ç”¨ LLM è¯„ä¼°ç›¸å…³æ€§"""
        prompt = f"""è¯„ä¼°ä»¥ä¸‹æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ï¼ˆ0-10åˆ†ï¼‰ï¼š

æŸ¥è¯¢: {query}
æ–‡æ¡£: {document[:500]}

åªè¿”å›æ•°å­—åˆ†æ•°ï¼š"""
        
        response = client.chat([
            {"role": "user", "content": prompt}
        ])
        try:
            return float(response.strip())
        except ValueError:
            return 0.0
```

## æ··åˆæ£€ç´¢

### å‘é‡æ£€ç´¢ + BM25 å…³é”®è¯æ£€ç´¢

```python
class HybridRetriever(MapFunction):
    """æ··åˆæ£€ç´¢ï¼šç»“åˆå‘é‡ç›¸ä¼¼åº¦å’Œå…³é”®è¯åŒ¹é…"""
    
    def __init__(self, alpha=0.7, **kwargs):
        """
        Args:
            alpha: å‘é‡æ£€ç´¢æƒé‡ (1-alpha ä¸º BM25 æƒé‡)
        """
        super().__init__(**kwargs)
        self.alpha = alpha
    
    def execute(self, data):
        query = data["query"]
        
        # å‘é‡æ£€ç´¢
        vector_results = self._vector_search(query, top_k=10)
        
        # BM25 å…³é”®è¯æ£€ç´¢
        bm25_results = self._bm25_search(query, top_k=10)
        
        # èåˆç»“æœ (Reciprocal Rank Fusion)
        fused_results = self._rrf_fusion(vector_results, bm25_results)
        
        return {
            "query": query,
            "results": fused_results[:5]
        }
    
    def _vector_search(self, query, top_k):
        """å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢"""
        embedding = client.embed([query])[0]
        # æŸ¥è¯¢å‘é‡æ•°æ®åº“
        return []
    
    def _bm25_search(self, query, top_k):
        """BM25 å…³é”®è¯æ£€ç´¢"""
        # ä½¿ç”¨ BM25 ç®—æ³•æ£€ç´¢
        return []
    
    def _rrf_fusion(self, results1, results2, k=60):
        """Reciprocal Rank Fusion ç»“æœèåˆ"""
        scores = {}
        
        for rank, doc in enumerate(results1):
            doc_id = doc["id"]
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)
        
        for rank, doc in enumerate(results2):
            doc_id = doc["id"]
            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)
        
        # æŒ‰èåˆå¾—åˆ†æ’åº
        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        return [{"id": doc_id, "score": scores[doc_id]} for doc_id in sorted_ids]
```

## Control Plane æ¨¡å¼ï¼ˆé«˜çº§ï¼‰

å¯¹äºé«˜å¹¶å‘åœºæ™¯ï¼Œæ¨èä½¿ç”¨ Control Plane æ¨¡å¼ï¼š

```python
from sage.llm import UnifiedInferenceClient
from sage.common.config.ports import SagePorts

# ä½¿ç”¨ Control Plane æ¨¡å¼è·å¾—æ™ºèƒ½è°ƒåº¦
client = UnifiedInferenceClient.create(
    control_plane_url=f"http://localhost:{SagePorts.GATEWAY_DEFAULT}/v1",
    default_llm_model="Qwen/Qwen2.5-7B-Instruct",
    default_embedding_model="BAAI/bge-m3",
)

# æŸ¥çœ‹å®¢æˆ·ç«¯çŠ¶æ€
status = client.get_status()
print(f"æ¨¡å¼: {status['mode']}")
print(f"LLM å¯ç”¨: {status['llm_available']}")
print(f"Embedding å¯ç”¨: {status['embedding_available']}")
```

## æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

1. **ä½¿ç”¨ UnifiedInferenceClient** - ç»Ÿä¸€ç®¡ç† LLM å’Œ Embedding è°ƒç”¨
2. **åˆ†æ‰¹åµŒå…¥** - å¤§é‡æ–‡æœ¬ä½¿ç”¨æ‰¹é‡ embed æé«˜æ•ˆç‡
3. **ç¼“å­˜ç­–ç•¥** - å¯¹é‡å¤æŸ¥è¯¢ç¼“å­˜ embedding ç»“æœ
4. **é‡æ’åº** - ä½¿ç”¨ Cross-Encoder æå‡æ£€ç´¢è´¨é‡
5. **æ··åˆæ£€ç´¢** - ç»“åˆå‘é‡å’Œå…³é”®è¯æ£€ç´¢

### âŒ é¿å…çš„é—®é¢˜

- å•æ¬¡åµŒå…¥å¤§é‡æ–‡æœ¬å¯¼è‡´è¶…æ—¶
- å¿½ç•¥æ£€ç´¢ç»“æœçš„å»é‡å’Œè¿‡æ»¤
- ç›´æ¥ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœè€Œä¸é‡æ’åº
- ä¸Šä¸‹æ–‡è¿‡é•¿è¶…è¿‡ LLM ä¸Šä¸‹æ–‡çª—å£

## ç›¸å…³é˜…è¯»

- [Middleware RAG ç»„ä»¶](../../guides/packages/sage-middleware/components/sage_db.md)
- [Libs RAG å·¥å…·](../../guides/packages/sage-libs/rag.md)
- [æ€§èƒ½è°ƒä¼˜](performance-tuning.md) - ä¼˜åŒ– RAG ç³»ç»Ÿæ€§èƒ½
- [sage-common æ¦‚è§ˆ](../../guides/packages/sage-common/overview.md) - åŒ…å« UnifiedInferenceClient è¯´æ˜

---

**ä¸‹ä¸€æ­¥**ï¼šå­¦ä¹  [æ€§èƒ½è°ƒä¼˜](performance-tuning.md) ä¼˜åŒ– RAG ç³»ç»Ÿæ€§èƒ½
