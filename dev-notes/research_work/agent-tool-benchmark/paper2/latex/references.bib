@article{react2022,
  title={ReAct: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and others},
  journal={NeurIPS},
  year={2022}
}

@article{reflexion2024,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and others},
  journal={ICLR},
  year={2024}
}

@article{gorilla2023,
  title={Gorilla: Large language model connected with massive APIs},
  author={Patil, Shishir and others},
  journal={ICML},
  year={2023}
}

@article{toolllm2023,
  title={ToolLLM: Facilitating large language models to master tool use},
  author={Qin, Chengwei and others},
  journal={NeurIPS},
  year={2023}
}

@article{autogpt2023,
  title={AutoGPT: An autonomous GPT-4 experiment},
  author={Richards, Toran Bruce},
  journal={ArXiv},
  year={2023}
}

@article{tot2023,
  title={Tree of Thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and others},
  journal={NeurIPS},
  year={2023}
}

@article{lats2024,
  title={Language Agent Tree Search},
  author={Zhang, Zhexin and others},
  journal={ICLR},
  year={2024}
}

@article{voyager2023,
  title={Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author={Wang, Zhoujie and others},
  journal={Nature Machine Intelligence},
  year={2023}
}

@article{ssis2025,
  title={Streaming Sample Importance Scoring for Continual Tool Learning},
  author={Zhang, Alice and Chen, Bo and Park, Evelyn},
  journal={Under Review},
  year={2025}
}

@article{ewc2017,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and others},
  journal={PNAS},
  year={2017}
}

@article{coreset2018,
  title={Coresets for data-efficient training},
  author={Borsos, Zal{\'a}n and others},
  journal={ICML},
  year={2018}
}

@article{ppo2017,
  title={Proximal Policy Optimization Algorithms},
  author={Schulman, John and others},
  journal={ArXiv},
  year={2017}
}

@article{dpo2023,
  title={Direct Preference Optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and others},
  journal={NeurIPS},
  year={2023}
}

@article{vllm2023,
  title={vLLM: Easy, fast, and cheap LLM serving with PagedAttention},
  author={Kwon, Sang and others},
  journal={OSDI},
  year={2023}
}

@article{pagedattention2023,
  title={PagedAttention: Scalably Serving LLMs},
  author={Kwon, Sang and others},
  journal={OSDI},
  year={2023}
}

@article{continual2024,
  title={Continual Reinforcement Learning with LLM Agents},
  author={Kim, Minji and others},
  journal={ICML},
  year={2024}
}

@article{memory2024,
  title={Long-Term Memory for LLM Agents},
  author={Sohn, Geunho and others},
  journal={ICLR},
  year={2024}
}

@article{collab2024,
  title={Collaborative Agents: Communication Protocols for Tool Use},
  author={Li, Han and others},
  journal={ICML},
  year={2024}
}

@article{toolarena2024,
  title={ToolArena: Benchmarking Tool-Augmented Language Agents},
  author={Chen, Yujia and others},
  journal={NeurIPS},
  year={2024}
}

@article{sagebench2025,
  title={SAGE-Bench: Evaluating Tool-Augmented Agents},
  author={IntelliStream, Research Team},
  journal={Under Review},
  year={2025}
}
